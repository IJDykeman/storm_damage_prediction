{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['f', 'display']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded all file header attributes into dict\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from os import listdir\n",
    "import functools\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "\n",
    "import pylab\n",
    "from tsne import bh_sne\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy as np\n",
    "import sklearn.manifold\n",
    "import os\n",
    "import gc\n",
    "from time import gmtime, strftime\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import math\n",
    "from scipy.stats.stats import pearsonr\n",
    "import random as rand\n",
    "import pyproj\n",
    "import functools\n",
    "\n",
    "\n",
    "def display(image):\n",
    "    plt.imshow(image, cmap = plt.get_cmap('gray'), interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "def memoize(obj, maxlen = 2000):\n",
    "    \"\"\"A decorator to cache advice objects using the advice key\"\"\"\n",
    "    cache = obj.cache = {}\n",
    "    deck = obj.deck = deque([], maxlen = maxlen)\n",
    "\n",
    "    @functools.wraps(obj)\n",
    "    def memoizer(*args, **kwargs):\n",
    "        key = args[0]\n",
    "        if key not in cache:\n",
    "            if len(deck) == deck.maxlen:\n",
    "              del cache[deck.popleft()[0]]\n",
    "            temp = obj(*args, **kwargs)\n",
    "            cache[key] = temp\n",
    "            deck.append((key, temp))\n",
    "        return cache[key]\n",
    "\n",
    "    return memoizer\n",
    "\n",
    "@memoize\n",
    "def load_mat_from_file(key):\n",
    "    f = file(\"/home/isaac/Desktop/devika/ARCGIS/ArcGis/pickle_files/\"+key+\".npy\",\"rb\")\n",
    "    return np.load(f)\n",
    "\n",
    "@memoize\n",
    "def get_file_containing(housex, housey):\n",
    "#         print \"house coords:\", housex, housey\n",
    "    for key in file_attributes:\n",
    "        block = file_attributes[key]\n",
    "    #         print block['xllcorner'], block['yllcorner']\n",
    "        if block['xllcorner'] < housex < block['xllcorner'] + block['width']:\n",
    "            if block['yllcorner'] < housey < block['yllcorner'] + block['height']:\n",
    "                return key\n",
    "    assert(1==0)\n",
    "\n",
    "def imagex(housex, housey, block):\n",
    "    return (int(housex)-block['xllcorner'])/5\n",
    "def imagey(housex, housey, block):\n",
    "    return (int(housey)-block['yllcorner'])/5\n",
    "\n",
    "\n",
    "\n",
    "#housex and housey in meters\n",
    "def get_heightmap_around(housex, housey, window_width_pixels = 28):\n",
    "    \n",
    "    def image_slice(key, housex, housey, radius = window_width_pixels/2):\n",
    "        x = housex\n",
    "        y = housey\n",
    "        block = file_attributes[key]\n",
    "        mat = load_mat_from_file(key)\n",
    "        x1 = np.max([0,              imagex(x,y,block) - radius])\n",
    "        x2 = np.min([block['ncols'], imagex(x,y,block) + radius])\n",
    "        \n",
    "        y1 = np.max([0,              imagey(x,y,block) - radius])\n",
    "        y2 = np.min([block['nrows'], imagey(x,y,block) + radius])\n",
    "        \n",
    "        assert(x1 <= x2)\n",
    "        assert(y1 <= y2)\n",
    "        return  mat[block['nrows'] - y2:block['nrows'] - y1, x1:x2]\n",
    "    \n",
    "    window_width_feet = window_width_pixels * 5\n",
    "    housex /= .3048 #convert to feet\n",
    "    housey /= .3048\n",
    "    x = housex\n",
    "    y = housey\n",
    "    ul = get_file_containing(x - window_width_feet/2, y - window_width_feet/2)\n",
    "    ur = get_file_containing(x + window_width_feet/2, y - window_width_feet/2)\n",
    "    ll = get_file_containing(x - window_width_feet/2, y + window_width_feet/2)\n",
    "    lr = get_file_containing(x + window_width_feet/2, y + window_width_feet/2)\n",
    "\n",
    "    for key in [ul, ur, ll, lr]:\n",
    "        image = image_slice(key, x, y)\n",
    "#         print image.shape\n",
    "        if image.shape == (window_width_pixels, window_width_pixels):\n",
    "            return image\n",
    "\n",
    "    \n",
    "\n",
    "    if ul == ur and lr == ll : # horizontal split\n",
    "\n",
    "\n",
    "        result = np.zeros([window_width_pixels, window_width_pixels])\n",
    "        upper_slice = image_slice(ul, housex, housey)\n",
    "        lower_slice = image_slice(ll, housex, housey)\n",
    "        \n",
    "        result[:upper_slice.shape[0], :] = upper_slice\n",
    "        result[ window_width_pixels - lower_slice.shape[0]:, :] = lower_slice\n",
    "\n",
    "\n",
    "        return result\n",
    "    elif ul == ll and ur == lr and ul != ur and ll != lr: # vertical split\n",
    "        result = np.zeros([window_width_pixels, window_width_pixels])\n",
    "        left_slice = image_slice(ll, housex, housey)\n",
    "        right_slice = image_slice(ur, housex, housey)\n",
    "#         print left_slice.shape\n",
    "#         print right_slice.shape\n",
    "        \n",
    "        result[:, :left_slice.shape[1]] = left_slice\n",
    "        result[:, window_width_pixels - right_slice.shape[1]:] = right_slice\n",
    "\n",
    "#         plt.imshow(result)\n",
    "#         plt.show()\n",
    "#         display(result)\n",
    "\n",
    "        return result\n",
    "    else: # four way split\n",
    "#         print \"four way\"\n",
    "        ll_slice = image_slice(ll, housex, housey)\n",
    "        ul_slice = image_slice(ul, housex, housey)\n",
    "        lr_slice = image_slice(lr, housex, housey)\n",
    "        ur_slice = image_slice(ur, housex, housey)\n",
    "        \n",
    "        result = np.zeros([window_width_pixels, window_width_pixels])\n",
    "        result[:ll_slice.shape[0], :ll_slice.shape[1]] = ll_slice\n",
    "        result[:lr_slice.shape[0], window_width_pixels - lr_slice.shape[1]:] = lr_slice\n",
    "        \n",
    "        result[window_width_pixels - ul_slice.shape[0]:, :ul_slice.shape[1]] = ul_slice\n",
    "        result[window_width_pixels - ur_slice.shape[0]:, window_width_pixels - ur_slice.shape[1]:] = ur_slice\n",
    "#         display(result)\n",
    "        return result\n",
    "\n",
    "        \n",
    "\n",
    "def window_violated_chunk_borders(housex, housey):\n",
    "    corners = [(housex-window_width_feet/2, housey-window_width_feet/2),\n",
    "              (housex-window_width_feet/2, housey+window_width_feet/2),\n",
    "              (housex+window_width_feet/2, housey+window_width_feet/2),\n",
    "              (housex+window_width_feet/2, housey-window_width_feet/2)]\n",
    "    files = map(lambda x: get_file_containing(x[0], x[1]), corners)\n",
    "    return len(set(files)) > 1\n",
    "\n",
    "\n",
    "\n",
    "file_attributes = {}\n",
    "\n",
    "mypath = \"/home/isaac/Desktop/devika/ARCGIS/ArcGis/ascii_files\"\n",
    "for filename in [f for f in listdir(mypath) if isfile(join(mypath, f))]: \n",
    "    attributes = {}\n",
    "    with open(mypath + \"/\" + filename) as FileObj:\n",
    "        for index, line in enumerate(FileObj):\n",
    "            if(index < 6):\n",
    "#                 print line\n",
    "                attributes[line.split(\" \")[0]] = int(line.split(\" \")[-1][:-2])\n",
    "            else:\n",
    "                break # don't load the other lines into memory becuase that's a waste of time.\n",
    "\n",
    "    attributes['width'] = attributes['ncols'] * attributes['cellsize']\n",
    "    attributes['height'] = attributes['nrows'] * attributes['cellsize']\n",
    "    file_attributes[filename] = attributes\n",
    "print \"loaded all file header attributes into dict\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mage_width = 40\n",
    "\n",
    "wgs84=pyproj.Proj(\"+init=EPSG:4326\") # LatLon with WGS84 datum used by GPS units and Google Earth\n",
    "UTM26N=pyproj.Proj(\"+init=EPSG:2278\") # UTM coords, zone Texas Central, WGS84 datum\n",
    "\n",
    "# lon = [meta['lon'][250000]] #these are good becuase of clear local features\n",
    "# lat = [meta['lat'][250000]]\n",
    "\n",
    "\n",
    "lon = [meta['lon'][12000]]\n",
    "lat = [meta['lat'][12000]]\n",
    "\n",
    "print \"house lat lon\", lat, lon\n",
    "xx, yy = pyproj.transform(wgs84, UTM26N, lon, lat)\n",
    "print xx, yy\n",
    "housex=xx[0]\n",
    "housey=yy[0]\n",
    "get_heightmap_around(housex,housey)\n",
    "\n",
    "n=10000\n",
    "metamat = np.zeros([len(meta),1,image_width,image_width])\n",
    "# metamat = metamat.astype(float32)\n",
    "metaY = np.zeros([len(meta)])\n",
    "\n",
    "\n",
    "\n",
    "# print \"number 3767\"\n",
    "# lon = [meta['lon'][3767]]\n",
    "# lat = [meta['lat'][3767]]\n",
    "# print lat,lon\n",
    "# xx, yy = pyproj.transform(wgs84, UTM26N, lon, lat)\n",
    "# housex=xx[0]\n",
    "# housey=yy[0]\n",
    "# new_element = get_heightmap_around(housex, housey, window_width_pixels=image_width)\n",
    "# print new_element.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index in range(len(meta)):\n",
    "    \n",
    "#     index = rand.randint(0, len(meta)-1)\n",
    "    lon = [meta['lon'][index]]\n",
    "    lat = [meta['lat'][index]]\n",
    "\n",
    "    xx, yy = pyproj.transform(wgs84, UTM26N, lon, lat)\n",
    "    housex=xx[0]\n",
    "    housey=yy[0]\n",
    "    \n",
    "    \n",
    "    new_element = get_heightmap_around(housex, housey, window_width_pixels=image_width)\n",
    "    if(new_element == None):\n",
    "        print index\n",
    "        break\n",
    "    if index%1000 == 0:\n",
    "        print index\n",
    "        \n",
    "    \n",
    "    \n",
    "    new_element = np.array([new_element])\n",
    "    metamat[index] = new_element\n",
    "    metamat[index] = metamat[index] - np.min(metamat[index])\n",
    "\n",
    "    metaY[index] = y_data['y'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A Convolutional Network implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
