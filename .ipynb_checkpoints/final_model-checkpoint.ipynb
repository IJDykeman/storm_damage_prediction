{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, CuDNN 3007)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.layers import Conv2DLayer\n",
    "from lasagne.layers import MaxPool2DLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import adam\n",
    "from lasagne.layers import get_all_params\n",
    "import theano as T\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import TrainSplit\n",
    "from nolearn.lasagne import objective\n",
    "import gc\n",
    "import pandas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imagewidth = 40\n",
    "def display(image, min = 0.0, max = 1.0):\n",
    "    plt.imshow(image, cmap = plt.get_cmap('gray'), interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def load_data(path):\n",
    "    print(\"loading...\")\n",
    "    gc.collect() # collect garbage\n",
    "    data = pandas.read_hdf(path, '/df')\n",
    "    df = pandas.DataFrame(data)\n",
    "    data_dict = {}\n",
    "    for label in set(df._get_numeric_data().columns).union({'hcad'}):\n",
    "        # union hcad to ensure that hcad col comes in even if not considered numerical\n",
    "        # if label != 'hcad':\n",
    "        data_dict[label] = df[label].astype(float)\n",
    "        # df[label][df[label] > 1] = 1.0\n",
    "\n",
    "    # df['hcad'] = df['hcad'].astype(float)\n",
    "    result = pandas.DataFrame.from_dict(data_dict)\n",
    "\n",
    "    result = result.replace([np.inf, -np.inf], 1)\n",
    "    \n",
    "    return result.sort(['hcad']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = np.zeros([870476, 256])\n",
    "for i in range(88):\n",
    "    piece = np.load(\"/home/isaac/Desktop/devika/final_model_inputs/compressed_terrain_\"+str(i)+\".npy\")\n",
    "    mat[i*10000: i*10000 + len(piece)] = piece\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(870476, 256)\n"
     ]
    }
   ],
   "source": [
    "print mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "\n",
    "# for i in range(2,1000):\n",
    "kmeans = KMeans(init='k-means++', n_clusters = 3)\n",
    "kmeans.fit(mat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "a = kmeans.labels_\n",
    "b = np.zeros((a.size, a.max()+1))\n",
    "b[np.arange(len(mat)), a] = 1\n",
    "print b\n",
    "print b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "Opening /home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_200.hd in read-only mode\n",
      "loading...\n",
      "Opening /home/isaac/Dropbox/data_for_brian/wind_features/hcad_interp_withoutpartial_rad100_hist8x8.mat.hd in read-only mode\n",
      "loading...\n",
      "Opening /home/isaac/Dropbox/data_for_brian/y_df.hd in read-only mode\n"
     ]
    }
   ],
   "source": [
    "hcad = load_data(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_200.hd\")\n",
    "wind_data = load_data(\"/home/isaac/Dropbox/data_for_brian/wind_features/hcad_interp_withoutpartial_rad100_hist8x8.mat.hd\")\n",
    "y_data = load_data(\"/home/isaac/Dropbox/data_for_brian/y_df.hd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print wind_data.columns\n",
    "speed_names = [\n",
    "    u'intp_spd0',\n",
    "    u'intp_spd1',\n",
    "    u'intp_spd2',\n",
    "    u'intp_spd3',\n",
    "    u'intp_spd4',\n",
    "    u'intp_spd5',\n",
    "    u'intp_spd6',\n",
    "    u'intp_spd7',\n",
    "    u'intp_spd8',\n",
    "    u'intp_spd9',\n",
    "\n",
    "]\n",
    "wind_maxes = np.array(wind_data[speed_names].max(axis=1)).reshape(-1, 1)\n",
    "print wind_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30002004]\n",
      " [ 0.30002591]\n",
      " [ 0.29971594]\n",
      " ..., \n",
      " [ 0.0959446 ]\n",
      " [ 0.053092  ]\n",
      " [ 0.05290391]]\n",
      "0.300026 0.0486294\n"
     ]
    }
   ],
   "source": [
    "# load hidden layer acitvations of hcad data\n",
    "hcad_1d = np.load(\"/home/isaac/Desktop/devika/gitignored/autoencoder_hidden_activations.npy\")\n",
    "print hcad_1d\n",
    "print np.max(hcad_1d), np.min(hcad_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[870476     89]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trXx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bc97ffa2d4c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# x = np.concatenate((x, wind_maxes), axis = 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrXx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrXx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtrY\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trXx' is not defined"
     ]
    }
   ],
   "source": [
    "mat = mat / np.max(mat)\n",
    "print np.array(wind_data.shape)\n",
    "trY = np.array(y_data['y100_mean']).astype(np.float32)\n",
    "trX = np.concatenate ((np.array(mat).astype(np.float32),\n",
    "                     np.array(wind_data)/np.max([-np.min(wind_data),np.max(wind_data)]),\n",
    "                    hcad_1d), axis = 1)\n",
    "# x = np.concatenate ((b.astype(np.float32), np.array(wind_data)/np.max([-np.min(wind_data),np.max(wind_data)])), axis = 1)\n",
    "# x = np.concatenate ((b.astype(np.float32), wind_maxes, hcad_1d), axis = 1)\n",
    "\n",
    "# x = np.concatenate((x, wind_maxes), axis = 1)\n",
    "trXx = trXx.astype(np.float32)\n",
    "\n",
    "print trY\n",
    "print trX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print np.max(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "# Neural Network with 18391 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       346\n",
      "  1  hidden       50\n",
      "  2  hidden2      20\n",
      "  3  output        1\n",
      "\n",
      "  epoch    train loss    valid loss    train/val  dur\n",
      "-------  ------------  ------------  -----------  -----\n",
      "      1       \u001b[36m0.09894\u001b[0m       \u001b[32m0.17265\u001b[0m      0.57307  3.79s\n",
      "      2       0.09904       \u001b[32m0.17265\u001b[0m      0.57366  3.74s\n",
      "      3       0.09906       \u001b[32m0.17235\u001b[0m      0.57475  3.74s\n",
      "      4       0.09910       \u001b[32m0.17192\u001b[0m      0.57640  3.74s\n",
      "      5       0.09914       \u001b[32m0.17129\u001b[0m      0.57881  3.74s\n",
      "      6       0.09918       \u001b[32m0.17062\u001b[0m      0.58131  3.74s\n",
      "      7       0.09923       \u001b[32m0.16995\u001b[0m      0.58387  3.74s\n",
      "      8       0.09927       \u001b[32m0.16925\u001b[0m      0.58654  3.75s\n",
      "      9       0.09931       \u001b[32m0.16856\u001b[0m      0.58918  3.76s\n",
      "     10       0.09935       \u001b[32m0.16797\u001b[0m      0.59146  3.76s\n",
      "     11       0.09938       \u001b[32m0.16726\u001b[0m      0.59420  3.76s\n",
      "     12       0.09935       \u001b[32m0.16702\u001b[0m      0.59486  3.75s\n",
      "     13       0.09940       \u001b[32m0.16623\u001b[0m      0.59796  3.75s\n",
      "     14       0.09938       \u001b[32m0.16598\u001b[0m      0.59875  3.75s\n",
      "     15       0.09937       \u001b[32m0.16571\u001b[0m      0.59964  3.75s\n",
      "     16       0.09931       \u001b[32m0.16555\u001b[0m      0.59988  3.75s\n",
      "     17       0.09925       \u001b[32m0.16517\u001b[0m      0.60092  3.75s\n",
      "     18       0.09923       \u001b[32m0.16465\u001b[0m      0.60265  3.75s\n",
      "     19       0.09917       \u001b[32m0.16396\u001b[0m      0.60486  3.75s\n",
      "     20       0.09913       \u001b[32m0.16348\u001b[0m      0.60640  3.76s\n",
      "     21       0.09914       \u001b[32m0.16308\u001b[0m      0.60788  3.76s\n",
      "     22       0.09919       \u001b[32m0.16234\u001b[0m      0.61099  3.76s\n",
      "     23       0.09909       \u001b[32m0.16104\u001b[0m      0.61535  3.76s\n",
      "     24       0.09907       0.16113      0.61485  3.75s\n",
      "     25       0.09913       \u001b[32m0.15993\u001b[0m      0.61985  3.75s\n",
      "     26       0.09900       \u001b[32m0.15845\u001b[0m      0.62478  3.75s\n",
      "     27       0.09896       0.15909      0.62205  3.75s\n",
      "     28       0.09898       \u001b[32m0.15652\u001b[0m      0.63239  3.75s\n",
      "     29       \u001b[36m0.09886\u001b[0m       0.15673      0.63080  3.75s\n",
      "     30       0.09894       \u001b[32m0.15506\u001b[0m      0.63806  3.76s\n",
      "     31       0.09887       0.15623      0.63285  3.75s\n",
      "     32       0.09892       \u001b[32m0.15277\u001b[0m      0.64755  3.75s\n",
      "     33       \u001b[36m0.09877\u001b[0m       0.15393      0.64168  3.75s\n",
      "     34       0.09881       \u001b[32m0.15124\u001b[0m      0.65333  3.75s\n",
      "     35       \u001b[36m0.09871\u001b[0m       \u001b[32m0.15099\u001b[0m      0.65377  3.75s\n",
      "     36       0.09881       0.15590      0.63378  3.76s\n",
      "     37       0.09893       \u001b[32m0.14971\u001b[0m      0.66081  3.75s\n",
      "     38       0.09873       0.15006      0.65793  3.75s\n",
      "     39       0.09873       \u001b[32m0.14909\u001b[0m      0.66221  3.75s\n",
      "     40       0.09874       0.15217      0.64884  3.75s\n",
      "     41       0.09879       0.14947      0.66097  3.75s\n",
      "     42       0.09873       0.15203      0.64941  3.75s\n",
      "     43       0.09882       0.15172      0.65134  3.75s\n",
      "     44       0.09890       0.15805      0.62574  3.75s\n",
      "     45       0.09902       \u001b[32m0.14844\u001b[0m      0.66707  3.76s\n",
      "     46       \u001b[36m0.09871\u001b[0m       0.14848      0.66483  3.75s\n",
      "     47       \u001b[36m0.09868\u001b[0m       \u001b[32m0.14843\u001b[0m      0.66484  3.75s\n",
      "     48       0.09873       0.14989      0.65871  3.76s\n",
      "     49       0.09873       \u001b[32m0.14715\u001b[0m      0.67096  3.75s\n",
      "     50       \u001b[36m0.09865\u001b[0m       0.14924      0.66104  3.74s\n",
      "     51       0.09875       0.15216      0.64899  3.73s\n",
      "     52       0.09881       \u001b[32m0.14696\u001b[0m      0.67237  3.73s\n",
      "     53       0.09866       0.14797      0.66678  3.73s\n",
      "     54       0.09869       0.14766      0.66838  3.73s\n",
      "     55       0.09870       0.14937      0.66080  3.73s\n",
      "     56       0.09872       0.14732      0.67006  3.73s\n",
      "     57       0.09868       0.14726      0.67013  3.73s\n",
      "     58       0.09868       0.14726      0.67011  3.73s\n",
      "     59       0.09869       0.14776      0.66790  3.73s\n",
      "     60       0.09869       0.14741      0.66948  3.73s\n",
      "     61       0.09868       0.14700      0.67127  3.73s\n",
      "     62       0.09866       \u001b[32m0.14489\u001b[0m      0.68097  3.73s\n",
      "     63       \u001b[36m0.09863\u001b[0m       0.14729      0.66965  3.73s\n",
      "     64       0.09866       \u001b[32m0.14411\u001b[0m      0.68459  3.76s\n",
      "     65       \u001b[36m0.09861\u001b[0m       0.14433      0.68323  3.73s\n",
      "     66       \u001b[36m0.09861\u001b[0m       0.14423      0.68369  3.73s\n",
      "     67       0.09862       \u001b[32m0.14374\u001b[0m      0.68606  3.73s\n",
      "     68       0.09863       0.14585      0.67626  3.73s\n",
      "     69       0.09866       0.14584      0.67652  3.73s\n",
      "     70       0.09866       0.14404      0.68493  3.73s\n",
      "     71       0.09862       0.14432      0.68337  3.73s\n",
      "     72       0.09863       \u001b[32m0.14364\u001b[0m      0.68662  3.75s\n",
      "     73       0.09862       0.14377      0.68598  3.75s\n",
      "     74       0.09862       \u001b[32m0.14299\u001b[0m      0.68965  3.76s\n",
      "     75       \u001b[36m0.09861\u001b[0m       0.14343      0.68750  3.75s\n",
      "     76       \u001b[36m0.09859\u001b[0m       \u001b[32m0.14288\u001b[0m      0.69007  3.75s\n",
      "     77       \u001b[36m0.09858\u001b[0m       0.14305      0.68917  3.76s\n",
      "     78       0.09862       0.14423      0.68378  3.75s\n",
      "     79       0.09865       0.14486      0.68099  3.75s\n",
      "     80       0.09865       \u001b[32m0.14257\u001b[0m      0.69193  3.76s\n",
      "     81       0.09860       \u001b[32m0.14244\u001b[0m      0.69219  3.75s\n",
      "     82       0.09860       0.14252      0.69183  3.73s\n",
      "     83       0.09860       \u001b[32m0.14224\u001b[0m      0.69320  3.73s\n",
      "     84       0.09859       \u001b[32m0.14215\u001b[0m      0.69359  3.73s\n",
      "     85       0.09862       0.14384      0.68561  3.73s\n",
      "     86       0.09861       \u001b[32m0.14161\u001b[0m      0.69634  3.73s\n",
      "     87       \u001b[36m0.09858\u001b[0m       0.14164      0.69596  3.74s\n",
      "     88       0.09859       0.14195      0.69453  3.77s\n",
      "     89       0.09859       0.14175      0.69550  3.76s\n",
      "     90       0.09858       0.14187      0.69485  3.76s\n",
      "     91       0.09859       0.14200      0.69433  3.76s\n",
      "     92       0.09858       \u001b[32m0.14142\u001b[0m      0.69710  3.76s\n",
      "     93       \u001b[36m0.09858\u001b[0m       0.14166      0.69587  3.76s\n",
      "     94       0.09858       0.14161      0.69615  3.76s\n",
      "     95       0.09858       0.14153      0.69652  3.76s\n",
      "     96       0.09858       0.14171      0.69563  3.76s\n",
      "     97       0.09860       0.14218      0.69344  3.75s\n",
      "     98       0.09858       \u001b[32m0.14106\u001b[0m      0.69883  3.75s\n",
      "     99       0.09858       0.14222      0.69315  3.75s\n",
      "    100       0.09858       \u001b[32m0.14088\u001b[0m      0.69978  3.74s\n",
      "    101       \u001b[36m0.09857\u001b[0m       0.14102      0.69899  3.73s\n",
      "    102       \u001b[36m0.09856\u001b[0m       0.14092      0.69941  3.74s\n",
      "    103       0.09857       0.14096      0.69924  3.87s\n",
      "    104       0.09856       0.14134      0.69735  3.74s\n",
      "    105       0.09858       0.14111      0.69859  3.74s\n",
      "    106       0.09858       0.14164      0.69596  3.74s\n",
      "    107       0.09856       \u001b[32m0.14083\u001b[0m      0.69987  3.74s\n",
      "    108       0.09856       0.14085      0.69974  3.73s\n",
      "    109       0.09856       \u001b[32m0.14069\u001b[0m      0.70057  3.74s\n",
      "    110       \u001b[36m0.09855\u001b[0m       \u001b[32m0.14052\u001b[0m      0.70134  3.74s\n",
      "    111       \u001b[36m0.09855\u001b[0m       0.14088      0.69952  3.73s\n",
      "    112       0.09856       \u001b[32m0.14042\u001b[0m      0.70186  3.76s\n",
      "    113       0.09858       0.14208      0.69383  3.76s\n",
      "    114       0.09858       \u001b[32m0.13995\u001b[0m      0.70437  3.76s\n",
      "    115       \u001b[36m0.09855\u001b[0m       0.14039      0.70196  3.76s\n",
      "    116       0.09856       0.14061      0.70093  3.76s\n",
      "    117       \u001b[36m0.09855\u001b[0m       \u001b[32m0.13991\u001b[0m      0.70438  3.76s\n",
      "    118       \u001b[36m0.09854\u001b[0m       0.14058      0.70097  3.75s\n",
      "    119       0.09857       \u001b[32m0.13980\u001b[0m      0.70506  3.73s\n",
      "    120       0.09856       0.14048      0.70160  3.73s\n",
      "    121       0.09856       \u001b[32m0.13969\u001b[0m      0.70554  3.74s\n",
      "    122       0.09856       0.14018      0.70307  3.73s\n",
      "    123       0.09857       0.14180      0.69513  3.73s\n",
      "    124       0.09859       0.14096      0.69940  3.73s\n",
      "    125       0.09857       0.14000      0.70410  3.73s\n",
      "    126       0.09856       0.14074      0.70033  3.73s\n",
      "    127       0.09856       \u001b[32m0.13931\u001b[0m      0.70752  3.73s\n",
      "    128       0.09855       0.13954      0.70627  3.74s\n",
      "    129       0.09855       \u001b[32m0.13841\u001b[0m      0.71198  3.75s\n",
      "    130       \u001b[36m0.09854\u001b[0m       0.14111      0.69832  3.75s\n",
      "    131       0.09860       0.14057      0.70140  3.75s\n",
      "    132       0.09857       0.13900      0.70914  3.75s\n",
      "    133       0.09856       0.14013      0.70338  3.76s\n",
      "    134       0.09856       0.13952      0.70643  3.76s\n",
      "    135       0.09858       0.14149      0.69670  3.76s\n",
      "    136       0.09860       0.14052      0.70170  3.76s\n",
      "    137       0.09857       0.13851      0.71165  3.76s\n",
      "    138       0.09855       0.14009      0.70351  3.76s\n",
      "    139       0.09856       0.13960      0.70605  3.73s\n",
      "    140       0.09855       0.13926      0.70764  3.73s\n",
      "    141       \u001b[36m0.09852\u001b[0m       0.13864      0.71064  3.73s\n",
      "    142       0.09854       0.14128      0.69746  3.73s\n",
      "    143       0.09857       0.14042      0.70201  3.74s\n",
      "    144       0.09857       0.14081      0.70004  3.74s\n",
      "    145       0.09860       0.14259      0.69154  3.73s\n",
      "    146       0.09864       0.14108      0.69914  3.73s\n",
      "    147       0.09861       0.14059      0.70138  3.73s\n",
      "    148       0.09857       \u001b[32m0.13757\u001b[0m      0.71653  3.73s\n",
      "    149       0.09857       0.14385      0.68525  3.73s\n",
      "    150       0.09860       0.13761      0.71653  3.73s\n",
      "    151       0.09853       0.14097      0.69896  3.73s\n",
      "    152       0.09859       0.14104      0.69901  3.73s\n",
      "    153       0.09859       0.13934      0.70756  3.73s\n",
      "    154       0.09861       0.14279      0.69061  3.73s\n",
      "    155       0.09863       0.13869      0.71112  3.73s\n",
      "    156       0.09855       0.13786      0.71486  3.74s\n",
      "    157       0.09853       0.13914      0.70818  3.75s\n",
      "    158       0.09854       0.13796      0.71426  3.75s\n",
      "    159       0.09858       0.14356      0.68668  3.76s\n",
      "    160       0.09861       \u001b[32m0.13702\u001b[0m      0.71967  3.74s\n",
      "    161       0.09853       0.13789      0.71451  3.73s\n",
      "    162       0.09854       0.14025      0.70261  3.74s\n",
      "    163       0.09860       0.14233      0.69278  3.73s\n",
      "    164       0.09863       0.14067      0.70115  3.73s\n",
      "    165       0.09859       \u001b[32m0.13662\u001b[0m      0.72161  3.73s\n",
      "    166       \u001b[36m0.09851\u001b[0m       0.13664      0.72094  3.73s\n",
      "    167       0.09854       0.14235      0.69225  3.73s\n",
      "    168       0.09864       0.14249      0.69226  3.73s\n",
      "    169       0.09860       0.13665      0.72156  3.73s\n",
      "    170       0.09852       0.13670      0.72070  3.73s\n",
      "    171       \u001b[36m0.09851\u001b[0m       0.13763      0.71572  3.73s\n",
      "    172       \u001b[36m0.09851\u001b[0m       0.13885      0.70943  3.73s\n",
      "    173       0.09851       0.13801      0.71379  3.74s\n",
      "    174       0.09852       0.13775      0.71518  3.73s\n",
      "    175       0.09852       \u001b[32m0.13510\u001b[0m      0.72921  3.73s\n",
      "    176       0.09854       0.13590      0.72513  3.73s\n",
      "    177       0.09856       0.14085      0.69975  3.73s\n",
      "    178       0.09859       0.13568      0.72666  3.73s\n",
      "    179       0.09853       0.13813      0.71336  3.73s\n",
      "    180       0.09857       0.14030      0.70256  3.74s\n",
      "    181       0.09856       0.13632      0.72302  3.73s\n",
      "    182       \u001b[36m0.09850\u001b[0m       0.13713      0.71834  3.73s\n",
      "    183       0.09855       0.14251      0.69151  3.73s\n",
      "    184       0.09858       0.13644      0.72247  3.73s\n",
      "    185       0.09853       0.13897      0.70904  3.73s\n",
      "    186       0.09852       0.13661      0.72118  3.73s\n",
      "    187       \u001b[36m0.09850\u001b[0m       0.13811      0.71319  3.73s\n",
      "    188       0.09852       0.13840      0.71186  3.73s\n",
      "    189       \u001b[36m0.09848\u001b[0m       0.13680      0.71991  3.73s\n",
      "    190       \u001b[36m0.09847\u001b[0m       \u001b[32m0.13509\u001b[0m      0.72890  3.73s\n",
      "    191       0.09849       0.13626      0.72280  3.73s\n",
      "    192       0.09851       0.13719      0.71807  3.74s\n",
      "    193       0.09854       0.13607      0.72419  3.73s\n",
      "    194       0.09862       0.13707      0.71946  3.74s\n",
      "    195       0.09861       0.13775      0.71585  3.73s\n",
      "    196       0.09855       0.13515      0.72919  3.73s\n",
      "    197       0.09851       0.13710      0.71857  3.76s\n",
      "    198       0.09851       0.13946      0.70634  3.76s\n",
      "    199       0.09863       0.13969      0.70608  3.76s\n",
      "    200       0.09864       0.13889      0.71022  3.76s\n",
      "    201       0.09859       0.13524      0.72898  3.76s\n",
      "    202       0.09852       0.13594      0.72469  3.76s\n",
      "    203       0.09853       0.13957      0.70595  3.75s\n",
      "    204       0.09855       0.13797      0.71430  3.75s\n",
      "    205       0.09852       0.13628      0.72294  3.75s\n",
      "    206       0.09854       0.14017      0.70302  3.74s\n",
      "    207       0.09854       0.13546      0.72742  3.75s\n",
      "    208       0.09848       0.13514      0.72876  3.75s\n",
      "    209       0.09854       0.14020      0.70285  3.75s\n",
      "    210       0.09861       0.13773      0.71596  3.75s\n",
      "    211       0.09859       0.13541      0.72806  3.75s\n",
      "    212       0.09852       0.13603      0.72426  3.75s\n",
      "    213       0.09851       0.13735      0.71720  3.75s\n",
      "    214       0.09851       0.13690      0.71957  3.75s\n",
      "    215       0.09855       0.14081      0.69992  3.75s\n",
      "    216       0.09859       0.13616      0.72413  3.75s\n",
      "    217       0.09860       0.14070      0.70077  3.75s\n",
      "    218       0.09857       \u001b[32m0.13492\u001b[0m      0.73059  3.74s\n",
      "    219       0.09857       0.14190      0.69465  3.74s\n",
      "    220       0.09862       0.13766      0.71641  3.75s\n",
      "    221       0.09861       0.13898      0.70952  3.75s\n",
      "    222       0.09860       0.13548      0.72774  3.74s\n",
      "    223       0.09851       0.13559      0.72651  3.74s\n",
      "    224       0.09849       0.13695      0.71915  3.77s\n",
      "    225       0.09852       0.13965      0.70550  3.75s\n",
      "    226       0.09853       0.13691      0.71969  3.75s\n",
      "    227       0.09848       0.13565      0.72602  3.75s\n",
      "    228       \u001b[36m0.09847\u001b[0m       0.13500      0.72938  3.75s\n",
      "    229       \u001b[36m0.09846\u001b[0m       0.13537      0.72731  3.75s\n",
      "    230       0.09850       0.13761      0.71581  3.75s\n",
      "    231       0.09856       \u001b[32m0.13482\u001b[0m      0.73103  3.75s\n",
      "    232       0.09850       0.13791      0.71423  3.75s\n",
      "    233       0.09856       0.13519      0.72909  3.75s\n",
      "    234       0.09854       0.13732      0.71760  3.75s\n",
      "    235       0.09853       0.13746      0.71677  3.75s\n",
      "    236       0.09855       0.13883      0.70987  3.75s\n",
      "    237       0.09859       0.14114      0.69853  3.75s\n",
      "    238       0.09862       0.13762      0.71663  3.75s\n",
      "    239       0.09855       0.13503      0.72979  3.75s\n",
      "    240       0.09852       0.13886      0.70950  3.76s\n",
      "    241       0.09855       0.13827      0.71273  3.75s\n",
      "    242       0.09864       0.14355      0.68717  3.75s\n",
      "    243       0.09861       \u001b[32m0.13444\u001b[0m      0.73349  3.74s\n",
      "    244       0.09851       0.13665      0.72088  3.74s\n",
      "    245       0.09851       0.13738      0.71709  3.75s\n",
      "    246       0.09858       0.14297      0.68950  3.75s\n",
      "    247       0.09865       0.13978      0.70574  3.75s\n",
      "    248       0.09859       0.13549      0.72766  3.75s\n",
      "    249       0.09855       0.13720      0.71827  3.75s\n",
      "    250       0.09859       0.14061      0.70115  3.75s\n",
      "    251       0.09859       0.13517      0.72935  3.76s\n",
      "    252       0.09858       0.14149      0.69675  3.76s\n",
      "    253       0.09861       0.13593      0.72542  3.76s\n",
      "    254       0.09853       0.13691      0.71968  3.76s\n",
      "    255       0.09857       0.14027      0.70271  3.76s\n",
      "    256       0.09857       0.13487      0.73082  3.76s\n",
      "    257       0.09852       0.13875      0.71009  3.75s\n",
      "    258       0.09858       0.13999      0.70415  3.75s\n",
      "    259       0.09854       0.13522      0.72877  3.76s\n",
      "    260       0.09856       0.14234      0.69246  3.75s\n",
      "    261       0.09859       0.13524      0.72896  3.76s\n",
      "    262       0.09852       0.13724      0.71785  3.76s\n",
      "    263       0.09850       0.13639      0.72221  3.76s\n",
      "    264       0.09854       0.14082      0.69974  3.75s\n",
      "    265       0.09867       0.13961      0.70678  3.75s\n",
      "    266       0.09862       0.13556      0.72753  3.75s\n",
      "    267       0.09856       0.13562      0.72676  3.75s\n",
      "    268       0.09851       0.13552      0.72692  3.75s\n",
      "    269       0.09850       0.13724      0.71771  3.75s\n",
      "    270       0.09854       0.14064      0.70064  3.75s\n",
      "    271       0.09854       0.13552      0.72712  3.75s\n",
      "    272       0.09854       0.14130      0.69739  3.75s\n",
      "    273       0.09862       0.14010      0.70394  3.75s\n",
      "    274       0.09856       0.13466      0.73193  3.75s\n",
      "    275       0.09851       0.13771      0.71539  3.76s\n",
      "    276       0.09852       0.13679      0.72022  3.75s\n",
      "    277       0.09850       0.13661      0.72103  3.76s\n",
      "    278       0.09851       0.13815      0.71308  3.76s\n",
      "    279       0.09850       0.13707      0.71861  3.76s\n",
      "    280       0.09850       0.13909      0.70818  3.76s\n",
      "    281       0.09858       0.14160      0.69620  3.76s\n",
      "    282       0.09858       0.13702      0.71949  3.76s\n",
      "    283       0.09853       0.13639      0.72241  3.75s\n",
      "    284       0.09854       0.13835      0.71226  3.76s\n",
      "    285       0.09852       0.13634      0.72263  3.76s\n",
      "    286       0.09849       0.13653      0.72140  3.76s\n",
      "    287       0.09848       0.13602      0.72403  3.75s\n",
      "    288       0.09849       0.13556      0.72654  3.76s\n",
      "    289       0.09848       0.13520      0.72838  3.75s\n",
      "    290       0.09848       0.13533      0.72768  3.75s\n",
      "    291       0.09847       \u001b[32m0.13432\u001b[0m      0.73307  3.75s\n",
      "    292       0.09847       0.13500      0.72937  3.76s\n",
      "    293       0.09848       0.13580      0.72519  3.75s\n",
      "    294       0.09851       0.13587      0.72503  3.75s\n",
      "    295       0.09849       \u001b[32m0.13417\u001b[0m      0.73402  3.75s\n",
      "    296       \u001b[36m0.09846\u001b[0m       0.13461      0.73141  3.75s\n",
      "    297       0.09846       0.13458      0.73163  3.76s\n",
      "    298       0.09846       \u001b[32m0.13415\u001b[0m      0.73397  3.75s\n",
      "    299       \u001b[36m0.09845\u001b[0m       0.13557      0.72617  3.75s\n",
      "    300       0.09845       0.13502      0.72915  3.75s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x7f953b4f0b90>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x7f953b4f0a50>,\n",
       "     check_input=True, custom_scores=None,\n",
       "     hidden2_nonlinearity=<function rectify at 0x7f953c600938>,\n",
       "     hidden2_num_units=20,\n",
       "     hidden_nonlinearity=<function rectify at 0x7f953c600938>,\n",
       "     hidden_num_units=50, input_shape=(None, 346),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden', <class 'lasagne.layers.dense.DenseLayer'>), ('hidden2', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=300, more_params={},\n",
       "     objective=<function objective at 0x7f953b4fa1b8>,\n",
       "     objective_loss_function=<function squared_error at 0x7f953c0545f0>,\n",
       "     on_batch_finished=[],\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x7f940140a0e0>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x7f940140a2d8>],\n",
       "     output_nonlinearity=<function sigmoid at 0x7f953c5e4f50>,\n",
       "     output_num_units=1, regression=True,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x7f953b4f0c10>,\n",
       "     update=<function nesterov_momentum at 0x7f953c054e60>,\n",
       "     update_learning_rate=0.01, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(float32, matrix))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[('input', layers.InputLayer),\n",
    "            ('hidden', layers.DenseLayer),\n",
    "            ('hidden2', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "            ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, 346),\n",
    "    hidden_num_units=50,  # number of units in 'hidden' layer\n",
    "     hidden_nonlinearity = lasagne.nonlinearities.rectify,\n",
    "    hidden2_num_units=20,  # number of units in 'hidden' layer\n",
    "    hidden2_nonlinearity=lasagne.nonlinearities.rectify,\n",
    "\n",
    "    output_nonlinearity=lasagne.nonlinearities.sigmoid,\n",
    "    output_num_units=1, \n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=.01,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    max_epochs=300,\n",
    "    verbose=1,\n",
    "     regression=True,\n",
    "#     objective_loss_function = lasagne.objectives.squared_error,\n",
    "#     allow_input_downcast=True\n",
    "    )\n",
    "net1.initialize()\n",
    "    # Train the network\n",
    "print(\"training...\")\n",
    "\n",
    "net1.fit(x, y)\n",
    "\n",
    "#record val loss: 0.13931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd8VFXex/HPmTRSSK8kJITQQ1WKoEKwAFbYVRQUWHAf\nFzu7upZdHxVXfVZXRUVdERdRVIqgKLIoETAiHUJoIRQT0nvvpMx5/pjkmpAAIWQIgd/79cqLmXvP\nPXPuJMx3zjm3KK01QgghxLkytXcDhBBCdEwSIEIIIVpFAkQIIUSrSIAIIYRoFQkQIYQQrSIBIoQQ\nolWsHiBKqQlKqSNKqWNKqaebWd9bKbVNKVWplHr8lHV/U0rFKqUOKKW+UErZW7u9QgghWsaqAaKU\nMgHvAeOBcGCqUqrPKcXygEeB10/ZNgS4HxiitR4I2AJTrNleIYQQLWftHshw4LjWOklrXQ0sByY2\nLKC1ztVaRwM1p2xbDFQBzkopW8AJSLdye4UQQrSQtQMkEEhp8Dy1btlZaa0LgDeBZCANKNRab2jz\nFgohhGiVi3YSXSnVHfgLEAJ0AVyUUve0b6uEEELUs7Vy/WlAcIPnQXXLWmIosFVrnQ+glPoaGAUs\nPbWgUkou6CWEEOdIa63OZ3tr90B2Az2UUiF1R1BNAdacoXzDnTkKXKWU6qSUUsD1QNzpNtRaX5I/\nL7zwQru3QfZP9k/279L7aQtW7YForWuVUo8AkVjCapHWOk4pNduyWi9USvkBe4DOgFkpNQfop7Xe\nr5RaAkQDtUAMsNCa7RVCCNFy1h7CQmv9A9D7lGUfNnicBXQ9zbavc8rhvUIIIS4OF+0kurCIiIho\n7yZYlexfxyb7d3lTbTUW1p6UUvpS2A8hhLhQlFLo85xEt/oQlhDi8tOtWzeSkpLauxkCCAkJITEx\n0Sp1Sw9ECNHm6r7dtnczBKf/XbRFD0TmQIQQQrSKBIgQQohWkQARQgjRKhIgQghxjh588EFeeeWV\nVm07duxYPv744zZuUfuQo7CEEJeV0NBQFi1axHXXXdfqOj744IM2bFHHJT0QIYRooLa2tr2b0GFI\ngAghLhszZswgOTmZ2267DVdXV9544w2SkpIwmUx8/PHHhISEcP311wNw1113ERAQgIeHBxERERw+\nfNioZ9asWTz//PMA/Pzzz3Tt2pV58+bh5+dHYGAgn3zySYvao7Xm5Zdfplu3bvj7+zNz5kyKi4sB\nOHnyJNOnT8fb2xsPDw9GjBhBTk4OAJ988glhYWG4uroSFhbGsmXL2vBdajkJECHEZWPJkiUEBwez\ndu1aiouL+etf/2qs27x5M0eOHGH9+vUA3HzzzcTHx5Odnc0VV1zBvffee9p6MzMzKSkpIT09nf/8\n5z88/PDDFBUVnbU9ixcvZsmSJfz8888kJCRQUlLCo48+CsCnn35KcXExaWlp5Ofns2DBAhwdHSkv\nL2fOnDmsX7+e4uJitm3bxuDBg8/znWkdCRAhxAWnVNv8tNapJ9YppXjxxRdxdHTEwcEBgJkzZ+Lk\n5ISdnR3PP/88+/fvp6SkpNn67O3tee6557CxseGmm27CxcWFo0ePnrUdS5cu5fHHHyckJAQnJyf+\n+c9/snz5csxmM3Z2duTl5XHs2DGUUgwZMgQXFxcAbGxsOHjwIJWVlfj5+dG3b9/Wvxnn4ZIJkF+S\nfmnvJgghWkjrtvlpS0FBQcZjs9nMM888Q48ePXB3dyc0NBSlFLm5uc1u6+Xlhcn028epk5MTpaWl\nZ33N9PR0QkJCjOchISFUV1eTlZXF9OnTGT9+PFOmTCEoKIhnnnmG2tpanJycWLFiBR988AEBAQHc\ndtttLQora7hkAuTuVXfz+la58rsQ4szUabouDZcvXbqU7777jk2bNlFYWEhiYmKb3oipXpcuXRpd\nMywpKQk7Ozv8/PywtbXlueeeIzY2lm3btvHdd9+xZMkSAG688UYiIyPJzMykd+/e3H///W3arpa6\nZAJk9/27+Xjfx7y78932booQ4iLm7+9PQkJCo2WnBkNJSQkODg54eHhQVlbG3/72t9MGz/mYOnUq\nb731FomJiZSWlvLss88yZcoUTCYTUVFRHDp0CLPZjIuLC3Z2dphMJrKzs1mzZg3l5eXY2dnh4uKC\njY1Nm7etJaweIEqpCUqpI0qpY0qpp5tZ31sptU0pVamUerzB8l5KqRil1N66f4uUUo+d7nUCXQNZ\nO3Ut/9j8D+JyTnvnWyHEZe6ZZ57hpZdewtPTk3nz5gFNeyUzZswgODiYwMBA+vfvz6hRo87pNc4U\nNg3X3XfffUyfPp3Ro0cTFhaGk5MT8+fPBywT83feeSdubm6Eh4czduxYpk+fjtlsZt68eQQGBuLt\n7c3mzZvb7bwUq16NVyllAo5huZ95OpZ7pE/RWh9pUMYbCAEmAQVa63mnqScVGKG1TmlmvXE13gV7\nFrAoZhHb/7gdW5OcJylEe5Cr8V48OvLVeIcDx7XWSVrramA5MLFhAa11rtY6Gqg5Qz03APHNhcep\nZl85G09HT/75yz/Pp91CCCHOwtoBEgg0/NBPrVt2ru4GWnSmjFKKRbcv4t1d7xKTEdOKlxJCCNES\nF/0Yj1LKDrgdeOZM5ebOnWs8joiIYN74ecz4ZgbRf4rG3sbeuo0UQoiLXFRUFFFRUW1ap7XnQK4C\n5mqtJ9Q9fwbQWuvXmin7AlBy6hyIUup24KH6Ok7zOk3uSKi15vbltzOsyzCeH/N8G+yNEKKlZA7k\n4tGR50B2Az2UUiFKKXtgCrDmDOWb25mptHD4qlFFSvHBLR/w7q53OZR96Fw3F0IIcRZWvye6UmoC\n8A6WsFqktX5VKTUbS09koVLKD9gDdAbMQCnQT2tdqpRyApKA7lrr5q8hwJnvif7hng/5eN/HbL1v\nqxyVJcQFIj2Qi4c1eyBWD5AL4UwBYtZmbv7iZgb5DeK1G5uMnAkhrEAC5OLRkYew2p1Jmfj895+z\n9NBSfjrxU3s3RwghLhmXfIAAeDt58/7N7zN77WxKTp52JEwIIU6r/r4f9fr378/mzZtbVPZUJpOp\nyeVUOqLLIkAAbu99Ozf3vJle7/Vie8r29m6OEKIDangZkkOHDjF69OgWlT2XdR3JZRMgAG9PeJu3\nxr/FA/99gFqz3LZSCNE+LpX5ocsqQADuDr8bj04ezPlhjgxnCXGZ+de//sXkyZMbLZszZw5//vOf\nAcutYvv164erqys9evRg4cKFp60rNDSUTZs2AVBZWcnMmTPx9PSkf//+7N69u8VtKi4uZsaMGfj6\n+hIaGsorr7xirIuPjyciIgJ3d3d8fX2ZOnWqse4vf/kLfn5+uLm5MWjQoEa33L1QLrsAUUqx4s4V\nlFSV0Of9Pvx7978pqCho72YJIS6AKVOm8P3331NWVgZYbhy1cuVK43a1fn5+rFu3juLiYhYvXsxf\n/vIX9u3bd9Z6586dy4kTJzhx4gTr16/n008/bXGbHnnkEUpKSkhMTCQqKoolS5awePFiAJ577jnG\njx9PYWEhqampxu1uIyMj2bJlC7/++itFRUV8+eWXeHl5nevbcd4uyxMj/Fz8+HTSp+xM3cnr217n\nxZ9fZOXklYwOOf14phCi7agX22YOQL9wbkNBwcHBXHHFFaxevZpp06axceNGnJ2dGTZsGAA33XST\nUfbaa69l3Lhx/PLLL2e95/jKlStZsGABbm5uuLm58dhjj/HSSy+dtT1ms5kVK1Zw4MABnJycCAkJ\n4YknnuCzzz5j1qxZ2NnZkZSURFpaGoGBgcZl5e3s7CgpKeHw4cMMHz6c3r17n9P70FYuywCpNyJo\nBKvuWsWGhA3c8eUdfHP3N1wdfHV7N0uIS965fvC3palTp7Js2TKmTZvGsmXLuOeee4x133//Pf/4\nxz84duwYZrOZiooKBg4ceNY609PTG90St+Ftas8kNzeXmpoagoODG22blpYGWIbcnnvuOYYPH46n\npyePP/44s2bNYuzYsTzyyCM8/PDDJCcn8/vf/5433njDuGf6hXLZDWE154buN/D57z7nji/vIK04\nrb2bI4SwosmTJxMVFUVaWhqrV682AqSqqoo777yTp556ipycHAoKCrjppptaNOEdEBBASspvFx5v\neJvaM/H29jZ6GQ23DQy0XLTcz8+PhQsXkpaWxoIFC3jooYeMw38feeQR9uzZw+HDhzl69Civv37h\nb+ktAVJnfI/x/OnKP/HwuocvmSMkhBBNeXt7M2bMGGbNmkX37t2N4Z+qqiqqqqrw9vbGZDLx/fff\nExkZ2aI677rrLv75z38acxXvvfdei7YzmUzcddddPPvss5SWlpKUlMRbb73F9OnTAVi1apXRG3F3\nd8dkMmEymdizZw+7du2ipqYGR0dHOnXqhMl04T/OJUAaePbaZ4kviOfN7W+2d1OEEFZ0zz33sHHj\nRmPyHMDFxYX58+czefJkPD09Wb58ORMnTjxtHQ3P5XjhhRcIDg4mNDSUCRMmMGPGjDO+fsNt58+f\nj5OTE927d2f06NFMmzaNWbNmAbB7925GjBiBq6srkyZNYv78+XTr1o3i4mLuv/9+PD09CQ0Nxdvb\nmyeffLK1b0erXfLXwjpXqcWpXLv4Wh4c+iBPjnrykjnhR4gLSa6FdfGQa2FdQEGuQfwy6xc+P/A5\n7+x8p72bI4QQFy3pgZzG8bzjjFw0kv0P7CfQtTV34RXi8iU9kIuH9EDaQU+vnvxxyB95batcAl4I\nIZojAXIG9195P1/GfkmNuaa9myKEEBcdqweIUmqCUuqIUuqYUurpZtb3VkptU0pVKqUeP2Wdm1Jq\npVIqTikVq5QaYe32NtTDswdd3bqy6cQmKmsqL+RLCyHERc+qAaKUMgHvAeOBcGCqUqrPKcXygEeB\n5s6CeQdYp7XuCwwC4qzY3Gbd0/8eJnw+gas/ljPUhRCiIWv3QIYDx7XWSVrramA50OjAaq11rtY6\nGmg0TqSUcgWu1VovritXo7UutnJ7m3hsxGPkPZVHXnke+zLPflE1IYTlchxKKfm5CH5aelmV1rD2\ntbACgZQGz1OxhEpLhAK5SqnFWHofe4A5WuuKtm3imdmYbPBw9OAPg/7AJ/s+4e0Jb1/IlxeiQ0pM\nTGzvJogL4GK+mKItcAXwsNZ6j1LqbeAZ4IXmCs+dO9d4HBERQURERJs2ZsagGYz6eBRvjnsTG5NN\nm9YthBDWFhUVRVRUVJvWadXzQJRSVwFztdYT6p4/A2itdZNjY5VSLwAlWut5dc/9gO1a6+51z68B\nntZa39bMtm1+HkhzBnwwgIW3LuSqoKvkDHUhRIfWEc4D2Q30UEqFKKXsgSnAmjOUN3ZGa50FpCil\netUtuh648LfcauD2Xrfz/u73CZsfRlZpVns2RQgh2p1Vh7C01rVKqUeASCxhtUhrHaeUmm1ZrRfW\n9TT2AJ0Bs1JqDtBPa10KPAZ8oZSyAxKAWdZs79nc3vt2rlp0FQ42DuzL3Mf4HuPbszlCCNGu5FIm\n58CszSyOWcz+rP0EuwXz11F/Pe86vz/+PRN6TJAhMSHEBdURhrAuKSZl4o9X/JFBfoM4mH3wvOvT\nWjNpxSRyynPaoHVCCHFhSYC0wgC/ARzMOv8AKasuo6q2irKqsjZolRBCXFgSIK3Qz6cfR3KPkFWa\ndV7XycqvyAegtKq0rZomhBAXjARIK7jYu9Clcxe6zOvCkv1LWl2PBIgQoiO7mE8kvKh9M+Ubfoz/\nkfXx66msqaTkZAlPX9P4WpHl1eU42Tmdtg4JECFERyY9kFbq79ufyeGT2ZiwkXd2vsOOtB1Nygz5\ncAhJhUmnrUMCRAjRkUmAnIcg1yB8nX1JK07jRMGJRuu01iQWJpJRmnHa7SVAhBAdmQxhnadpA6dh\nb2PPS5tfQmttnM9RWFlIVW2VERLNkQARQnRk0gM5T3+/9u88MfIJgEZhkVVmudRJXnneabfNK8/D\npEwSIEKIDkkCpA0opeju0Z2EggSSi5IByCzNBCCv4vQBkl+RT5fOXSRAhBAdkgRIGwl1D+XVra9y\n0xc3ARgXWzzjEFZlPsFuwRIgQogOSeZA2kioeyjzdszD1mRLVW0VmaWZ2JpszziElV8hASKE6Lik\nB9JGunt0x9PRkyDXII7lHSOrLIteXr3OOoQV4hZCabUEiBCi45EeSBsZFzYOt05ufB33NYdzDpNZ\nmkk/n35nPQor2C2YuNy4C9hSIYRoG9IDaSM9vXoybeA0wn3Cic2OJassi3Cf8EY9kLXH1vLalteo\nqK7go+iPyCvPI9gtmJKTJc3WmVuey4zVMy7ULgghxDmRAGlj4b7hxObENtsD2Zy0mXW/rmNL8hYe\nXvcwtiZbfJx8TjsHsi9zH6uPrL5QTRdCiHNi9QBRSk1QSh1RSh1TSj3dzPreSqltSqlKpdTjp6xL\nVErtV0rFKKV2WbutbSHcJ5zd6btJKUqhn0+/RpPox/OPE5sdy8Hsgzww9AEOP3wYF3uX0wbIkdwj\nlFaVUl5dfqGaL4QQLWbVAFFKmYD3gPFAODBVKdXnlGJ5wKPA681UYQYitNZDtNbDrdnWthLuG87U\n/lMB6OHZg4qaCqpqqwA4lneMvIo8Np7YyADfAQS7BZ8xQI7mHgUgp0xuOCWEuPhYuwcyHDiutU7S\nWlcDy4GJDQtorXO11tFAczfWUBegjW3KpEy8esOrZD+ZTSfbTnh08qCgooBacy0JBQkM8R9CZHwk\nA/wGAJy5B5J3BIDssuwL1n4hhGgpa384BwIpDZ6n1i1rKQ38qJTarZS6v01bdoF4OXnxyi+v8N2x\n7/B28mZYl2HUmGsI9wkHmgZIaVUpxSeLAUsPpLdXb+OyKEIIcTG52A/jvVprnaGU8sESJHFa6y3N\nFZw7d67xOCIigoiIiAvTwrPwc/Zj9ZHVLDu0jAG+A+jn049Q91A6O3QGwN7GHo2mqrYKext7ntv0\nHAAvX/cyOeU53NnvTumBCCHOW1RUFFFRUW1ap7UDJA0IbvA8qG5Zi2itM+r+zVFKrcYyJHbWALmY\nrJy8Eic7J3q914teXr0YGzqW3PJcY71SyuiFeHTyYPWR1fT06smxvGP08OxBgEtAowB5dN2j3Nrr\nVsb3GN8euyOE6KBO/WL94osvnned1g6Q3UAPpVQIkAFMAaaeobwyHijlBJi01qVKKWdgHHD+e3yB\n+Tj7ADB/wnxcHVwZ6DeQgX4DG5WpD5DkomSKTxZzOOcw0RnRDPYfjJ+zH2kllsw1azPLDi3DzsZO\nAkQI0e6sGiBa61ql1CNAJJb5lkVa6zil1GzLar1QKeUH7AE6A2al1BygH+ADrFZK6bp2fqG1jrRm\ne63pjn53nHadi70L01dPp/hkMTMHz+SjvR8RGR/J6JDRuDm4EZMZA8D+zP0UnSxie+r2C9VsIYQ4\nLaW1bu82nDellO7I+7E9ZTs55TmUV5czJmQME5dPZH/WfrbM2kJ+RT5vbn+TB4Y+QHR6NKklqaw6\nvIr8p/JxsHVo76YLIToopRRaa3X2kqd3sU+iXxZGdh3Z6Hk/n34cyDrAIP9BxGbHsjdjL9O+nkZF\nTQWrJq/iQNYB9mXuY0TQCL6M/ZL/++X/WHbHMvr69G2nPRBCXI461DkWl4t+Pv0YEjAEext7fJ19\nyavI46mrn+LggweZ2GciY0LGMOObGdy69FYeWfcId/S9gzGfjOHH+B8BeDHqReMkRCGEsBYZwroI\nxefHczz/OBN6TKCqtopub3cjZnYMfi5+AJysOUl0RjS55bn09e5LT6+e/JL0C7cuu5XEOYkMXDCQ\nf0T8g1lDZrXzngghLlYyhHWJCvMMI8wzDLCcJ5L2eBpK/fZ7drB1YFTXUY22uTbkWob4D+GHX38g\ntTiV4/nHL2ibhRCXHxnC6gAahseZDA8czsK9CwH4Nf/XRuu01uzP3E9iYWJbN08IcZmSALmEDA8c\nTlRiFKO6jmoSINEZ0Vyz+Br6/7s/ZVVl7dRCIcSlRALkEjIicAQAd/S9g+P5x2k4L7Q9ZTtT+0/l\nioAr2Jqytb2aKIS4hEiAXEKCXIMIcg3ihu434GDj0OgSKLvSdzEicAQR3SKISow6a11VtVU8uu5R\niiqLrNhiIURHJgFyCVFKsf+B/Qz0G0gPzx4s2b+EZQeXkVuey87UnYwIsgTIphObWHts7RlvVDU3\nai7v7X6PHak7LuAeCCE6EgmQS4ynoycA/X37syhmEctjl3PlwivJLM2kr3dfRgaNJCYzhnu/vpdn\nNjyDWZs59RDolKIUFuxZwPSB04nOiG7yGlpr0kvSL8j+dETpJemYtbm9myGE1UmAXKIW3LqAww8f\n5tsp3/LIsEcYFzYOG5MNjnaOHHjgAPGPxfNV3Fd4vubJ/276X6pqq4z7ty/Zv4S7wu9iQo8J7M3Y\n26TuqMQorvn4mgu9Sx3G3avuZnPS5haV3ZK8pckBD8K6Vhxa0eRLk2gdCZBLlK3JFpOy/HqfvPpJ\nVt21yljX27s33k7eRP8pmqiZUXwY/SHXfXodgxcMJqUohcX7FnPfkPu4MuDKZgPkx4QfOVF4otFl\n6cVvssuyW3wb4g/2fMB3R7+zcotEvaraKqZ8NYWSqhJjmVmbpcfYShIglzF/F38G+w/mpbEvMchv\nENMHTif0nVCuCLiCYV2G0dOrJ7nluUbPpN6GhA24ObgRnd50eOtyVFFd0eh5XnkeeRV5jZYVVRax\nYM+CJtvmludSUFlg1faJ39R/6Sk5+VuAvLrlVV7b8lp7NalDkwARPDjsQd6/5X1evu5lcp/K5cvJ\nX6KUwqRMjO8xniEfDiH4rWBu/uJmNp3YxJHcI/xh0B/Yk77nnF7nZM1JwHLb3qraKmvsygUXlRjF\niP+MMJ6btZmCyoImobs7fTcvb365yfY5ZTmNylbVVrHm6JpGZdb/up6ITyL46vBX59XWu1beddmf\nSFp/ZGLDHkhSYRIHsw+2us6v476mxlxz3m3riCRAhEEphXsn90bLvrzzS9ZPW8/GGRsZ220sj35v\nuSPiqK6j2JOxh5M1J1mwZwE7Unfw04mf+NuGv7H22NomdR/KPkTgvEC+P/49M7+Zady691yZtZlJ\nyyc1+gZ5JtY+aTIyPpKD2QeJz48HoPhkMWZtJq+8cQ/kRMEJMkozmnzQ5JQ3DpBtKduYvHJyo3a/\nvfNtssuyjfvCtNaW5C0kFCScVx3WtD9zf4uH/lqrPkBKq0qNZbkVuec1D/XA2gcu6vfVmqweIEqp\nCUqpI0qpY0qpp5tZ31sptU0pVamUeryZ9Sal1F6l1JpT1wnrU0rRx7sPPb168uTVTxL7UCxL71jK\n0C5D2ZCwgb7v92XV4VVMXz2dpzc8jVmbmfXtLB5f/zgjF43knq/uYfZ3s4n4JIKIbhG8/MvLrDu+\njhWxzU9k1ppreez7x9ifub/Z9hzMOsi3R79tce9n5KKRLI5ZfE77XFBRQFxOXIvK/pT4Ez09exqh\nWR8c+ZWNeyAJBQmYtZmMkgxjmda6yRDWztSdVNVWNZqEzy3PZXjg8CahdC7qX+t86rC2l395mVWH\nV5294HkweiANvoDklOWcNkB2pO6gurb6tPVprSmsLKSgwjrDkNtStrHu+Dqr1N0WrBogSikT8B4w\nHggHpiql+pxSLA94FHj9NNXMAQ5brZGiVcI8w/hu6nd8OflLfpz+I8cfPc6u+3fx2o2v8dFtH1FQ\nWcDLY19mQo8J9PLqxc7/2cmnkz7lUPYh7u5/N51sO7EhYQNHc49SXVvN8kPL8X3dl2sWX8PXcV/z\n2A+PNRswkfGRKBQ703aetY1JhUnEF8TzfNTzTeYp6q09trZJr+C9Xe8xacUkas21zW7z7s53WbJ/\nCSUnSziYdZAXI15kzTHL95v63kReeR5VtVVEp0eTXpLOicITAKQWpxr1lFWXUVlT2agHsiNtBwN8\nB/Bjwo/GstzyXPp49yG3ovUHLRSdLKLaXN1kbuZiUlBRYNy+uS3sStvF4ZzGHx31PZyGQ1j1IX7q\nsCPArG9nnfHLSnl1OdXmagorC8+pbYv2LuKNbW+ctdyyg8tYeXjlOdV9IVn7arzDgeNa6yQApdRy\nYCJwpL6A1joXyFVK3XrqxkqpIOBm4BWgSe9EtK+IbhHNLp/UZxKT+kxqdt27N73L8MDhLD+0nNuW\n3Ya/iz+pxan08urF8juXE5MRwx8G/4Gxn45l0IJB9PHuwx8G/YHc8lw+3vcxlTWV3N3/bnal7QIs\nHwgu9i442DpQWVOJk52T8Vrr49czqc8kyqvL+TD6Q8aHjaewstC4gVfxyWJuW3Ybjw5/lPk3zTe2\n25a6jdTiVNYcXcPv+v6uyT6sObaGA1kHKKos4souVzKxz0Tm/jyX+Tvn09urN052TuRX5PPCTy+w\nIHoBY7uNJbU4lQCXgEYBUj+hW//tVWvNjtQdzJ8wn7k/zzXK5ZTl0Me7D+vj15/1d7IydiW2Jtsm\n7a7/5n2x9EDKq8t5dcur/GPsP9iQsIGx3cZSWFnYpgHyxrY36OnZk1euf8VY1twQVk55Dl1du/Jr\n/q8MDxxuWVaWg4+zD3nleeSUn35YrT44GgbI5qTNxOfHN3s7haTCJAI6B3Ag6wCrj6zm8ZGPG0dL\nNudA9gEcbR1buMcXnrWHsAKBlAbPU+uWtdRbwJOAHLR9iZgxaAZ9vPvwv6P/l/yn80n8cyJVz1Vx\n+OHDXBd6HU+MegJvJ29++sNPLLp9EdeHXs/Lv7zMa1tfY4DvAPZm7OXpq59mZ9pOntv0HMFvB/M/\n3/0PT0Y+yejFo9mfuZ9xn43jZM1J1sevZ3zYeP5+zd+Zt30ed6+6myd/fNJoS1xOHH28+xhDYlpr\nzNrM9pTtvHbDa/x7z7+b3YfY7Fh6evbk5V9e5s1xb+Jk50TktEj+vvHvJBcl08OzB3kVeRzNO8o/\nIv7BT4k/EV8Qz+iQ0Y0CJKcsh2C3YOObb0pxCmZt5vd9f098fjyVNZVUVFdQVVtFqHtoiw6b/inx\nJ35O+hmwTMhvOrHJeC3ggvRAKmsqjcfZZdnN9iSP5R3jX1v/hdaaqV9N5UjuEQoqC0grbrsAicmM\nMXp+DdvgsDwsAAAgAElEQVRja7I1hrBqzbUUVBQwPHA4x/Mst0DIr8gnbH4YZm0mvyLfeO8ySzNJ\nLkpuVF9zAfLTiZ94IeqFZg8Nnr56OpHxkeRW5JJSnMJPJ346bfu11hzIOkBGacZpy7S3i3YSXSl1\nC5Cltd4HqLofcYmwNdkavYXmvoF5O3kzLHAYs4fOZvsft3P44cO8e9O7xD8WzyC/QVTVVrHhxAZi\nH4ple8p2lh5ailKKsZ+OJS43jj+u+SM/J/7M+LDxDAu0HJLs4+xDbE4sWaVZAMTmxDKsyzDu7Hsn\n/z32X+5bcx/3fHUPPs4+3DvgXnak7iC/Ip9rPr7GuMNjfkU+pVWlrL1nLQcfPMjQLkMBCHEPIdQj\nlJ1pO+np2ZP8inziC+K5OvhqglyDqKiu4MqAKxsHSHkOPT17UlhZiFmb2Z22m2FdhmFjsqFL5y6k\nFaeRV5GHt5M33k7eLeo9ZJZmGt+yNydtZsbqGcZrgfUDJD4/nisXXmk8n/D5BLYkb2lSLqUohZO1\nJ8kuy7bMzVTkUVhZ2Oj9OR9FlUX8mv9r0wApz6abezejB1JQWYCrgyt9vPsY8yAJBQmUVJWQVpxG\nra41gvu9Xe8Zw05FlUW8GPWiERwN57HSS9JJKU5p9ppz8QXxZJZmkluey/Wh159xeCq1OJXKmsqL\n+qoP1h7CSgOCGzwPqlvWElcDtyulbgYcgc5KqSVa6xnNFZ47d67xOCIigoiIiNa0V1zElFIEu1n+\nnL6+62sG+w+ms0NnvrrrKypqKnC0deSjvR/x56v+zFX/uYovJ39p3MVx6e+XYm9jz0PrHuLVLa8y\n2H8wh3MO08+nH8O6DOPhdQ+TVZZFJ9tOjAsbh4ejB11du/L2jrdJKEhg/OfjWTN1DUWVRYT7hjc5\nWg2gt1dvtqdu57Zet/Ht0W8pry6nu0d3rg+9HrM209WtK7vTdxvlc8tzCegcgJOdEyUnS4jJjOGK\ngCsAy4UxU4pTcO/kjo+zD15OXuSW56K1PuP9YbLKsowhj91pu40jv7LLsi23Rz7Pifi3drzFxzEf\ns+7edcbvYmvyVmrMNYzpNsbyoV1wwmhnWkka0RnRXBtybaO66r/J78vcB1h6BoWVhc0eDqu1ZspX\nU1g8cXGjIcoz2Ze5jxC3EE4UNO2BdPfobsyB1A9VdffobvTc6o+oOpJrGWmvD9/4gnijN7Xy8Epe\n3/a68QWiYQ8kvTSdiG4RfHHgC64Lvc5YXlFdQXpJOlmlWeSW53JLz1uavVRQvYPZB7m669VsTtpM\nVW0V9jb2Ldr304mKiiIqKuq86jiVtQNkN9BDKRUCZABTgKlnKG/8z9Ba/x34O4BSagzwxOnCAxoH\niLj0NfxAGhIwxHj871ssw045T+ZgY7IxltcHydT+U3ns+8dYvG8xYZ5hzB0zl2uCryG1OJW7w+9m\nzlVzsFGW7UZ1HcXbO97mxYgX8XH24fol1zM+bDz9ffo326ZeXr34Ku4r7r/ifjrZdsLOZId7J3d+\n3/f3nKw5SZBrkPENe8WhFSQUJODj5IOHowf5FfnEZMbwP0P+B8AoW2OuwdvJm062nXCwdaCkqgRX\nB9fTvi9ZpVk42lkCZFf6LszaTHpJujGPkleRR2VNJbYmW2xN5/bf/1jeMV7f9jq39ryVv2/8O5//\n/nMAlh9aTnJxMmO6jSG5KJmKmgqKTxbjYu9CbnmuERINpRRbRrbrr3SQWJiIk50T1bXV5FfkY29j\nj4u9C2D50P8y9kueGvUUV3a5skldzdmbsZebe97M4n2LKa8uN4InpyyHK/yvMIawcstz8XbyJsg1\nyBg+qw+Qo3mWXmd9gCQUJBht+uzAZ5RVlxm/z0YBUpLOHwb9gaUHlzZqU/05OPW9rr7effnh1x9O\nuw8Hsg4w2H8wcblxZJVm0dWta4v2/XRO/WL94osvnld9YOUhLK11LfAIEAnEAsu11nFKqdlKqT8B\nKKX8lFIpwF+AZ5VSyUopF2u2S1z6GoZHQ7f3vp3EPydyR9872Juxl34+/XCwdeDZa5/l8ZGPM9Bv\nIOG+4QCMDBpJSVUJN/W8iWkDp/HJxE/44uAXxvpT9fLqBVguaOnp6Gnclnh0yGg+uPUDo1eRU5bD\nvV/fy793/xtvJ288HT0pqCwgJiPGCMOurl1JLU4lpywHbydvwDKsVz+ckl2WzR1f3mGcnFmv4RDW\n7rTd+Dn7kVKUQk55Dn29+5JXnseff/gz7+96/5zf05jMGEZ1HcVbE94iMj7S+HafWJTIphObqK6t\nNnoWmaWZ5FXkYdbmZgMkuSgZW5MtezMtAXI87zgenTwIdA1kzg9z+MsPfzHK1h9JdeoRVWeyO303\nQ7sMJdgtmKTCJBILE/ndit+RVZZFmGeYMYSVU56Dj5MPgZ0DjQl8I0Dqhi3r50ASChLILssmuSiZ\n2OxYAjsHEpcbZ/z+6qWXpDM8cHiT4bj6erPKLD2Qvj59zzhBvyd9D0P8hxDgEnDRzoNYfQ5Ea/2D\n1rq31rqn1vrVumUfaq0X1j3O0lp31Vq7a609tdbBWuvSU+r4WWt9u7XbKi4ff7ryTzjZOdHNvRsA\nf7v2b02CIaJbBMO6DKO3V28Abul1C+/e9C639bqt2TrrA8TL0QsvRy+6e3RvtL6ra1ccbBx4eN3D\nhLiHGB9eno6exOXEUVlTSYhbCFA3hFWUYvmG7Ng0QJYeXMrquNV8GP2hUX9ZVRk15hryK/JJK06j\nvLqca0OuJaU4heyybKMHEp0Rza70XWitWXFoRbNnyDcnJiOGIf5DcLF3YZD/IOMbemJhIiZlYnvq\ndqNnkVGaQVZpFt09unM072iTKw+kFKcw0G+g0QP5teBX3Du5E9g5kOWHlnM8/7hRNjYnFpMytThA\nqmqrjAMourl340ThCfZl7uObI99g1mYCXAKMIazc8lx8nHyMOSewfNAHdg7kSN4Rgt2CySnPofhk\nMbnlueSU5bAvcx8jgkYQ7BZMXG4c3dy7GT2QGnMNeeV5DPYfTHZZdqMhuYSCBHp79SaxMJEacw3d\nPbo3e+JkdW01Wmu2p25nZNeRBHQOIKMkg8qaSubvnN+kfHu6aCfRhbCmEUEjiHs47rQ9FbCc67Lr\n/l2N5hweGf6I0bM41ak9kO7ujQPExmTDK9e9wsrDK5k3bh7BbsF4O3nj0cmDTSc2Mdh/sPFaQa5B\npJakGkMsYAmm+jmMT/d/yrzx83h588vGMEhWWRb+Lv64ObixPn695Ru4a7DRA+nl1YuiyiIOZR8i\nJiOGpQeX8uymZ/nX1n816cnU01obH/4xmTEM9h8MQDe3biQVJqG1JrEwkXsH3EtkfCTJRcl4OnqS\nUZJBVlkWIW4hdPfo3uTDP7komeFdhpNQkECoe6ilB+Jo6YF0su3UaPI7NjuW0SGjOZzbfIBU1Vax\nMva3yehNJzbRx7sPga6BhLqHcqLgBMlFyUwfOJ33b36fzg6dG82BeDt5497JnRpzDSUnS0goSGBE\n0AiO5B6hl1cvcstzjQ//vIo8kgqTCHYNJqBzAEdyjzQKkKzSLGPI0cfZp9GJowkFCYzsOpK43Di8\nnbzpbN+ZGnMN5dXljY5UG/bRMP6z9z/UmGsIdQ8lwCWA9JJ0tqds588//PmiuslbiwJEKTVHKeWq\nLBbVnRk+ztqNE8Ka6ieB24qXoxcenTzwcvLCz8WPnl49m5SZHD6Zt8e/zU09b+K/9/yXcWHj8HT0\nZNmhZdza67dTobq6WYawcstz8XH2AX7rgcRkxJBXnsdjIx5j8cTFTPt6Gkdzj5JVagkQPxc/IuMj\nGew/2Bg2yy7LJsAlgM4OnfF28iaxMJElB5bw/Jjn6evTl20p25rdpwfWPsAT659Aa01MpqUHApaj\nzhILEymoLMDWZMtd4XcZATI8cDiZpZlklWbh5+LHYP/BxjCW1pry6nIySjKMCeiBfgNJLU7FvZM7\nYR5hPHDlA2SWZhpngB/OPcydfe9sEkI7Unfw793/JiYjhpnfzsSszRRWFvLR3o+4s++dAHT36E5C\nQQLJRcmE+4Rz35D76Gzf2RjCqn9/lVIEugaSXJRMWkkaQwOGklqcSh+vPuSU5ZBQkEAf7z442zmz\nP2s/wW7B+Dv7k1yUTKh7qBEg6SXpdOncBfjtQIh6CYUJjAwaSfHJYrydvFFK4ePsQ3R6NEM+HEJl\nTSXH846zP2s/T0Q+wcigkSiljCGsrSlb0egWnUR7obS0B3Kf1roYGAd4ANOBV63WKiE6IKUUUTOj\n6OnZk/kT5nPvgHublDEpE3OumoO9jT39ffvjbO+Mp6Mn7p3ceWDoA0Y5YwirIrfJHMirW19lzog5\nmJSJW3rdwphuY9iXuY/M0kz8XPzwdfZl44mNDPIbRFe3rqQUp5BVmmU5msvRiysDrqSvT182JGzg\nph43Ma77OCLjI43Xfnnzy5woOMGGhA18duAzdqfvJq0kDYUyPhy7uXcjqcgyt9DNvRsjg0ZyJPcI\nyUXJDOsyjIzSDLLLsvFz9mOw328BsihmEaHvhOLl5EWIu2W4boDvADQaj04ePD/mef55wz8JcAkg\nuSgZrTWx2bHc3vt2UopSGl1R4Nsj3/LFwS84nn+c8upykgqTGLRgENW11UwbOA2Avt59icuNI7ko\n2Xg9F3sXSk6WUGOuYeOJjfTxtlwcI7BzIFtTtuLv4k+gq+V0tRD3EKrN1RzKPkR3j+74OvuyN2Ov\nJUBc/AEIdQ+loKKAV7e8yqKYRcZ7VD+PVe9Y3jGGdRmGSZmM36mPkw8bEjawP2u/5ei24+u4Z8A9\n2NnYMTLIcsJr/RDW1pSt9PHuw/aU7a3467SOlgZIfR/+ZuAzrXUscl6GEE0M9BuIUgovJy8cbB1a\ntM24sHG8f/P7jQ5R9XX2pehkEbHZsY0CZPWR1fx04idmD51tlO3v059D2YfIKsvCz9kSILnluQz2\nH0xX16788OsP+Ln40aVzF7ycvBjgO4DBfoO5KugqfJx9uDHsRiITLAGyL3Mfz/30HO/sfIf/3fS/\nvDPhHWJzYtmRuoOhXYYaQ2whbpYeSGJhIiFuITjYOjA6ZDRundzo7tHd0gMpy8LX2bdRD+TzA58z\nOmQ0I4NGEuASYLxnAO6d3I2jw0I9QjlReIKEggRsTbYEuQbR27t3owtK7snYQ1xOnHEC4Npja6k1\n17Jm6hqj19bftz8Hsw+SXJRs9Dg7O1h6IPN3zsfPxY+betwEQKBrICtiV1jeFyfL9l6OXvg4+bDu\n+Dr6evfFx9mHQ9mH6OrW1QiQEPcQCisLWRSziA+jP2zUA6kPkOyybDJKMhjgNwAfJx/jd+rr7Mu2\n1G2MDBrJS5tf4qO9H3FH3zv4dNKnRgiOCBzB2uNr2ZayjSdGPsG21G0XzdV/W3ocX7RSKhIIBf6m\nlOoMyB1YhGgDDc8VqGdSJh4d/ig55TnGB+wtPW8hrzyP58c8bxxOChDuG87yQ8uxNdni7+KPg40D\nDjYO9PbuTXZZNpU1lbxx4xuYlIkg1yCGdhmKi70LxSeLAbgq6Cp+zf+VnLIc/u+X/+ORYY+wMHoh\nXTp34b4h9/Hizy+y9OBSrgn+7S6UIe4hjXogAOPDxpNRmmEMudiYbOjp2dMIkLTiNMuZ1U9k4GDr\nYMznDPAbAIBHJw+j/vq5i+j0aCb1mYRSijv63sHyQ8sZ1XUUWmui06OprKlkS8oWfJx8+GT/J8al\nSBq2s7CykOKTxb8FiL1lDuTjmI9ZdPsiIxQDOwey7OAyFty6wAggT0dPvJ28icuN4+7+d/Pf4/+l\n2lxNsFuwMRfh7+KPjcmGnLIcrgu9znidrq5dSSmyDGFtOrGJ0SGjsTXZ4uvsaxwY4ePswzdHvuHV\n618lxD2Er+O+ZlzYuEa/3yEBQ7j/ivtZEbuCib0n8tB/H8LpFSfemfAODw57sIV/ZdbR0gD5IzAY\nSNBalyulPIGmF3oRQrSZN8Y1vtjekIAhjc55qRfuE86h7EP4OvsywHcA9jb2hPuGY2uyJcAlgE0z\nNjE2dCwAX/z+CxxsHBodGGBvY8+YkDF8duAzfkz4kZS/pJBems7NPW7GxmRDf9/+rDm6hjkj5hjb\ndOnchZyyHA5mH2SgryXgpg6YagztZJZmYm9jj5+LHz7OPrjYu/Dspme5vfftRs/M09GTO/vdSQ/P\nHtiabBudnBnqbumBRMZH8toNlps93TPgHq7++GreHPcmKcUpONk5Ee4bzs+JPzNz8EwWxSxicr/J\njd4bkzIR7hPOnvQ9Ro/Hxd6FvHLLme8N38/AzoFoNDd0v8E4R8bT0RMfZx/GhY3D1cEVX2dfFIrA\nzoHGEVTundxx7+TOIL9BrLprlXFlhSDXIHak7QBgY8JGrg+9HrCck9RwCKu0qpReXr24MezGRvNg\nDc2NmMtDwx7Cx9mHH6b9gJejF7csvQVPR0/u7n93o7Jaazad2ERSURIzB88847W2zldLA2QksE9r\nXaaUmgZcAbxjtVYJIVqsp1dPUopT+Drua+aMmMOBrANcG2w50VIpZYQHQCfbTs3WcWP3G3lm4zPc\nO+BeXOxdWDV5lREyA3wHsPHERoYFDjPK25psCXQN5KvDXzF3zFzAMsQ2sc9EssuySStOw6RM+Dr7\nAjDYfzDfHv2W/Q/8dpl+pRQrJ680tm0YIN3cu/HcT89RUVPBmG5jAOjh2YMwjzC+jP0SOxs7hnYZ\niq+zL1uStzCx90QWxSxq0gOpb39maaZxxJ2LvQsna08y3H94o7O7A10D6ebeje4e3Y25Fk9HT167\n4TXjkOz6Q37tbOyMIaz6ABnVdVSjnkOwWzCR8ZEM+2gYv+b/ypyrLAHs6+yLl5OXUR/8dgTf6ZiU\nyXi9+h7runvXceNnN2JjsuG2Xrdh1mYc7Rz54dcfuP+7+6k2VzPIb1Cjky/3Zuxl04lN/HXUX8/4\nei3V0gD5ABiklBoEPAH8B1gCjGmTVgghWs3exp4wjzDGhIyhr09f+vr0bfKt9GzGhY3jsR8e474h\n9wE06qEM8BvAFQFXNLmMSLhPOOPDxhuT0/V8nHy4LvQ6vor7Cj9nyxUAZl85mzkj5pz2yDdvJ288\nHH8bwprQYwJFJ4sYHza+0Rnzb094m9uW3YaznTPPj3megooCPB09uSb4GuxMdlwZ0PRM9f6+/Y1z\nVsByOLWTnRPDugxrVO760Ov5z23/AcDRzhFXB1e8nbyNqxiA5cO//oxwPxc/XB1ccXNwo0vnLk2u\nTj08cDirJq/C2d6ZjJIMwn0s5xk9N/o5PB09jfocbBxadZb5QL+BfH/v9/xuxe+Y+c1Mrgu9jq/v\n/ponIp9gwa0LiIyPZEPChkYB8v6u91lyYInRAzpvWuuz/gB76/59Hvhjw2UXw49lN4S4fO1M3akL\nKwpbvb3ZbNbfHf1Om83mJuvKqsr0sdxjTZZX1VSdsb4tSVuara85D//3YX04+3CLyi7cs1AvPbBU\na631xoSN+pqPr9Faa51SlNJs+fj8eL3i0IpGy3xf99WLYxaf8XV+zfu1ybK1R9fq2d/NNp4XVRZp\nrc/8XpzJt0e+1eHvh7dq23olJ0t0SlGK9n3dV9/3zX163GfjtNls1t8e+VbfsOQGvS9jn157dK3O\nL8/X3v/y1t8f/14/FfmUrvvcPK/PXqWbudTyqZRSPwM/APcB1wLZwH6t9YC2ibHzo5TSLdkPIcSl\nRWtNaVUpnR06n9N2V3x4BcvuWEZv795WalnL5JXnsTVlK7f3Pv8Lbbzw0wvM2zGPQw8eIsQ9hKLK\nIrrM64KznTN9ffpyPO84fi5+xMy2HMmmlEJrfV5H07Y0QPyBe4DdWutflFLBQITWesn5vHhbkQAR\nQpyLWnPtGa9C0BGVVZVxPP+4cbUAgGc3PsukPpMY2mUor219DX8Xf2YOnglcwACpezE/oH7QcJfW\nOvt8XrgtSYAIIcS5aYsAaemlTO4CdgGTgbuAnUqpO8/nhYUQQnRsLR3C2g/cWN/rUEr5ABu01oOs\n3L4WkR6IEEKcmwvWAwFMpwxZ5Z3DtkIIIS5BLT0P5Ael1HpgWd3zu4F11mmSEEKIjqBFvQit9ZPA\nQmBg3c9CrfXTLdlWKTVBKXVEKXVMKdVkG6VUb6XUNqVUpVLq8QbLHZRSO5VSMUqpWKXU/7Vsl4QQ\nQlwILT4Kq1WVK2UCjgHXA+lY7pE+RWt9pEEZbyAEmAQUaK3nNVjnpC3X3rIBtmK5L/rWZl5H5kCE\nEOIctMUcyBmHsJRSJUBzn8wKy1mMrmepfzhwXGudVFffcmAiYASI1joXyFVKNbmKmNa6vO6hA5be\nUsGpZYQQQrSPMwaI1vrcTu9sKhBIafA8FUuotEhdDyYaCAMWaK1bdlNkIYQQVndRH0mltTZrrYcA\nQcBopZRcvFEIIS4SLT0Kq7XSgIaX3wyqW3ZOtNbFSqn/AkOBn5srM3fuXONxREQEERER5/oyQghx\nyYqKiiIqKqpN67T2JLoNcBTLJHoGlrPZp2qt45op+wJQqrV+s+65N1CttS5SSjkC64EXtdYbm9lW\nJtGFEOIcWH0S/XxprWuVUo8AkViGyxZpreOUUrMtq/XCumts7QE6A2al1BygHxAAfKosNyYwYbkX\ne5PwEEII0T6s2gO5UKQHIoQQ5+ZCXspECCGEaEQCRAghRKtIgAghhGgVCRAhhBCtIgEihBCiVSRA\nhBBCtIoEiBBCiFaRABFCCNEqEiBCCCFaRQJECCFEq0iACCGEaBUJECGEEK0iASKEEKJVJECEEEK0\nigSIEEKIVpEAEUII0SpWDxCl1ASl1BGl1DGl1NPNrO+tlNqmlKpUSj3eYHmQUmqTUipWKXVQKfWY\ntdsqhBCi5ax9T3QTcAzLPdHTgd3AFK31kQZlvIEQYBJQoLWeV7fcH/DXWu9TSrkA0cDEhts2qEPu\nSCiEEOegI9yRcDhwXGudpLWuBpYDExsW0Frnaq2jgZpTlmdqrffVPS4F4oBAK7dXCCFEC1k7QAKB\nlAbPU2lFCCilugGDgZ1t0iohhBDnzba9G3A2dcNXq4A5dT2RZs2dO9d4HBERQUREhNXbJoQQHUVU\nVBRRUVFtWqe150CuAuZqrSfUPX8G0Frr15op+wJQUj8HUrfMFlgLfK+1fucMryNzIEIIcQ46whzI\nbqCHUipEKWUPTAHWnKH8qTvzMXD4TOEhhBCifVi1BwKWw3iBd7CE1SKt9atKqdlYeiILlVJ+wB6g\nM2AGSoF+wCBgM3AQ0HU/f9da/9DMa0gPRAghzkFb9ECsHiAXggSIEEKcm44whCWEEOISJQEihBCi\nVSRAhBBCtIoEiBBCiFaRABFCCNEqEiBCCCFaRQJECCFEq0iACCGEaBUJECGEEK0iASKEEKJVJECE\nEEK0igSIEEKIVpEAEUII0SoSIEIIIVpFAkQIIUSrSIAIIYRoFasHiFJqglLqiFLqmFLq6WbW91ZK\nbVNKVSqlHj9l3SKlVJZS6oC12ymEEOLcWDVAlFIm4D1gPBAOTFVK9TmlWB7wKPB6M1UsrttWCCHE\nRcbaPZDhwHGtdZLWuhpYDkxsWEBrnau1jgZqTt1Ya70FKLByG4UQQrSCtQMkEEhp8Dy1bpkQQogO\nzra9G9BW5s6dazyOiIggIiKi3doihBAXm6ioKKKiotq0TqW1btMKG1Wu1FXAXK31hLrnzwBaa/1a\nM2VfAEq01vNOWR4CfKe1HniG19HW3A8hhLjUKKXQWqvzqcPaQ1i7gR5KqRCllD0wBVhzhvLN7Yw6\nzXIhhBDtyKo9ELAcxgu8gyWsFmmtX1VKzcbSE1molPID9gCdATNQCvTTWpcqpZYCEYAXkAW8oLVe\n3MxrSA9ECCHOQVv0QKweIBeCBIgQQpybjjCEJYQQ4hIlASKEEKJVJECEEEK0igSIEEKIVpEAEUII\n0SoSIEIIIVpFAkQIIUSrSIAIIYRoFQkQIYQQrSIBIoQQolUkQIQQQrSKBIgQQohWkQARQgjRKhIg\nQgghWkUCRAghRKtIgAghhGgVqweIUmqCUuqIUuqYUurpZtb3VkptU0pVKqUeP5dthRBCtB+r3pFQ\nKWUCjgHXA+lY7pE+RWt9pEEZbyAEmAQUaK3ntXTbBnXIHQmFEOIcdIQ7Eg4Hjmutk7TW1cByYGLD\nAlrrXK11NFBzrtsKIYRoP9YOkEAgpcHz1Lpl1t5WCCGEldm2dwPayty5c43HERERREREnLZs/WiX\nOq/OmxBCdBxRUVFERUW1aZ3WngO5CpirtZ5Q9/wZQGutX2um7AtASYM5kHPZVh84oCkshNxcyMyE\nrKzf/q1/nJ0NDg5QXg41NeDuDh4e4OUFXbpAYKDlX1dXcHSE4mJwcoI+fSyvU1IC11xjWd+WqqrA\n3r5t6xRCiDNpizkQaweIDXAUy0R4BrALmKq1jmum7AtAqdb6zVZsq8PDNW5u4OMDfn7g72/5t+Fj\nX184edISCnZ2UFBg+cnNhYwMSEuD9HQoLbWETOfOltA4dszyOp06wfbtlm1raiA8HGprwcbGEjh2\ndnD11WA2Q/fucOAA7NwJV10FQ4daej6FhVBWZqk7OtpS39GjMGMG3Huv5XWLi8HNzRJscXGQkmKp\n19kZKiosZZSy7Ie9/W89KaUsbbC3b/pjMln2vbLS8q+9vaXNNjaNt6//AUuwnTwJnp6W1zKbLftg\nNlt+4Lf6O3JvzmxuvN/1y2prLe/n2Wht+XuoqrL8jdjYWJ7Xv7e1tZb3vbLSUtbLq/FrVVdbfj82\nNpbnVVWWvxNvb8tysPwdVlQ0Xnam9sD5/06qqy1/q/V/Z/Vyciz/R/z9LX9DZ1NTY2lL/f41t95k\nsvxUVFjep/q/z7Pt65lobWmnnZ3l91K/LD3dUren529lCwosZQMCwNb2t7K1tZb3oVOnc3s/S0og\nMdHyuXO631n934yTU6t38bxc9AEClkNxgXewzLcs0lq/qpSajaU3sVAp5cf/t3fvMXKVZRzHv7+9\n360e3VYAAAqZSURBVHqlWwqUOyQFJKkghIAEExGRGKsEI4QYNZqQCEJiVCBqEING/tDESxRRTBA1\ngJIAJiSCKBKi3ELLtWAJAcpSltJSdrfbC50+/vGc40yXXdrO7naY4fdJJjPz7rm8z7znvM85Z88F\nHgXmADuBMeDYiBibbNwp5rHPzsLauTMXjrY2WLUq92gqlexo33ork0JbG6xZA8cfDyedBPfemwtT\nW1smjr6+HPbEEzO5HH44XHMNrFyZC/a8ebBxYya2Y4/NhfqBB3I+fX3VlXZ8POdbW7dyoZz4qlRy\nJejpyZVz+/ZcWSuVHDdi1xdkbF1dsGFDDluu5FJ1hdi+PVewqZLQvvpem9gme5XDtrdnB1G+R+Tv\nXKlkrD09+XnLlmpCnjOnGnt/f06r7Oi2bq3+Np2d2R4dHdkO/f05n5GRnG53dzUJl38bHc1Ouqsr\nO5vR0Vw25s7N8oMPzmkND2e7b90Khx22a4cUke1Q1mdkJOs+OFht+yVLsoN8442s3/z5MDCQdSqX\ni46OnOfISL7WravusS9cmBtg4+O5PMydm3UaHMzPlUo16e7cmfVbvDj3/F99NWPo7c1h587N32rT\npuy4t2zJcTo7qxtG27dnbEcfveuGU39/1qW/P+N55ZUsnz8/59PVleOPjeU61NmZ0160KGNZvz7r\nUY6z33453pYt+Xu88UbWb+vW6oZaR0f+PnPmVNu6vz+H7e/PYTdvzvV0cDC/r10LS5fm/N56K8sX\nL87fZ/PmXGc2bMi6DQxk+3R0VJezcj0bGsrfsKcnl49ymP7+rE/tOOU6USa9sbFcxrq7q+tyd3eO\ns2wZXH11EySQfcGn8TbWxORTNsW++l5qb991BaxNdlK1g9uxo/oekSt2e3t1j6tcQaXsMEZHqwlq\nbCzLe3qyI6rtfMs6lXt4mzblOBP3ODZuzE5mx45qh7p5c3bIAwPZ2bS3Zwf20kvZCR56aLXTffnl\nd7ZBmQi6u7NjicjOq9zyHh7O8kWLMvZNmzKWcq+03DMdGKjWaenSakIdHs5DwF1d2fm0tWX52rVZ\n93IPqnwvk96SJVn3iclp27bswBcuzHlC9VBu+VuNjOSGWJk4entzGuUGTV9f1rGvLxPRgQfmfGsT\nTU9PdqZDQzn9wcE8bB2R0x4dzY7/gANyvm+/ndPq7c1X2a4bNuR0e3ur81i0KOvT25t1GBrKtu3u\nziRfG9frr+eroyPr1tlZTYTlEZBK5Z0bQvvvnzGOj+f45Ubi+HjWvVyGazeU2ttz+gMD1Y3bctne\nti3HWbwYzjzTCQRwAjEz21vNcB2ImZm1KCcQMzOrixOImZnVxQnEzMzq4gRiZmZ1cQIxM7O6OIGY\nmVldnEDMzKwuTiBmZlYXJxAzM6uLE4iZmdXFCcTMzOriBGJmZnVxAjEzs7rMegKRdLakZyX9V9Ll\nUwzzM0lrJK2StLym/DJJTxavS2e7rmZmtudmNYFIagN+AXwcOA64QNKyCcN8AjgyIo4GLgKuK8qP\nA74MfAhYDnxS0hGzWd/3ovvuu6/RVZhVjq+5Ob73t9neAzkZWBMRL0XE28DNwIoJw6wAfg8QEQ8B\n84rH3B4DPBQR2yKiAtwPnDvL9X3PafUF2PE1N8f3/jbbCeQgYG3N91eKsncbZqgoewo4XdICSX3A\nOcDBs1hXMzPbCx2NrsBUIuJZSdcC9wBjwEqg0thamZlZaVafiS7pFOB7EXF28f0KICLi2pphrgP+\nGRG3FN+fBc6IiOEJ0/oBsDYirptkPn4gupnZXpruM9Fnew/kEeAoSYcC64DzgQsmDHMncDFwS5Fw\nNpXJQ9JgRKyXdAjwGeCUyWYy3R/BzMz23qwmkIioSLoEuJv8f8sNEbFa0kX557g+Iu6SdI6k54HN\nwJdqJnGbpIXA28BXI2JkNutrZmZ7blYPYZmZWetq6ivR9+QixWYj6UVJj0taKenhomyBpLslPSfp\nb5LmNbqee0rSDZKGJT1RUzZlPJKuLC4qXS3prMbUes9NEd9Vkl6R9FjxOrvmb00Tn6Slkv4h6ena\ni3lbpf0mie9rRXmrtF+3pIeKvuRpST8symeu/SKiKV9k8nseOBToBFYByxpdrxmI6wVgwYSya4Fv\nFZ8vB37U6HruRTwfJi8EfWJ38QDHkmfbdQCHFe2rRsdQR3xXAV+fZNhjmik+YAmwvPg8ADwHLGuV\n9nuX+Fqi/Yo69xXv7cCDwGkz2X7NvAeyJxcpNiPxzj3DFcCNxecbgU/v0xpNQ0Q8ALw5oXiqeD4F\n3BwROyLiRWAN2c7vWVPEB9mOE62gieKLiNciYlXxeQxYDSylRdpvivjK69Savv0AImK8+NhN9itv\nMoPt18wJZE8uUmxGAdwj6RFJXynK9o/izLSIeA1Y3LDazYzFU8Qz1UWlzeiS4t5uv605RNC08Uk6\njNzTepCpl8dWiO+hoqgl2k9Sm6SVwGvAfRHxDDPYfs2cQFrVaRFxAnnl/cWSTieTSq1WO/Oh1eL5\nJXBERCwnV9wfN7g+0yJpAPgLcFmxpd5Sy+Mk8bVM+0XEzoj4ILnneLqkjzCD7dfMCWQIOKTm+9Ki\nrKlFxLrifT1wO7kLOVzcHwxJS4DXG1fDGTFVPEPseruapmzTiFgfxUFl4DdUDwM0XXySOsjO9aaI\nuKMobpn2myy+Vmq/UuQlEHeRN6edsfZr5gTy/4sUJXWRFyne2eA6TYukvmJrCEn9wFnAk2RcXywG\n+wJwx6QTeO8Sux5TniqeO4HzJXVJOhw4Cnh4X1VyGnaJr1gpS+eS93WD5ozvd8AzEfHTmrJWar93\nxNcq7SdpUXn4TVIv8DHyn+Qz136NPktgmmcYnE2eObEGuKLR9ZmBeA4nzyZbSSaOK4ryhcDfi1jv\nBuY3uq57EdOfgFeBbcDL5IWiC6aKB7iSPPtjNXBWo+tfZ3y/B54o2vJ28phz08VHnrFTqVkmHyvW\nuSmXxxaJr1Xa7/gippXA48A3ivIZaz9fSGhmZnVp5kNYZmbWQE4gZmZWFycQMzOrixOImZnVxQnE\nzMzq4gRiZmZ1cQIxayBJZ0j6a6PrYVYPJxCzxvPFWNaUnEDM9oCkC4uH8zwm6VfFXU5HJf1E0lOS\n7pG0XzHsckn/Ke7melvN7SSOLIZbJenR4nYRAHMk/bl4iM9NDQvSbC85gZjthqRlwOeAUyPvlLwT\nuBDoAx6OiA8A95MPIoJ8xsI3I+/m+lRN+R+BnxflpwLrivLlwKXkA32OlHTq7EdlNn0dja6AWRP4\nKHAC8IgkAT3AMJlIbi2G+QNwm6S5wLzIB01BJpNbi5tkHhQRdwJExHaAnBwPR3EXZkmryKfB/Xsf\nxGU2LU4gZrsn4MaI+PYuhdJ3JwwXNcPvjW01nyt4vbQm4UNYZrt3L3CepEEASQskHUI+Z/q8YpgL\ngQcin7uwUdJpRfnngX9FPqhoraQVxTS6iltsmzUtb+mY7UZErJb0HeBuSW3AduASYDNwcrEnMkz+\nnwTyGQu/LhLEC+Qt3iGTyfWSvl9M47OTzW72IjGbWb6du1mdJI1GxJxG18OsUXwIy6x+3vqy9zXv\ngZiZWV28B2JmZnVxAjEzs7o4gZiZWV2cQMzMrC5OIGZmVhcnEDMzq8v/ANnJjJQXWIyZAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f94e78b0d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nolearn.lasagne.visualize import plot_loss\n",
    "\n",
    "plot_loss(net1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "Opening /home/isaac/Dropbox/data_for_brian/meta/df_meta.hd in read-only mode\n"
     ]
    }
   ],
   "source": [
    "META = load_data(\"/home/isaac/Dropbox/data_for_brian/meta/df_meta.hd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def old_fast_show_ratio_plot(xy_points, y_data, log = False, normalize_buckets=True):\n",
    "    if log:\n",
    "        y_data = np.log(y_data)\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(3,3)\n",
    "    plt.hist(y_data)\n",
    "    plt.show()\n",
    "\n",
    "    buckets = defaultdict(list)\n",
    "    resolution = 200\n",
    "    x = np.array(xy_points['x'])\n",
    "    y = np.array(xy_points['y'])\n",
    "    H, xedges, yedges = numpy.histogram2d(x,y, bins=resolution, weights = y_data)\n",
    "    H_nums, dummy2, dummy1 = numpy.histogram2d(x,y, bins=resolution)\n",
    "    plt.show()\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(12,12)\n",
    "    if normalize_buckets:\n",
    "        H=H/H_nums\n",
    "    H[H_nums == 0.0] = numpy.nan\n",
    "#     if log:\n",
    "#         H = np.log(H)\n",
    "\n",
    "\n",
    "    plt.imshow(H,\n",
    "               interpolation='nearest', cmap=cm.gist_rainbow)#, vmin = 0, vmax = 1)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    return np.nan_to_num(H)\n",
    "\n",
    "\n",
    "def plot_on_map(pddf, meta = META):\n",
    "    for col in pddf.columns:\n",
    "        print(\"\\n\\n\\n\",col)\n",
    "        xy = pandas.DataFrame.from_dict({'x': -meta['pointx'],'y': meta['pointy']})\n",
    "        print(\"linear plot\")\n",
    "        old_fast_show_ratio_plot(xy,np.array(pddf[col]))\n",
    "#         print(\"log plot\")\n",
    "#         old_fast_show_ratio_plot(xy,np.array(pddf[col]), log = True)\n",
    "#         colored_scatter(xy,np.array(pddf[col]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = lasagne.layers.get_output(net1.get_all_layers()[-1], x).eval()\n",
    "print output\n",
    "# np.save(\"/home/isaac/Desktop/devika/gitignored/autoencoder_hidden_activations\", hidden_activations)\n",
    "\n",
    "plt.hist(y, bins = 100, label = \"actual damage\")\n",
    "plt.show()\n",
    "plt.hist(output, bins = 100, label = \"predicted damage\")\n",
    "plt.show()\n",
    "\n",
    "plot_on_map(pandas.DataFrame.from_dict({\"x\": y - output.T[0]}))\n",
    "plot_on_map(pandas.DataFrame.from_dict({\"x\": output.T[0]}))\n",
    "plot_on_map(pandas.DataFrame.from_dict({\"x\": y}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "(870476, 346)\n",
      "(870476, 1)\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas SGEMM launch failed : a.shape=(30, 1), b.shape=(1, 256), m=30, n=256, k=1\n\t [[Node: MatMul_33 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_22_0/_5, Variable_66/read)]]\n\t [[Node: Mean_12/_17 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_228_Mean_12\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'MatMul_33', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 589, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-f9df80844c10>\", line 60, in <module>\n    pred = multilayer_perceptron(x, weights, biases)\n  File \"<ipython-input-31-f9df80844c10>\", line 38, in multilayer_perceptron\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1209, in matmul\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1178, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-f9df80844c10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;31m# Run optimization op (backprop) and cost op (to get loss value)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             _, c = sess.run([optimizer, cost], feed_dict={x: trY[start:end],\n\u001b[1;32m---> 82\u001b[1;33m                                                           y: trY[start:end]})\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[1;31m# Compute average loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mavg_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 636\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 708\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    709\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas SGEMM launch failed : a.shape=(30, 1), b.shape=(1, 256), m=30, n=256, k=1\n\t [[Node: MatMul_33 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_22_0/_5, Variable_66/read)]]\n\t [[Node: Mean_12/_17 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_228_Mean_12\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'MatMul_33', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 589, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-f9df80844c10>\", line 60, in <module>\n    pred = multilayer_perceptron(x, weights, biases)\n  File \"<ipython-input-31-f9df80844c10>\", line 38, in multilayer_perceptron\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1209, in matmul\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1178, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A Multilayer Perceptron implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "print \"hi\"\n",
    "\n",
    "import tensorflow as tf\n",
    "print trX.shape\n",
    "print trY.shape\n",
    "\n",
    "trX = trX.astype(np.float32)\n",
    "trY = trY.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 30\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = trY.shape[1] # MNIST data input (img shape: 28*28)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, 1]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([1]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.square((pred - y)))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(trX)/batch_size)\n",
    "        # Loop over all batches\n",
    "        for start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\n",
    "#             sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: trY[start:end],\n",
    "                                                          y: trY[start:end]})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost)\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
