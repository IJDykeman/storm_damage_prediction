{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "from tsne import bh_sne # thi is the correct tsne to use.  It's the one discussed btnw\n",
    "import sklearn.manifold\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from time import gmtime, strftime\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import math\n",
    "from scipy.stats.stats import pearsonr\n",
    "import random as rand\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict\n",
    "def memo(f):\n",
    "    memo = {}\n",
    "    def helper(x):\n",
    "        if x not in memo:            \n",
    "            memo[x] = f(x)\n",
    "        return memo[x]\n",
    "    return helper\n",
    "\n",
    "def zero_to_one(array):\n",
    "    array = array - np.min(array)\n",
    "    array = array/np.max(array)\n",
    "    return np.nan_to_num(array)\n",
    "\n",
    "\n",
    "@memo\n",
    "def load_dataset(path, scale=True):\n",
    "    gc.collect()\n",
    "    data = pandas.read_hdf(path, '/df')\n",
    "    df = pandas.DataFrame(data)\n",
    "    if scale:\n",
    "        for label in df._get_numeric_data().columns:\n",
    "            if label != 'hcad':\n",
    "                df[label] = df[label].astype(float)\n",
    "                df[label] = zero_to_one(df[label])\n",
    "                df[label][df[label] > 1] = 1.0\n",
    "    df['hcad'] = df['hcad'].astype(int)\n",
    "    df = df.replace([np.inf, -np.inf], 1)\n",
    "    \n",
    "    return df.sort(['hcad']).fillna(0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def tsne(df_data, dest_folder, n = None, file_tag= \"\", embedded_dimensions=2, perplexity = 50):\n",
    "    result_2d = {}\n",
    "    result_2d['hcad'] = df_data['hcad'][:n]\n",
    "    df_data = df_data.drop('hcad', 1) # don't embed the hcad number!\n",
    "    df_data = np.array(df_data)[:n]\n",
    "    embedding = bh_sne(np.array(df_data)[:n], perplexity=perplexity)\n",
    "#     embedding = bh_sne(np.array(df_data))\n",
    "\n",
    "    result_2d['x'] = zero_to_one(embedding[:, 0])\n",
    "    result_2d['y'] = zero_to_one(embedding[:, 1])\n",
    "    result_2d = pandas.DataFrame.from_dict(result_2d)\n",
    "    #name = file_tag+\"_\"+\"_\".join(df_data.columns)[:40] + \"_n:\"+str(len(result))\n",
    "    #result.to_pickle(dest_folder+name)\n",
    "    return embedding\n",
    "\n",
    "def hist_2d(vis_x,vis_y):\n",
    "    hh, locx, locy = scipy.histogram2d(vis_x, vis_y, bins=[200,200])\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(30,30)\n",
    "    plt.imshow(np.flipud(hh.T),cmap='jet', interpolation='none', shape = (1,1))\n",
    "    plt.colorbar()\n",
    "    \n",
    "def get_where_img0_is_1(pddf):\n",
    "    img0_metadata = (META.loc[META['img0'] == 1])\n",
    "    return pddf.loc[pddf['hcad'].isin(list(img0_metadata['hcad']))]\n",
    "\n",
    "def pairwise_plot(pddf, sqrt = False):\n",
    "    if sqrt:\n",
    "        pddf = np.sqrt(pddf)\n",
    "    axes = pandas.tools.plotting.scatter_matrix(pddf, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def fast_show_ratio_plot(xy_points, y_data, log = False, normalize_buckets=True):\n",
    "    if log:\n",
    "        y_data = np.log(y_data)\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(3,3)\n",
    "    plt.hist(y_data)\n",
    "    plt.show()\n",
    "\n",
    "    buckets = defaultdict(list)\n",
    "    resolution = 200\n",
    "    x = np.array(xy_points['x'])\n",
    "    y = np.array(xy_points['y'])\n",
    "    H, xedges, yedges = numpy.histogram2d(x,y, bins=resolution, weights = y_data)\n",
    "    H_nums, dummy2, dummy1 = numpy.histogram2d(x,y, bins=resolution)\n",
    "    plt.show()\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(12,12)\n",
    "    if normalize_buckets:\n",
    "        H=H/H_nums\n",
    "    H[H_nums == 0.0] = numpy.nan\n",
    "#     if log:\n",
    "#         H = np.log(H)\n",
    "    \n",
    "\n",
    "    plt.imshow(H, \n",
    "               interpolation='nearest', cmap=cm.gist_rainbow)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    return np.nan_to_num(H)\n",
    "\n",
    "def colored_scatter(xy_points, y_data):\n",
    "        fig = plt.figure(frameon=False)\n",
    "        fig.set_size_inches(20,20)\n",
    "        plt.scatter(xy_points['x'], xy_points['y'], c=y_data,  marker='x', facecolor='b', cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "def load_mega_hcad():\n",
    "    # hcad = load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df.hd\")\n",
    "    hcad_data = [load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_100.hd\"),\n",
    "     load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_200.hd\"),\n",
    "     load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_400.hd\"),\n",
    "    load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_1000.hd\")]\n",
    "\n",
    "    mega_hcad = {}\n",
    "\n",
    "    for column in hcad_data[0]:\n",
    "        for index, dataset in enumerate(hcad_data):\n",
    "            mega_hcad[column+\"_\"+str(index)] = dataset[column]\n",
    "    mega_hcad = pandas.DataFrame.from_dict(mega_hcad).as_matrix()\n",
    "    y_data_np = Y_DATA.as_matrix()\n",
    "    X_train = np.expand_dims(mega_hcad[:600000], axis=1)\n",
    "    y_train = y_data_np[:600000, 1]\n",
    "    print(\"y train\",y_train.shape)\n",
    "    X_val = np.expand_dims(mega_hcad[600000:700000], axis=1)\n",
    "    y_val = y_data_np[600000:700000, 1]\n",
    "    X_test = np.expand_dims(mega_hcad[700000:], axis=1)\n",
    "    y_test = y_data_np[700000:, 1]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# tsne_embed = pandas.read_pickle(\"/home/isaac/Desktop/devika/gitignored/img1_hcad/_mean_accrued_depr_pct_std_accrued_depr_p_n:104878\")\n",
    "# hist_2d(np.array(tsne_embed['x']),np.array(tsne_embed['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all the data at 200m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /home/isaac/Dropbox/data_for_brian/meta/df_meta.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/wind_features/hcad_interp_withoutpartial_rad200_hist16x16.mat.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/terrain_features/dsmgrid/terrain_200.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/y_df.hd in read-only mode\n"
     ]
    }
   ],
   "source": [
    "hcad = load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_200.hd\")\n",
    "# hcad = hcad[['hcad', 'mean_accrued_depr_pct', 'mean_bld_val', 'mean_land_val','mean_quality','mean_rcnld', 'mean_tot_mkt_val','mean_year_built','mean_year_remodeled']]\n",
    "META = load_dataset(\"/home/isaac/Dropbox/data_for_brian/meta/df_meta.hd\")\n",
    "WIND = load_dataset(\"/home/isaac/Dropbox/data_for_brian/wind_features/hcad_interp_withoutpartial_rad200_hist16x16.mat.hd\")\n",
    "TERRAIN = load_dataset(\"/home/isaac/Dropbox/data_for_brian/terrain_features/dsmgrid/terrain_200.hd\")\n",
    "\n",
    "Y_DATA = load_dataset(\"/home/isaac/Dropbox/data_for_brian/y_df.hd\")\n",
    "img0_y_data = get_where_img0_is_1(Y_DATA)\n",
    "\n",
    "\n",
    "# @memo\n",
    "img0_terrain_data = get_where_img0_is_1(TERRAIN)\n",
    "img0_wind_data = get_where_img0_is_1(WIND)\n",
    "img0_hcad_data = get_where_img0_is_1(hcad)\n",
    "img0_metadata = (META.loc[META['img0'] == 1])\n",
    "# print get_where_img0_is_1(WIND)\n",
    "def plot_on_map(pddf, meta = META):\n",
    "    for col in pddf.columns:\n",
    "        print(\"\\n\\n\\n\",col)\n",
    "        xy = pandas.DataFrame.from_dict({'x': -meta['pointx'],'y': meta['pointy']})\n",
    "        print(\"linear plot\")\n",
    "        fast_show_ratio_plot(xy,np.array(pddf[col]))\n",
    "        print(\"log plot\")\n",
    "        fast_show_ratio_plot(xy,np.array(pddf[col]), log = True)\n",
    "        colored_scatter(xy,np.array(pddf[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /home/isaac/Dropbox/data_for_brian/y_df.hd in read-only mode\n"
     ]
    }
   ],
   "source": [
    "Y_DATA = load_dataset(\"/home/isaac/Dropbox/data_for_brian/y_df.hd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_2d = tsne(img0_hcad_data ,\"/home/isaac/Desktop/devika/gitignored/6_dimensions_hcad_img0/\", \n",
    "                 file_tag =\"hcad_img0\", n=None, embedded_dimensions=2)\n",
    "fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': embedding_2d[:,0],\n",
    "                                             'y': embedding_2d[:,1]}), np.array(img0_y_data['y200_mean']))\n",
    "# print embedding_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans cluster the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "kmeans = KMeans(init='k-means++', n_clusters=30)\n",
    "kmeans.fit(embedding_2d)\n",
    "print kmeans.labels_\n",
    "\n",
    "# np.random.shuffle(kmeans.labels_)\n",
    "colored_scatter(pandas.DataFrame.from_dict({'x': - img0_metadata ['pointx'][:len(kmeans.labels_)],\n",
    "                                            'y': img0_metadata['pointy'][:len(kmeans.labels_)]}), kmeans.labels_)\n",
    "fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': - img0_metadata ['pointx'][:len(kmeans.labels_)],\n",
    "                                                 'y': img0_metadata['pointy'][:len(kmeans.labels_)]}),kmeans.labels_)\n",
    "\n",
    "for label in range(max(kmeans.labels_)+1):\n",
    "    print \"class\", label, \"damage:\",np.mean(np.array(img0_y_data['y200_mean'])[numpy.where(kmeans.labels_==label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pandas.read_pickle(\"/home/isaac/Desktop/devika/gitignored/testing_refactored_code/_mean_accrued_depr_pct_std_accrued_depr_p_n:100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = digits['images']\n",
    "tsne(images.reshape(1797,64), \"/home/isaac/Desktop/devika/gitignored/digits_test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print np.array(tsne_embed['x'])[0]\n",
    "# show_ratio_plot(tsne_embed, hcad['mean_bld_val'])\n",
    "# print get_meta()\n",
    "# print Y_DATA\n",
    "\n",
    "plot_on_map(img0_hcad_data, meta = img0_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ### create a [point, 100m, 200m, ... , point, 100m, 200m, ...] data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "y train (600000,)\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "(500, 1, 260)\n",
      "[ 1.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
      "  1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  1.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.\n",
      "  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.\n",
      "  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.  0.  1.  0.  0.\n",
      "  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
      "  1.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0.  0.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "GpuElemwise. Input dimension mis-match. Input 1 (indices start at 0) has shape[2] == 256, but the output's size on that axis is 496.\nApply node that caused the error: GpuElemwise{Composite{((i0 + Abs(i0)) * i1)},no_inplace}(GpuElemwise{Add}[(0, 0)].0, GpuElemwise{Composite{Cast{float32}(LT(i0, i1))}}[(0, 0)].0)\nToposort index: 51\nInputs types: [CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D)]\nInputs shapes: [(260, 32, 496), (260, 32, 256)]\nInputs strides: [(15872, 496, 1), (8192, 256, 1)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuFlatten{2}(GpuElemwise{Composite{((i0 + Abs(i0)) * i1)},no_inplace}.0)]]\n\nDebugprint of the apply node: \nGpuElemwise{Composite{((i0 + Abs(i0)) * i1)},no_inplace} [@A] <CudaNdarrayType(float32, 3D)> ''   \n |GpuElemwise{Add}[(0, 0)] [@B] <CudaNdarrayType(float32, 3D)> ''   \n | |GpuSubtensor{::, ::, int64} [@C] <CudaNdarrayType(float32, 3D)> ''   \n | | |Rebroadcast{?,?,1} [@D] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | | |Rebroadcast{?,?,0} [@E] <CudaNdarrayType(float32, 4D)> ''   \n | | |   |GpuDnnConv{algo='small', inplace=True} [@F] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     |GpuContiguous [@G] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     | |GpuDimShuffle{0,1,x,2} [@H] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     |   |GpuFromHost [@I] <CudaNdarrayType(float32, 3D)> ''   \n | | |     |     |inputs [@J] <TensorType(float32, 3D)>\n | | |     |GpuContiguous [@K] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     | |GpuDimShuffle{0,1,x,2} [@L] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     |   |W [@M] <CudaNdarrayType(float32, 3D)>\n | | |     |GpuAllocEmpty [@N] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     | |Shape_i{0} [@O] <TensorType(int64, scalar)> ''   \n | | |     | | |inputs [@J] <TensorType(float32, 3D)>\n | | |     | |Shape_i{0} [@P] <TensorType(int64, scalar)> ''   \n | | |     | | |W [@M] <CudaNdarrayType(float32, 3D)>\n | | |     | |TensorConstant{1} [@Q] <TensorType(int64, scalar)>\n | | |     | |Elemwise{Composite{((((i0 + i1) - i2) // i3) + i3)}}[(0, 2)] [@R] <TensorType(int64, scalar)> ''   \n | | |     |   |Shape_i{2} [@S] <TensorType(int64, scalar)> ''   \n | | |     |   | |inputs [@J] <TensorType(float32, 3D)>\n | | |     |   |TensorConstant{0} [@T] <TensorType(int8, scalar)>\n | | |     |   |Shape_i{2} [@U] <TensorType(int64, scalar)> ''   \n | | |     |   | |W [@M] <CudaNdarrayType(float32, 3D)>\n | | |     |   |TensorConstant{1} [@V] <TensorType(int8, scalar)>\n | | |     |GpuDnnConvDesc{border_mode='valid', subsample=(1, 1), conv_mode='conv'} [@W] <CDataType{cudnnConvolutionDescriptor_t}> ''   \n | | |     | |MakeVector{dtype='int64'} [@X] <TensorType(int64, vector)> ''   \n | | |     | | |Shape_i{0} [@O] <TensorType(int64, scalar)> ''   \n | | |     | | |Shape_i{1} [@Y] <TensorType(int64, scalar)> ''   \n | | |     | | | |inputs [@J] <TensorType(float32, 3D)>\n | | |     | | |TensorConstant{1} [@Q] <TensorType(int64, scalar)>\n | | |     | | |Shape_i{2} [@S] <TensorType(int64, scalar)> ''   \n | | |     | |MakeVector{dtype='int64'} [@Z] <TensorType(int64, vector)> ''   \n | | |     |   |Shape_i{0} [@P] <TensorType(int64, scalar)> ''   \n | | |     |   |Shape_i{1} [@BA] <TensorType(int64, scalar)> ''   \n | | |     |   | |W [@M] <CudaNdarrayType(float32, 3D)>\n | | |     |   |TensorConstant{1} [@Q] <TensorType(int64, scalar)>\n | | |     |   |Shape_i{2} [@U] <TensorType(int64, scalar)> ''   \n | | |     |Constant{1.0} [@BB] <float32>\n | | |     |Constant{0.0} [@BC] <float32>\n | | |Constant{0} [@BD] <int64>\n | |GpuDimShuffle{x,0,x} [@BE] <CudaNdarrayType(float32, (True, False, True))> ''   \n |   |b [@BF] <CudaNdarrayType(float32, vector)>\n |GpuElemwise{Composite{Cast{float32}(LT(i0, i1))}}[(0, 0)] [@BG] <CudaNdarrayType(float32, 3D)> ''   \n   |GPU_mrg_uniform{CudaNdarrayType(float32, 3D),inplace}.1 [@BH] <CudaNdarrayType(float32, 3D)> ''   \n   | |<CudaNdarrayType(float32, vector)> [@BI] <CudaNdarrayType(float32, vector)>\n   | |Elemwise{Cast{int32}} [@BJ] <TensorType(int32, vector)> ''   \n   |   |MakeVector{dtype='int64'} [@BK] <TensorType(int64, vector)> ''   \n   |     |Shape_i{0} [@O] <TensorType(int64, scalar)> ''   \n   |     |TensorConstant{32} [@BL] <TensorType(int64, scalar)>\n   |     |TensorConstant{256} [@BM] <TensorType(int64, scalar)>\n   |CudaNdarrayConstant{[[[ 0.5]]]} [@BN] <CudaNdarrayType(float32, (True, True, True))>\n\nStorage map footprint:\n - GpuElemwise{Add}[(0, 0)].0, Shape: (260, 32, 496), ElemSize: 4 Byte(s), TotalSize: 16506880 Byte(s)\n - GpuElemwise{Composite{Cast{float32}(LT(i0, i1))}}[(0, 0)].0, Shape: (260, 32, 256), ElemSize: 4 Byte(s), TotalSize: 8519680 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (8192, 50), ElemSize: 4 Byte(s), TotalSize: 1638400 Byte(s)\n - W, Shared Input, Shape: (8192, 50), ElemSize: 4 Byte(s), TotalSize: 1638400 Byte(s)\n - GpuFromHost.0, Shape: (260, 1, 500), ElemSize: 4 Byte(s), TotalSize: 520000 Byte(s)\n - inputs, Input, Shape: (260, 1, 500), ElemSize: 4 Byte(s), TotalSize: 520000 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - GPU_mrg_uniform{CudaNdarrayType(float32, 3D),inplace}.0, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - targets, Input, Shape: (500,), ElemSize: 4 Byte(s), TotalSize: 2000 Byte(s)\n - W, Shared Input, Shape: (32, 1, 5), ElemSize: 4 Byte(s), TotalSize: 640 Byte(s)\n - <CudaNdarrayType(float32, 3D)>, Shared Input, Shape: (32, 1, 5), ElemSize: 4 Byte(s), TotalSize: 640 Byte(s)\n - b, Shared Input, Shape: (50,), ElemSize: 4 Byte(s), TotalSize: 200 Byte(s)\n - W, Shared Input, Shape: (50, 1), ElemSize: 4 Byte(s), TotalSize: 200 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (50,), ElemSize: 4 Byte(s), TotalSize: 200 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (50, 1), ElemSize: 4 Byte(s), TotalSize: 200 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)\n - b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (3,), ElemSize: 8 Byte(s), TotalSize: 24 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Subtensor{int64}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Subtensor{int64}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{-1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{(inv(i0) / i1)}}.0, Shape: (1, 1), ElemSize: 8 Byte(s), TotalSize: 8 Byte(s)\n - Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{256}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{32}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{(1, 1) of 2.0}, Shape: (1, 1), ElemSize: 8 Byte(s), TotalSize: 8 Byte(s)\n - Constant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[ 0.89999998]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - b, Shared Input, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[[ 0.01]]]}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[[[ 0.]]]]}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - TensorConstant{0.00999999977648}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[[[ 0.5]]]}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.89999998]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[ 0.01]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.5]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[[ 0.89999998]]]}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n TotalSize: 30085190.0 Byte(s) 0.028 GB\n TotalSize inputs: 4538526.0 Byte(s) 0.004 BG\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2a4a0de73004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    169\u001b[0m             val_acc / val_batches * 100))\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-2a4a0de73004>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    619\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    295\u001b[0m     exc_value = exc_type(str(exc_value) + detailed_err_msg +\n\u001b[0;32m    296\u001b[0m                          '\\n' + '\\n'.join(hints))\n\u001b[1;32m--> 297\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: GpuElemwise. Input dimension mis-match. Input 1 (indices start at 0) has shape[2] == 256, but the output's size on that axis is 496.\nApply node that caused the error: GpuElemwise{Composite{((i0 + Abs(i0)) * i1)},no_inplace}(GpuElemwise{Add}[(0, 0)].0, GpuElemwise{Composite{Cast{float32}(LT(i0, i1))}}[(0, 0)].0)\nToposort index: 51\nInputs types: [CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D)]\nInputs shapes: [(260, 32, 496), (260, 32, 256)]\nInputs strides: [(15872, 496, 1), (8192, 256, 1)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuFlatten{2}(GpuElemwise{Composite{((i0 + Abs(i0)) * i1)},no_inplace}.0)]]\n\nDebugprint of the apply node: \nGpuElemwise{Composite{((i0 + Abs(i0)) * i1)},no_inplace} [@A] <CudaNdarrayType(float32, 3D)> ''   \n |GpuElemwise{Add}[(0, 0)] [@B] <CudaNdarrayType(float32, 3D)> ''   \n | |GpuSubtensor{::, ::, int64} [@C] <CudaNdarrayType(float32, 3D)> ''   \n | | |Rebroadcast{?,?,1} [@D] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | | |Rebroadcast{?,?,0} [@E] <CudaNdarrayType(float32, 4D)> ''   \n | | |   |GpuDnnConv{algo='small', inplace=True} [@F] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     |GpuContiguous [@G] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     | |GpuDimShuffle{0,1,x,2} [@H] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     |   |GpuFromHost [@I] <CudaNdarrayType(float32, 3D)> ''   \n | | |     |     |inputs [@J] <TensorType(float32, 3D)>\n | | |     |GpuContiguous [@K] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     | |GpuDimShuffle{0,1,x,2} [@L] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     |   |W [@M] <CudaNdarrayType(float32, 3D)>\n | | |     |GpuAllocEmpty [@N] <CudaNdarrayType(float32, (False, False, True, False))> ''   \n | | |     | |Shape_i{0} [@O] <TensorType(int64, scalar)> ''   \n | | |     | | |inputs [@J] <TensorType(float32, 3D)>\n | | |     | |Shape_i{0} [@P] <TensorType(int64, scalar)> ''   \n | | |     | | |W [@M] <CudaNdarrayType(float32, 3D)>\n | | |     | |TensorConstant{1} [@Q] <TensorType(int64, scalar)>\n | | |     | |Elemwise{Composite{((((i0 + i1) - i2) // i3) + i3)}}[(0, 2)] [@R] <TensorType(int64, scalar)> ''   \n | | |     |   |Shape_i{2} [@S] <TensorType(int64, scalar)> ''   \n | | |     |   | |inputs [@J] <TensorType(float32, 3D)>\n | | |     |   |TensorConstant{0} [@T] <TensorType(int8, scalar)>\n | | |     |   |Shape_i{2} [@U] <TensorType(int64, scalar)> ''   \n | | |     |   | |W [@M] <CudaNdarrayType(float32, 3D)>\n | | |     |   |TensorConstant{1} [@V] <TensorType(int8, scalar)>\n | | |     |GpuDnnConvDesc{border_mode='valid', subsample=(1, 1), conv_mode='conv'} [@W] <CDataType{cudnnConvolutionDescriptor_t}> ''   \n | | |     | |MakeVector{dtype='int64'} [@X] <TensorType(int64, vector)> ''   \n | | |     | | |Shape_i{0} [@O] <TensorType(int64, scalar)> ''   \n | | |     | | |Shape_i{1} [@Y] <TensorType(int64, scalar)> ''   \n | | |     | | | |inputs [@J] <TensorType(float32, 3D)>\n | | |     | | |TensorConstant{1} [@Q] <TensorType(int64, scalar)>\n | | |     | | |Shape_i{2} [@S] <TensorType(int64, scalar)> ''   \n | | |     | |MakeVector{dtype='int64'} [@Z] <TensorType(int64, vector)> ''   \n | | |     |   |Shape_i{0} [@P] <TensorType(int64, scalar)> ''   \n | | |     |   |Shape_i{1} [@BA] <TensorType(int64, scalar)> ''   \n | | |     |   | |W [@M] <CudaNdarrayType(float32, 3D)>\n | | |     |   |TensorConstant{1} [@Q] <TensorType(int64, scalar)>\n | | |     |   |Shape_i{2} [@U] <TensorType(int64, scalar)> ''   \n | | |     |Constant{1.0} [@BB] <float32>\n | | |     |Constant{0.0} [@BC] <float32>\n | | |Constant{0} [@BD] <int64>\n | |GpuDimShuffle{x,0,x} [@BE] <CudaNdarrayType(float32, (True, False, True))> ''   \n |   |b [@BF] <CudaNdarrayType(float32, vector)>\n |GpuElemwise{Composite{Cast{float32}(LT(i0, i1))}}[(0, 0)] [@BG] <CudaNdarrayType(float32, 3D)> ''   \n   |GPU_mrg_uniform{CudaNdarrayType(float32, 3D),inplace}.1 [@BH] <CudaNdarrayType(float32, 3D)> ''   \n   | |<CudaNdarrayType(float32, vector)> [@BI] <CudaNdarrayType(float32, vector)>\n   | |Elemwise{Cast{int32}} [@BJ] <TensorType(int32, vector)> ''   \n   |   |MakeVector{dtype='int64'} [@BK] <TensorType(int64, vector)> ''   \n   |     |Shape_i{0} [@O] <TensorType(int64, scalar)> ''   \n   |     |TensorConstant{32} [@BL] <TensorType(int64, scalar)>\n   |     |TensorConstant{256} [@BM] <TensorType(int64, scalar)>\n   |CudaNdarrayConstant{[[[ 0.5]]]} [@BN] <CudaNdarrayType(float32, (True, True, True))>\n\nStorage map footprint:\n - GpuElemwise{Add}[(0, 0)].0, Shape: (260, 32, 496), ElemSize: 4 Byte(s), TotalSize: 16506880 Byte(s)\n - GpuElemwise{Composite{Cast{float32}(LT(i0, i1))}}[(0, 0)].0, Shape: (260, 32, 256), ElemSize: 4 Byte(s), TotalSize: 8519680 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (8192, 50), ElemSize: 4 Byte(s), TotalSize: 1638400 Byte(s)\n - W, Shared Input, Shape: (8192, 50), ElemSize: 4 Byte(s), TotalSize: 1638400 Byte(s)\n - GpuFromHost.0, Shape: (260, 1, 500), ElemSize: 4 Byte(s), TotalSize: 520000 Byte(s)\n - inputs, Input, Shape: (260, 1, 500), ElemSize: 4 Byte(s), TotalSize: 520000 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - GPU_mrg_uniform{CudaNdarrayType(float32, 3D),inplace}.0, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - targets, Input, Shape: (500,), ElemSize: 4 Byte(s), TotalSize: 2000 Byte(s)\n - W, Shared Input, Shape: (32, 1, 5), ElemSize: 4 Byte(s), TotalSize: 640 Byte(s)\n - <CudaNdarrayType(float32, 3D)>, Shared Input, Shape: (32, 1, 5), ElemSize: 4 Byte(s), TotalSize: 640 Byte(s)\n - b, Shared Input, Shape: (50,), ElemSize: 4 Byte(s), TotalSize: 200 Byte(s)\n - W, Shared Input, Shape: (50, 1), ElemSize: 4 Byte(s), TotalSize: 200 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (50,), ElemSize: 4 Byte(s), TotalSize: 200 Byte(s)\n - <CudaNdarrayType(float32, matrix)>, Shared Input, Shape: (50, 1), ElemSize: 4 Byte(s), TotalSize: 200 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)\n - b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (3,), ElemSize: 8 Byte(s), TotalSize: 24 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Subtensor{int64}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Subtensor{int64}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{-1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{(inv(i0) / i1)}}.0, Shape: (1, 1), ElemSize: 8 Byte(s), TotalSize: 8 Byte(s)\n - Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{256}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{32}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{(1, 1) of 2.0}, Shape: (1, 1), ElemSize: 8 Byte(s), TotalSize: 8 Byte(s)\n - Constant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[ 0.89999998]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - b, Shared Input, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[[ 0.01]]]}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[[[ 0.]]]]}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - TensorConstant{0.00999999977648}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[[[ 0.5]]]}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.89999998]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[ 0.01]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.5]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[[ 0.89999998]]]}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n TotalSize: 30085190.0 Byte(s) 0.028 GB\n TotalSize inputs: 4538526.0 Byte(s) 0.004 BG\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print load_mega_hcad()[0].shape\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "# theano.config.optimizer='fast_compile'\n",
    "# theano.config.exception_verbosity='high'\n",
    "\n",
    "import lasagne\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None,1, 260),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv1DLayer(\n",
    "            network, num_filters=32, filter_size=5,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "#     network = lasagne.layers.MaxPool1DLayer(network, pool_size=2)\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "#     network = lasagne.layers.Conv2DLayer(\n",
    "#             network, num_filters=32, filter_size=(5, 5),\n",
    "#             nonlinearity=lasagne.nonlinearities.rectify)\n",
    "#     network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=50,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=1,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "# ############################## Main program ################################\n",
    "# Everything else will be handled in our main program now. We could pull out\n",
    "# more functions to better separate the code, but it wouldn't make it any\n",
    "# easier to read.\n",
    "\n",
    "def main(num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_mega_hcad()\n",
    "    #print(X_train)\n",
    "    #print(y_train)\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor3('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "\n",
    "    network = build_cnn(input_var)\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.squared_error(prediction, target_var) ## changed from categorical cross entropy\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            print(inputs.shape)\n",
    "            print(targets)\n",
    "            train_err += train_fn(inputs.transpose(), targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 260, 1),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 1),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 1))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 1),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 1))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def main(model='mlp', num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "    print(y_train.shape)\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    if model == 'mlp':\n",
    "        network = build_mlp(input_var)\n",
    "    elif model.startswith('custom_mlp:'):\n",
    "        depth, width, drop_in, drop_hid = model.split(':', 1)[1].split(',')\n",
    "        network = build_custom_mlp(input_var, int(depth), int(width),\n",
    "                                   float(drop_in), float(drop_hid))\n",
    "    elif model == 'cnn':\n",
    "        network = build_cnn(input_var)\n",
    "    else:\n",
    "        print(\"Unrecognized model type %r.\" % model)\n",
    "        return\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            print(inputs.shape)\n",
    "            print(targets)\n",
    "\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('y train', (600000,))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DenseLayer' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4e57e2fc8b94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mclassif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-4e57e2fc8b94>\u001b[0m in \u001b[0;36mclassif\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDenseLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_learning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mclassif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nolearn/lasagne.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_output_layer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_layer_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nolearn/lasagne.py\u001b[0m in \u001b[0;36minitialize_layers\u001b[1;34m(self, layers)\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0minput_layer_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_layer_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0minput_layer_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_params_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_layer_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_layer_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minput_layer_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DenseLayer' object does not support indexing"
     ]
    }
   ],
   "source": [
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from nolearn.lasagne import NeuralNet\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mega_hcad()\n",
    "\n",
    "def classif(X, y):\n",
    "    l = InputLayer(shape=(None, X.shape[1]))\n",
    "    l = DenseLayer(l, num_units=len(np.unique(y)), nonlinearity=softmax)\n",
    "    net = NeuralNet(l, update_learning_rate=0.01, input_shape=(None, X.shape[1]))\n",
    "    net.fit(X, y)\n",
    "    print(net.score(X, y))\n",
    "classif(np.array(X_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### housing density - correlation with damage?  Apparently not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "density = fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': -META ['pointx'],\n",
    "                                             'y': META ['pointy']}), np.ones(len(META)), normalize_buckets = False)\n",
    "\n",
    "y_shuffle = np.copy(np.array(Y_DATA['y200_mean']))\n",
    "np.random.shuffle(y_shuffle)\n",
    "damage = fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': -META ['pointx'],\n",
    "                                             'y': META ['pointy']}), np.array(Y_DATA['y200_mean']))\n",
    "damage = damage[density != 0]\n",
    "\n",
    "density = density[density != 0]\n",
    "print density.flatten()\n",
    "damage.flatten()\n",
    "print pearsonr(density.flatten(), damage.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
