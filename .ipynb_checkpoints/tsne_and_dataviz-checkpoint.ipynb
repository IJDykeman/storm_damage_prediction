{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "from tsne import bh_sne # thi is the correct tsne to use.  It's the one discussed btnw\n",
    "import sklearn.manifold\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from time import gmtime, strftime\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import math\n",
    "from scipy.stats.stats import pearsonr\n",
    "import random as rand\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict\n",
    "def memo(f):\n",
    "    memo = {}\n",
    "    def helper(x):\n",
    "        if x not in memo:            \n",
    "            memo[x] = f(x)\n",
    "        return memo[x]\n",
    "    return helper\n",
    "\n",
    "def zero_to_one(array):\n",
    "    array = array - np.min(array)\n",
    "    array = array/np.max(array)\n",
    "    return np.nan_to_num(array)\n",
    "\n",
    "\n",
    "# @memo\n",
    "def load_dataset(path, scale=True, include_hcad = True):\n",
    "    gc.collect()\n",
    "    data = pandas.read_hdf(path, '/df')\n",
    "    df = pandas.DataFrame(data)\n",
    "    if scale:\n",
    "        for label in df._get_numeric_data().columns:\n",
    "            if label != 'hcad':\n",
    "                df[label] = df[label].astype(float)\n",
    "                df[label] = zero_to_one(df[label])\n",
    "                df[label][df[label] > 1] = 1.0\n",
    "    if include_hcad:\n",
    "        df['hcad'] = df['hcad'].astype(int)\n",
    "    df = df.replace([np.inf, -np.inf], 1)\n",
    "    \n",
    "    return df.sort(['hcad']).fillna(0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def tsne(df_data, dest_folder, n = None, file_tag= \"\", embedded_dimensions=2, perplexity = 50):\n",
    "    result_2d = {}\n",
    "    result_2d['hcad'] = df_data['hcad'][:n]\n",
    "    df_data = df_data.drop('hcad', 1) # don't embed the hcad number!\n",
    "    df_data = np.array(df_data)[:n]\n",
    "    embedding = bh_sne(np.array(df_data)[:n], perplexity=perplexity)\n",
    "#     embedding = bh_sne(np.array(df_data))\n",
    "\n",
    "    result_2d['x'] = zero_to_one(embedding[:, 0])\n",
    "    result_2d['y'] = zero_to_one(embedding[:, 1])\n",
    "    result_2d = pandas.DataFrame.from_dict(result_2d)\n",
    "    #name = file_tag+\"_\"+\"_\".join(df_data.columns)[:40] + \"_n:\"+str(len(result))\n",
    "    #result.to_pickle(dest_folder+name)\n",
    "    return embedding\n",
    "\n",
    "def hist_2d(vis_x,vis_y):\n",
    "    hh, locx, locy = scipy.histogram2d(vis_x, vis_y, bins=[200,200])\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(30,30)\n",
    "    plt.imshow(np.flipud(hh.T),cmap='jet', interpolation='none', shape = (1,1))\n",
    "    plt.colorbar()\n",
    "    \n",
    "def get_where_img0_is_1(pddf):\n",
    "    img0_metadata = (META.loc[META['img0'] == 1])\n",
    "    return pddf.loc[pddf['hcad'].isin(list(img0_metadata['hcad']))]\n",
    "\n",
    "def pairwise_plot(pddf, sqrt = False):\n",
    "    if sqrt:\n",
    "        pddf = np.sqrt(pddf)\n",
    "    axes = pandas.tools.plotting.scatter_matrix(pddf, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def fast_show_ratio_plot(xy_points, y_data, log = False, normalize_buckets=True):\n",
    "    if log:\n",
    "        y_data = np.log(y_data)\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(3,3)\n",
    "    plt.hist(y_data)\n",
    "    plt.show()\n",
    "\n",
    "    buckets = defaultdict(list)\n",
    "    resolution = 200\n",
    "    x = np.array(xy_points['x'])\n",
    "    y = np.array(xy_points['y'])\n",
    "    H, xedges, yedges = numpy.histogram2d(x,y, bins=resolution, weights = y_data)\n",
    "    H_nums, dummy2, dummy1 = numpy.histogram2d(x,y, bins=resolution)\n",
    "    plt.show()\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(12,12)\n",
    "    if normalize_buckets:\n",
    "        H=H/H_nums\n",
    "    H[H_nums == 0.0] = numpy.nan\n",
    "#     if log:\n",
    "#         H = np.log(H)\n",
    "    \n",
    "\n",
    "    plt.imshow(H, \n",
    "               interpolation='nearest', cmap=cm.gist_rainbow)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    return np.nan_to_num(H)\n",
    "\n",
    "def colored_scatter(xy_points, y_data):\n",
    "        fig = plt.figure(frameon=False)\n",
    "        fig.set_size_inches(20,20)\n",
    "        plt.scatter(xy_points['x'], xy_points['y'], c=y_data,  marker='x', facecolor='b', cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "def load_mega_hcad():\n",
    "    # hcad = load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df.hd\")\n",
    "    hcad_data = [load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_100.hd\"),\n",
    "     load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_200.hd\"),\n",
    "     load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_400.hd\"),\n",
    "    load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_1000.hd\")]\n",
    "\n",
    "    mega_hcad = {}\n",
    "\n",
    "    for column in hcad_data[0]:\n",
    "        for index, dataset in enumerate(hcad_data):\n",
    "            mega_hcad[column+\"_\"+str(index)] = dataset[column]\n",
    "    mega_hcad = pandas.DataFrame.from_dict(mega_hcad).as_matrix()\n",
    "    y_data_np = Y_DATA.as_matrix()\n",
    "    X_train = np.expand_dims(np.expand_dims(mega_hcad[:6000], axis=1), axis=3)\n",
    "    y_train = y_data_np[:6000, 1] # limit training data amount, as opposed to 600000\n",
    "    print(\"y train\",y_train.shape)\n",
    "    X_val = np.expand_dims(np.expand_dims(mega_hcad[600000:700000], axis=1), axis=3)\n",
    "    y_val = y_data_np[600000:700000, 1]\n",
    "    X_test = np.expand_dims(np.expand_dims(mega_hcad[700000:], axis=1), axis=3)\n",
    "    y_test = y_data_np[700000:, 1]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# tsne_embed = pandas.read_pickle(\"/home/isaac/Desktop/devika/gitignored/img1_hcad/_mean_accrued_depr_pct_std_accrued_depr_p_n:104878\")\n",
    "# hist_2d(np.array(tsne_embed['x']),np.array(tsne_embed['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all the data at 200m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /home/isaac/Dropbox/data_for_brian/meta/df_meta.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/wind_features/hcad_interp_withoutpartial_rad200_hist16x16.mat.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/terrain_features/dsmgrid/terrain_200.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/y_df.hd in read-only mode\n"
     ]
    }
   ],
   "source": [
    "hcad = load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_200.hd\")\n",
    "# hcad = hcad[['hcad', 'mean_accrued_depr_pct', 'mean_bld_val', 'mean_land_val','mean_quality','mean_rcnld', 'mean_tot_mkt_val','mean_year_built','mean_year_remodeled']]\n",
    "META = load_dataset(\"/home/isaac/Dropbox/data_for_brian/meta/df_meta.hd\")\n",
    "WIND = load_dataset(\"/home/isaac/Dropbox/data_for_brian/wind_features/hcad_interp_withoutpartial_rad200_hist16x16.mat.hd\")\n",
    "TERRAIN = load_dataset(\"/home/isaac/Dropbox/data_for_brian/terrain_features/dsmgrid/terrain_200.hd\")\n",
    "\n",
    "Y_DATA = load_dataset(\"/home/isaac/Dropbox/data_for_brian/y_df.hd\")\n",
    "img0_y_data = get_where_img0_is_1(Y_DATA)\n",
    "\n",
    "\n",
    "# @memo\n",
    "img0_terrain_data = get_where_img0_is_1(TERRAIN)\n",
    "img0_wind_data = get_where_img0_is_1(WIND)\n",
    "img0_hcad_data = get_where_img0_is_1(hcad)\n",
    "img0_metadata = (META.loc[META['img0'] == 1])\n",
    "# print get_where_img0_is_1(WIND)\n",
    "def plot_on_map(pddf, meta = META):\n",
    "    for col in pddf.columns:\n",
    "        print(\"\\n\\n\\n\",col)\n",
    "        xy = pandas.DataFrame.from_dict({'x': -meta['pointx'],'y': meta['pointy']})\n",
    "        print(\"linear plot\")\n",
    "        fast_show_ratio_plot(xy,np.array(pddf[col]))\n",
    "        print(\"log plot\")\n",
    "        fast_show_ratio_plot(xy,np.array(pddf[col]), log = True)\n",
    "        colored_scatter(xy,np.array(pddf[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /home/isaac/Dropbox/data_for_brian/y_df.hd in read-only mode\n"
     ]
    }
   ],
   "source": [
    "Y_DATA = load_dataset(\"/home/isaac/Dropbox/data_for_brian/y_df.hd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_2d = tsne(img0_hcad_data ,\"/home/isaac/Desktop/devika/gitignored/6_dimensions_hcad_img0/\", \n",
    "                 file_tag =\"hcad_img0\", n=None, embedded_dimensions=2)\n",
    "fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': embedding_2d[:,0],\n",
    "                                             'y': embedding_2d[:,1]}), np.array(img0_y_data['y200_mean']))\n",
    "# print embedding_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans cluster the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "kmeans = KMeans(init='k-means++', n_clusters=30)\n",
    "kmeans.fit(embedding_2d)\n",
    "print kmeans.labels_\n",
    "\n",
    "# np.random.shuffle(kmeans.labels_)\n",
    "colored_scatter(pandas.DataFrame.from_dict({'x': - img0_metadata ['pointx'][:len(kmeans.labels_)],\n",
    "                                            'y': img0_metadata['pointy'][:len(kmeans.labels_)]}), kmeans.labels_)\n",
    "fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': - img0_metadata ['pointx'][:len(kmeans.labels_)],\n",
    "                                                 'y': img0_metadata['pointy'][:len(kmeans.labels_)]}),kmeans.labels_)\n",
    "\n",
    "for label in range(max(kmeans.labels_)+1):\n",
    "    print \"class\", label, \"damage:\",np.mean(np.array(img0_y_data['y200_mean'])[numpy.where(kmeans.labels_==label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pandas.read_pickle(\"/home/isaac/Desktop/devika/gitignored/testing_refactored_code/_mean_accrued_depr_pct_std_accrued_depr_p_n:100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print np.array(tsne_embed['x'])[0]\n",
    "# show_ratio_plot(tsne_embed, hcad['mean_bld_val'])\n",
    "# print get_meta()\n",
    "# print Y_DATA\n",
    "\n",
    "plot_on_map(img0_hcad_data, meta = img0_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 970 (CNMeM is disabled)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Opening /home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_100.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_200.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_400.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_1000.hd in read-only mode\n",
      "('y train', (6000,))\n",
      "(6000,)\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 500 took 0.884s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 2 of 500 took 0.844s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 3 of 500 took 0.845s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 4 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-56b3932207ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-56b3932207ba>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(model, num_epochs)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0mval_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mval_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inp, out_)\u001b[0m\n\u001b[0;32m   4179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4180\u001b[0m             newshape = (x.shape[:outdim - 1] +\n\u001b[1;32m-> 4181\u001b[1;33m                         (numpy.prod(x.shape[outdim - 1:]),))\n\u001b[0m\u001b[0;32m   4182\u001b[0m             \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mprod\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   2480\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2482\u001b[1;33m             \u001b[0mprod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m             return _methods._prod(a, axis=axis, dtype=dtype,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 260, 1),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 1),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 1))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 1),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 1))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=1,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def main(model='mlp', num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_mega_hcad()\n",
    "    print(y_train.shape)\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "\n",
    "    network = build_cnn(input_var)\n",
    "\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.squared_error(prediction.transpose(), target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.squared_error(test_prediction.transpose(),\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "#             print(inputs.shape)\n",
    "#             print(targets)\n",
    "\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### housing density - correlation with damage?  Apparently not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "density = fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': -META ['pointx'],\n",
    "                                             'y': META ['pointy']}), np.ones(len(META)), normalize_buckets = False)\n",
    "\n",
    "y_shuffle = np.copy(np.array(Y_DATA['y200_mean']))\n",
    "np.random.shuffle(y_shuffle)\n",
    "damage = fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': -META ['pointx'],\n",
    "                                             'y': META ['pointy']}), np.array(Y_DATA['y200_mean']))\n",
    "damage = damage[density != 0]\n",
    "\n",
    "density = density[density != 0]\n",
    "print density.flatten()\n",
    "damage.flatten()\n",
    "print pearsonr(density.flatten(), damage.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
