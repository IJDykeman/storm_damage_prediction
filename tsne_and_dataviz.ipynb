{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['rand', 'pylab']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "from tsne import bh_sne # thi is the correct tsne to use.  It's the one discussed btnw\n",
    "import sklearn.manifold\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from time import gmtime, strftime\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import math\n",
    "from scipy.stats.stats import pearsonr\n",
    "import random as rand\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict\n",
    "def memo(f):\n",
    "    memo = {}\n",
    "    def helper(x):\n",
    "        if x not in memo:            \n",
    "            memo[x] = f(x)\n",
    "        return memo[x]\n",
    "    return helper\n",
    "\n",
    "def zero_to_one(array):\n",
    "    array = array - np.min(array)\n",
    "    array = array/np.max(array)\n",
    "    return np.nan_to_num(array)\n",
    "\n",
    "\n",
    "@memo\n",
    "def load_dataset(path, scale=True):\n",
    "    gc.collect()\n",
    "    data = pandas.read_hdf(path, '/df')\n",
    "    df = pandas.DataFrame(data)\n",
    "    if scale:\n",
    "        for label in df._get_numeric_data().columns:\n",
    "            if label != 'hcad':\n",
    "                df[label] = df[label].astype(float)\n",
    "                df[label] = zero_to_one(df[label])\n",
    "                df[label][df[label] > 1] = 1.0\n",
    "    df['hcad'] = df['hcad'].astype(int)\n",
    "    df = df.replace([np.inf, -np.inf], 1)\n",
    "    \n",
    "    return df.sort(['hcad']).fillna(0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def tsne(df_data, dest_folder, n = None, file_tag= \"\", embedded_dimensions=2, perplexity = 50):\n",
    "    result_2d = {}\n",
    "    result_2d['hcad'] = df_data['hcad'][:n]\n",
    "    df_data = df_data.drop('hcad', 1) # don't embed the hcad number!\n",
    "    df_data = np.array(df_data)[:n]\n",
    "    embedding = bh_sne(np.array(df_data)[:n], perplexity=perplexity)\n",
    "#     embedding = bh_sne(np.array(df_data))\n",
    "\n",
    "    result_2d['x'] = zero_to_one(embedding[:, 0])\n",
    "    result_2d['y'] = zero_to_one(embedding[:, 1])\n",
    "    result_2d = pandas.DataFrame.from_dict(result_2d)\n",
    "    #name = file_tag+\"_\"+\"_\".join(df_data.columns)[:40] + \"_n:\"+str(len(result))\n",
    "    #result.to_pickle(dest_folder+name)\n",
    "    return embedding\n",
    "\n",
    "def hist_2d(vis_x,vis_y):\n",
    "    hh, locx, locy = scipy.histogram2d(vis_x, vis_y, bins=[200,200])\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(30,30)\n",
    "    plt.imshow(np.flipud(hh.T),cmap='jet', interpolation='none', shape = (1,1))\n",
    "    plt.colorbar()\n",
    "    \n",
    "def get_where_img0_is_1(pddf):\n",
    "    img0_metadata = (META.loc[META['img0'] == 1])\n",
    "    return pddf.loc[pddf['hcad'].isin(list(img0_metadata['hcad']))]\n",
    "\n",
    "def pairwise_plot(pddf, sqrt = False):\n",
    "    if sqrt:\n",
    "        pddf = np.sqrt(pddf)\n",
    "    axes = pandas.tools.plotting.scatter_matrix(pddf, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def fast_show_ratio_plot(xy_points, y_data, log = False, normalize_buckets=True):\n",
    "    if log:\n",
    "        y_data = np.log(y_data)\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(3,3)\n",
    "    plt.hist(y_data)\n",
    "    plt.show()\n",
    "\n",
    "    buckets = defaultdict(list)\n",
    "    resolution = 200\n",
    "    x = np.array(xy_points['x'])\n",
    "    y = np.array(xy_points['y'])\n",
    "    H, xedges, yedges = numpy.histogram2d(x,y, bins=resolution, weights = y_data)\n",
    "    H_nums, dummy2, dummy1 = numpy.histogram2d(x,y, bins=resolution)\n",
    "    plt.show()\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(12,12)\n",
    "    if normalize_buckets:\n",
    "        H=H/H_nums\n",
    "    H[H_nums == 0.0] = numpy.nan\n",
    "#     if log:\n",
    "#         H = np.log(H)\n",
    "    \n",
    "\n",
    "    plt.imshow(H, \n",
    "               interpolation='nearest', cmap=cm.gist_rainbow)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    return np.nan_to_num(H)\n",
    "\n",
    "def colored_scatter(xy_points, y_data):\n",
    "        fig = plt.figure(frameon=False)\n",
    "        fig.set_size_inches(20,20)\n",
    "        plt.scatter(xy_points['x'], xy_points['y'], c=y_data,  marker='x', facecolor='b', cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "def load_mega_hcad():\n",
    "    # hcad = load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df.hd\")\n",
    "    hcad_data = [load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_100.hd\"),\n",
    "     load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_200.hd\"),\n",
    "     load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_400.hd\"),\n",
    "    load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_1000.hd\")]\n",
    "\n",
    "    mega_hcad = {}\n",
    "\n",
    "    for column in hcad_data[0]:\n",
    "        for index, dataset in enumerate(hcad_data):\n",
    "            mega_hcad[column+\"_\"+str(index)] = dataset[column]\n",
    "    mega_hcad = pandas.DataFrame.from_dict(mega_hcad).as_matrix()\n",
    "    y_data_np = Y_DATA.as_matrix()\n",
    "    X_train = np.expand_dims(np.expand_dims(mega_hcad[:6000], axis=1), axis=3)\n",
    "    y_train = y_data_np[:6000, 1] # limit training data amount, as opposed to 600000\n",
    "    print(\"y train\",y_train.shape)\n",
    "    X_val = np.expand_dims(np.expand_dims(mega_hcad[600000:700000], axis=1), axis=3)\n",
    "    y_val = y_data_np[600000:700000, 1]\n",
    "    X_test = np.expand_dims(np.expand_dims(mega_hcad[700000:], axis=1), axis=3)\n",
    "    y_test = y_data_np[700000:, 1]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# tsne_embed = pandas.read_pickle(\"/home/isaac/Desktop/devika/gitignored/img1_hcad/_mean_accrued_depr_pct_std_accrued_depr_p_n:104878\")\n",
    "# hist_2d(np.array(tsne_embed['x']),np.array(tsne_embed['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all the data at 200m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /home/isaac/Dropbox/data_for_brian/meta/df_meta.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/wind_features/hcad_interp_withoutpartial_rad200_hist16x16.mat.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/terrain_features/dsmgrid/terrain_200.hd in read-only mode\n",
      "Opening /home/isaac/Dropbox/data_for_brian/y_df.hd in read-only mode\n"
     ]
    }
   ],
   "source": [
    "hcad = load_dataset(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_200.hd\")\n",
    "# hcad = hcad[['hcad', 'mean_accrued_depr_pct', 'mean_bld_val', 'mean_land_val','mean_quality','mean_rcnld', 'mean_tot_mkt_val','mean_year_built','mean_year_remodeled']]\n",
    "META = load_dataset(\"/home/isaac/Dropbox/data_for_brian/meta/df_meta.hd\")\n",
    "WIND = load_dataset(\"/home/isaac/Dropbox/data_for_brian/wind_features/hcad_interp_withoutpartial_rad200_hist16x16.mat.hd\")\n",
    "TERRAIN = load_dataset(\"/home/isaac/Dropbox/data_for_brian/terrain_features/dsmgrid/terrain_200.hd\")\n",
    "\n",
    "Y_DATA = load_dataset(\"/home/isaac/Dropbox/data_for_brian/y_df.hd\")\n",
    "img0_y_data = get_where_img0_is_1(Y_DATA)\n",
    "\n",
    "\n",
    "# @memo\n",
    "img0_terrain_data = get_where_img0_is_1(TERRAIN)\n",
    "img0_wind_data = get_where_img0_is_1(WIND)\n",
    "img0_hcad_data = get_where_img0_is_1(hcad)\n",
    "img0_metadata = (META.loc[META['img0'] == 1])\n",
    "# print get_where_img0_is_1(WIND)\n",
    "def plot_on_map(pddf, meta = META):\n",
    "    for col in pddf.columns:\n",
    "        print(\"\\n\\n\\n\",col)\n",
    "        xy = pandas.DataFrame.from_dict({'x': -meta['pointx'],'y': meta['pointy']})\n",
    "        print(\"linear plot\")\n",
    "        fast_show_ratio_plot(xy,np.array(pddf[col]))\n",
    "        print(\"log plot\")\n",
    "        fast_show_ratio_plot(xy,np.array(pddf[col]), log = True)\n",
    "        colored_scatter(xy,np.array(pddf[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /home/isaac/Dropbox/data_for_brian/y_df.hd in read-only mode\n"
     ]
    }
   ],
   "source": [
    "Y_DATA = load_dataset(\"/home/isaac/Dropbox/data_for_brian/y_df.hd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_2d = tsne(img0_hcad_data ,\"/home/isaac/Desktop/devika/gitignored/6_dimensions_hcad_img0/\", \n",
    "                 file_tag =\"hcad_img0\", n=None, embedded_dimensions=2)\n",
    "fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': embedding_2d[:,0],\n",
    "                                             'y': embedding_2d[:,1]}), np.array(img0_y_data['y200_mean']))\n",
    "# print embedding_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans cluster the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "kmeans = KMeans(init='k-means++', n_clusters=30)\n",
    "kmeans.fit(embedding_2d)\n",
    "print kmeans.labels_\n",
    "\n",
    "# np.random.shuffle(kmeans.labels_)\n",
    "colored_scatter(pandas.DataFrame.from_dict({'x': - img0_metadata ['pointx'][:len(kmeans.labels_)],\n",
    "                                            'y': img0_metadata['pointy'][:len(kmeans.labels_)]}), kmeans.labels_)\n",
    "fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': - img0_metadata ['pointx'][:len(kmeans.labels_)],\n",
    "                                                 'y': img0_metadata['pointy'][:len(kmeans.labels_)]}),kmeans.labels_)\n",
    "\n",
    "for label in range(max(kmeans.labels_)+1):\n",
    "    print \"class\", label, \"damage:\",np.mean(np.array(img0_y_data['y200_mean'])[numpy.where(kmeans.labels_==label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pandas.read_pickle(\"/home/isaac/Desktop/devika/gitignored/testing_refactored_code/_mean_accrued_depr_pct_std_accrued_depr_p_n:100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print np.array(tsne_embed['x'])[0]\n",
    "# show_ratio_plot(tsne_embed, hcad['mean_bld_val'])\n",
    "# print get_meta()\n",
    "# print Y_DATA\n",
    "\n",
    "plot_on_map(img0_hcad_data, meta = img0_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ### create a [point, 100m, 200m, ... , point, 100m, 200m, ...] data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print load_mega_hcad()[0].shape\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "# theano.config.optimizer='fast_compile'\n",
    "# theano.config.exception_verbosity='high'\n",
    "\n",
    "import lasagne\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None,1, 260),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv1DLayer(\n",
    "            network, num_filters=32, filter_size=5,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "#     network = lasagne.layers.MaxPool1DLayer(network, pool_size=2)\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "#     network = lasagne.layers.Conv2DLayer(\n",
    "#             network, num_filters=32, filter_size=(5, 5),\n",
    "#             nonlinearity=lasagne.nonlinearities.rectify)\n",
    "#     network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=50,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=1,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "# ############################## Main program ################################\n",
    "# Everything else will be handled in our main program now. We could pull out\n",
    "# more functions to better separate the code, but it wouldn't make it any\n",
    "# easier to read.\n",
    "\n",
    "def main(num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_mega_hcad()\n",
    "    #print(X_train)\n",
    "    #print(y_train)\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor3('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "\n",
    "    network = build_cnn(input_var)\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.squared_error(prediction, target_var) ## changed from categorical cross entropy\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            print(inputs.shape)\n",
    "            print(targets)\n",
    "            train_err += train_fn(inputs.transpose(), targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "y train (6000,)\n",
      "(6000,)\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 500 took 0.857s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 2 of 500 took 0.874s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 3 of 500 took 0.875s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 4 of 500 took 0.875s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 5 of 500 took 0.873s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 6 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 7 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 8 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 9 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 10 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 11 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 12 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 13 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 14 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 15 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 16 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 17 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 18 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 19 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 20 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 21 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 22 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 23 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 24 of 500 took 0.845s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 25 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 26 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 27 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 28 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 29 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 30 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 31 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 32 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 33 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 34 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 35 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 36 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 37 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 38 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 39 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 40 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 41 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 42 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 43 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 44 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 45 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 46 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 47 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 48 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 49 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 50 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 51 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 52 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 53 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 54 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 55 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 56 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 57 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 58 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 59 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 60 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 61 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 62 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 63 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 64 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 65 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 66 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 67 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 68 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 69 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 70 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 71 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 72 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 73 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 74 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 75 of 500 took 0.847s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 76 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 77 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 78 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 79 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 80 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 81 of 500 took 0.847s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 82 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 83 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 84 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 85 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 86 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 87 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 88 of 500 took 0.857s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 89 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 90 of 500 took 0.852s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 91 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 92 of 500 took 0.844s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 93 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 94 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 95 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 96 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 97 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 98 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 99 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 100 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 101 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 102 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 103 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 104 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 105 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 106 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 107 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 108 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 109 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 110 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 111 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 112 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 113 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 114 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 115 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 116 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 117 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 118 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 119 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 120 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 121 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 122 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 123 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 124 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 125 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 126 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 127 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 128 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 129 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 130 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 131 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 132 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 133 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 134 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 135 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 136 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 137 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 138 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 139 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 140 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 141 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 142 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 143 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 144 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 145 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 146 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 147 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 148 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 149 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 150 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 151 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 152 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 153 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 154 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 155 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 156 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 157 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 158 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 159 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 160 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 161 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 162 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 163 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 164 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 165 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 166 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 167 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 168 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 169 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 170 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 171 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 172 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 173 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 174 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 175 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 176 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 177 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 178 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 179 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 180 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 181 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 182 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 183 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 184 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 185 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 186 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 187 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 188 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 189 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 190 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 191 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 192 of 500 took 0.849s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 193 of 500 took 0.859s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 194 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 195 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 196 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 197 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 198 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 199 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 200 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 201 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 202 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 203 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 204 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 205 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 206 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 207 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 208 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 209 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 210 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 211 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 212 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 213 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 214 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 215 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 216 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 217 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 218 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 219 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 220 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 221 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 222 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 223 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 224 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 225 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 226 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 227 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 228 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 229 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 230 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 231 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 232 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 233 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 234 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 235 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 236 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 237 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 238 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 239 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 240 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 241 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 242 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 243 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 244 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 245 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 246 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 247 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 248 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 249 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 250 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 251 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 252 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 253 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 254 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 255 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 256 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 257 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 258 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 259 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 260 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 261 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 262 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 263 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 264 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 265 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 266 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 267 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 268 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 269 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 270 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 271 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 272 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 273 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 274 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 275 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 276 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 277 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 278 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 279 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 280 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 281 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 282 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 283 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 284 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 285 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 286 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 287 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 288 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 289 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 290 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 291 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 292 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 293 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 294 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 295 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 296 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 297 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 298 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 299 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 300 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 301 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 302 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 303 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 304 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 305 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 306 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 307 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 308 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 309 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 310 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 311 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 312 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 313 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 314 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 315 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 316 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 317 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 318 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 319 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 320 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 321 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 322 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 323 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 324 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 325 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 326 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 327 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 328 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 329 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 330 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 331 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 332 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 333 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 334 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 335 of 500 took 0.840s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 336 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 337 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 338 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 339 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 340 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 341 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 342 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 343 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 344 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 345 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 346 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 347 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 348 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 349 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 350 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 351 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 352 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 353 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 354 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 355 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 356 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 357 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 358 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 359 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 360 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 361 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 362 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 363 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 364 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 365 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 366 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 367 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 368 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 369 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 370 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 371 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 372 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 373 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 374 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 375 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 376 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 377 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 378 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 379 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 380 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 381 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 382 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 383 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 384 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 385 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 386 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 387 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 388 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 389 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 390 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 391 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 392 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 393 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 394 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 395 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 396 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 397 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 398 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 399 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 400 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 401 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 402 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 403 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 404 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 405 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 406 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 407 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 408 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 409 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 410 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 411 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 412 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 413 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 414 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 415 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 416 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 417 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 418 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 419 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 420 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 421 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 422 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 423 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 424 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 425 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 426 of 500 took 0.859s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 427 of 500 took 0.854s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 428 of 500 took 0.920s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 429 of 500 took 0.860s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 430 of 500 took 0.848s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 431 of 500 took 0.846s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 432 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 433 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 434 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 435 of 500 took 0.859s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 436 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 437 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 438 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 439 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 440 of 500 took 0.848s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 441 of 500 took 0.844s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 442 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 443 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 444 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 445 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 446 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 447 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 448 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 449 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 450 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 451 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 452 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 453 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 454 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 455 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 456 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 457 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 458 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 459 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 460 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 461 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 462 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 463 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 464 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 465 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 466 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 467 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 468 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 469 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 470 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 471 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 472 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 473 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 474 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 475 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 476 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 477 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 478 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 479 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 480 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 481 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 482 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 483 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 484 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 485 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 486 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 487 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 488 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 489 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 490 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 491 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 492 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 493 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 494 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 495 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 496 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 497 of 500 took 0.841s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 498 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 499 of 500 took 0.843s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Epoch 500 of 500 took 0.842s\n",
      "  training loss:\t\t0.755167\n",
      "  validation loss:\t\t0.834160\n",
      "  validation accuracy:\t\t83.42 %\n",
      "Final results:\n",
      "  test loss:\t\t\t0.890371\n",
      "  test accuracy:\t\t89.04 %\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 260, 1),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 1),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 1))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 1),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 1))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=1,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def main(model='mlp', num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_mega_hcad()\n",
    "    print(y_train.shape)\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "\n",
    "    network = build_cnn(input_var)\n",
    "\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.squared_error(prediction.transpose(), target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.squared_error(test_prediction.transpose(),\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "#             print(inputs.shape)\n",
    "#             print(targets)\n",
    "\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### housing density - correlation with damage?  Apparently not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "density = fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': -META ['pointx'],\n",
    "                                             'y': META ['pointy']}), np.ones(len(META)), normalize_buckets = False)\n",
    "\n",
    "y_shuffle = np.copy(np.array(Y_DATA['y200_mean']))\n",
    "np.random.shuffle(y_shuffle)\n",
    "damage = fast_show_ratio_plot(pandas.DataFrame.from_dict({'x': -META ['pointx'],\n",
    "                                             'y': META ['pointy']}), np.array(Y_DATA['y200_mean']))\n",
    "damage = damage[density != 0]\n",
    "\n",
    "density = density[density != 0]\n",
    "print density.flatten()\n",
    "damage.flatten()\n",
    "print pearsonr(density.flatten(), damage.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
