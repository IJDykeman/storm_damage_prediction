{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created fast dictionary\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import functools\n",
    "from time import gmtime, strftime\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.stats.stats import pearsonr\n",
    "import random as rand\n",
    "import pyproj\n",
    "import functools\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import scipy\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "\n",
    "    \n",
    "def memoize(obj, maxlen = 2000):\n",
    "    \"\"\"A decorator to cache advice objects using the advice key\"\"\"\n",
    "    cache = obj.cache = {}\n",
    "    deck = obj.deck = deque([], maxlen = maxlen)\n",
    "\n",
    "    @functools.wraps(obj)\n",
    "    def memoizer(*args, **kwargs):\n",
    "        key = args[0]\n",
    "        if key not in cache:\n",
    "            if len(deck) == deck.maxlen:\n",
    "              del cache[deck.popleft()[0]]\n",
    "            temp = obj(*args, **kwargs)\n",
    "            cache[key] = temp\n",
    "            deck.append((key, temp))\n",
    "        return cache[key]\n",
    "        \n",
    "    return memoizer\n",
    "\n",
    "\n",
    "n_samples = 800000\n",
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "\n",
    "\n",
    "with open(\"../training_data/D_cbow_pdw_8B.pkl\", 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "with open(\"../training_data/training_data.pkl\", 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "\n",
    "for word in model:\n",
    "    model[word] = np.append(model[word], [0,0,0,0])\n",
    "    \n",
    "model[\"UNK\"] = np.zeros([504])\n",
    "model[\"UNK\"][500]=1\n",
    "model[\"GO\"] = np.zeros([504])\n",
    "model[\"GO\"][501]=1\n",
    "model[\"STOP\"] = np.zeros([504])\n",
    "model[\"STOP\"][502]=1\n",
    "model[\"PAD\"] = np.zeros([504])\n",
    "model[\"PAD\"][503]=1\n",
    "    \n",
    "\n",
    "dictionary_dict = defaultdict(list)\n",
    "for i in range(len(dictionary[0])):\n",
    "    dictionary_dict[dictionary[0][i]].append(dictionary[1][i])\n",
    "print (\"created fast dictionary\")\n",
    "    \n",
    "\n",
    "\n",
    "# print (get_hypernyms_to_adjectives(\"mansion\"))\n",
    "# print (get_hypernyms_to_adjectives(\"hut\"))\n",
    "# print (get_hypernyms_to_adjectives(\"hut\"))\n",
    "\n",
    "\n",
    "all_words = list(set(dictionary[0]))\n",
    "all_words.extend([\"PAD\",\"UNK\", \"STOP\",  \"GO\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "words = model.keys()\n",
    "random.shuffle(words)\n",
    "\n",
    "train_words = words[:int(len(words) * .75)]\n",
    "val_words = words[int(len(words) * .75) : ]\n",
    "\n",
    "\n",
    "\n",
    "def get_vec(word):\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    else:\n",
    "        return model['UNK']\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a,b)/(np.sqrt(np.sum(np.power(a,2)))*np.sqrt(np.sum(np.power(b,2))))\n",
    "\n",
    "def sort_topn(l, n):\n",
    "    return list(reversed(sorted(l, key = lambda x: x[1])))[:n]\n",
    "\n",
    "all_word_vectors = np.zeros([len(all_words), 504])\n",
    "i=0\n",
    "for word in all_words:\n",
    "    all_word_vectors[i,:] = model[word]\n",
    "    i+=1\n",
    "\n",
    "norms = np.linalg.norm(all_word_vectors, ord=2, axis=1)\n",
    "all_word_vectors = all_word_vectors / norms[:,None]\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "kd_forest = []\n",
    "\n",
    "num_trees = 7\n",
    "for i in range(num_trees):\n",
    "    num_words = len(all_words)\n",
    "    start = num_words/num_trees * i\n",
    "    end = start + num_words/num_trees\n",
    "    word_tree = KDTree(all_word_vectors[start:end,:])\n",
    "    kd_forest.append(word_tree)\n",
    "\n",
    "# for i in word_tree.query([model['dog']], k=10)[1][0]:\n",
    "#     print (all_words[i])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def alert(alert_contents):\n",
    "    server = smtplib.SMTP( \"smtp.gmail.com\", 587 )\n",
    "    server.starttls()\n",
    "    server.login( 'processing.update.system@gmail.com', 'updatesystem' )\n",
    "    server.sendmail( 'processing.update.system@gmail.com', 'ijdykeman@gmail.com', alert_contents )\n",
    "    \n",
    "    \n",
    "seq_max_len = 2\n",
    "word_vector_width = 504\n",
    "\n",
    "\n",
    "# @memoize\n",
    "\n",
    "\n",
    "def get_vectors_for_definition(defitnition):\n",
    "    definition = map(get_vec, defitnition)[:seq_max_len-1]\n",
    "    definition.append(model[\"STOP\"])\n",
    "    padding = np.array([model[\"PAD\"]]*(seq_max_len - len(definition)))\n",
    "    definition = np.array(definition)\n",
    "    if len(definition) != seq_max_len:\n",
    "        definition = np.append(definition, padding, axis = 0)\n",
    "    return definition\n",
    "\n",
    "def get_batch(n, val = False):\n",
    "    words = []\n",
    "    defs = []\n",
    "    y = []\n",
    "    for _ in range(n):\n",
    "        i = random.randint(0,len(dictionary[0])-1)\n",
    "        if val:\n",
    "            i=i%10\n",
    "        else:\n",
    "            i += random.randint(1,9)\n",
    "        i=i%len(dictionary[0])\n",
    "        word = get_vec(dictionary[0][i])\n",
    "        definition = get_vectors_for_definition(dictionary[1][i])\n",
    "        words.append(word)\n",
    "        defs.append(definition)\n",
    "        y.append([1,0])\n",
    "\n",
    "    words = np.array(words)\n",
    "    defs = np.array(defs)\n",
    "    y= np.array(y)\n",
    "    return words, defs, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs\n",
      "puppy\n",
      "pet\n",
      "kennel\n",
      "dog\n",
      "poodle\n",
      "rabbit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_most_similar(vec, exclude = None):\n",
    "    topn = [(\"none\", -2)]\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for i, tree in enumerate(kd_forest):\n",
    "        word = all_words[tree.query([vec], k=1)[1][0] + num_words/len(kd_forest)*i]\n",
    "        print word\n",
    "        words.append(word)\n",
    "    for word in words:\n",
    "        if word != exclude:\n",
    "            topn.append((word, cosine_similarity(vec, model[word])))\n",
    "            topn = sort_topn(topn, 15)\n",
    "    return topn[0][0]\n",
    "\n",
    "# def get_most_similar(vec, exclude = None):\n",
    "#     topn = [(\"none\", -2)]\n",
    "    \n",
    "#     words = []\n",
    "#     for i in kd_forest[0].query([vec], k=2)[1][0]:\n",
    "#         words.append(all_words[i])\n",
    "#     for word in words:\n",
    "#         if word != exclude:\n",
    "#             topn.append((word, cosine_similarity(vec, model[word])))\n",
    "#             topn = sort_topn(topn, 15)\n",
    "#     return topn[0][0]\n",
    "\n",
    "get_most_similar(model['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogs', 'puppy', 'pet', 'kennel', 'dog', 'poodle', 'rabbit']\n"
     ]
    }
   ],
   "source": [
    "vec = model['dog']\n",
    "def f(i):\n",
    "    return all_words[kd_forest[i].query([vec], k=1)[1][0] + num_words/len(kd_forest)*i]#[0]\n",
    "\n",
    "import multiprocessing\n",
    "pool = multiprocessing.Pool(7)\n",
    "print pool.map(f, range(len(kd_forest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import functools\n",
    "from time import gmtime, strftime\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.stats.stats import pearsonr\n",
    "import random as rand\n",
    "import pyproj\n",
    "import functools\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import scipy\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "\n",
    "    \n",
    "def memoize(obj, maxlen = 200):\n",
    "    \"\"\"A decorator to cache advice objects using the advice key\"\"\"\n",
    "    cache = obj.cache = {}\n",
    "    deck = obj.deck = deque([], maxlen = maxlen)\n",
    "\n",
    "    @functools.wraps(obj)\n",
    "    def memoizer(*args, **kwargs):\n",
    "        key = args[0]\n",
    "        if key not in cache:\n",
    "            if len(deck) == deck.maxlen:\n",
    "              del cache[deck.popleft()[0]]\n",
    "            temp = obj(*args, **kwargs)\n",
    "            cache[key] = temp\n",
    "            deck.append((key, temp))\n",
    "        return cache[key]\n",
    "        \n",
    "    return memoizer\n",
    "\n",
    "\n",
    "n_samples = 800000\n",
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "\n",
    "\n",
    "with open(\"../training_data/D_cbow_pdw_8B.pkl\", 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "with open(\"../training_data/training_data.pkl\", 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "\n",
    "for word in model:\n",
    "    model[word] = np.append(model[word], [0,0,0,0])\n",
    "    \n",
    "model[\"UNK\"] = np.zeros([504])\n",
    "model[\"UNK\"][500]=1\n",
    "model[\"GO\"] = np.zeros([504])\n",
    "model[\"GO\"][501]=1\n",
    "model[\"STOP\"] = np.zeros([504])\n",
    "model[\"STOP\"][502]=1\n",
    "model[\"PAD\"] = np.zeros([504])\n",
    "model[\"PAD\"][503]=1\n",
    "    \n",
    "\n",
    "dictionary_dict = defaultdict(list)\n",
    "for i in range(len(dictionary[0])):\n",
    "    dictionary_dict[dictionary[0][i]].append(dictionary[1][i])\n",
    "print (\"created fast dictionary\")\n",
    "    \n",
    "\n",
    "\n",
    "# print (get_hypernyms_to_adjectives(\"mansion\"))\n",
    "# print (get_hypernyms_to_adjectives(\"hut\"))\n",
    "# print (get_hypernyms_to_adjectives(\"hut\"))\n",
    "\n",
    "\n",
    "all_words = list(set(dictionary[0]))\n",
    "all_words.extend([\"PAD\",\"UNK\", \"STOP\",  \"GO\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "words = model.keys()\n",
    "random.shuffle(words)\n",
    "\n",
    "train_words = words[:int(len(words) * .75)]\n",
    "val_words = words[int(len(words) * .75) : ]\n",
    "\n",
    "\n",
    "\n",
    "def get_vec(word):\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    else:\n",
    "        return model['UNK']\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a,b)/(np.sqrt(np.sum(np.power(a,2)))*np.sqrt(np.sum(np.power(b,2))))\n",
    "\n",
    "def sort_topn(l, n):\n",
    "    return list(reversed(sorted(l, key = lambda x: x[1])))[:n]\n",
    "\n",
    "all_word_vectors = np.zeros([len(all_words), 504])\n",
    "i=0\n",
    "for word in all_words:\n",
    "    all_word_vectors[i,:] = model[word]\n",
    "    i+=1\n",
    "\n",
    "norms = np.linalg.norm(all_word_vectors, ord=2, axis=1)\n",
    "all_word_vectors = all_word_vectors / norms[:,None]\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "word_tree = KDTree(all_word_vectors)\n",
    "\n",
    "for i in word_tree.query([model['dog']], k=10)[1][0]:\n",
    "    print (all_words[i])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def alert(alert_contents):\n",
    "    server = smtplib.SMTP( \"smtp.gmail.com\", 587 )\n",
    "    server.starttls()\n",
    "    server.login( 'processing.update.system@gmail.com', 'updatesystem' )\n",
    "    server.sendmail( 'processing.update.system@gmail.com', 'ijdykeman@gmail.com', alert_contents )\n",
    "    \n",
    "    \n",
    "seq_max_len = 2\n",
    "word_vector_width = 504\n",
    "\n",
    "\n",
    "# @memoize\n",
    "def get_most_similar(vec, exclude = None):\n",
    "    topn = [(\"none\", -2)]\n",
    "    \n",
    "    words = []\n",
    "    for i in word_tree.query([vec], k=20)[1][0]:\n",
    "        words.append(all_words[i])\n",
    "    for word in words:\n",
    "        if word != exclude:\n",
    "            topn.append((word, cosine_similarity(vec, model[word])))\n",
    "            topn = sort_topn(topn, 15)\n",
    "    return topn[0][0]\n",
    "\n",
    "def get_vectors_for_definition(defitnition):\n",
    "    definition = map(get_vec, defitnition)[:seq_max_len-1]\n",
    "    definition.append(model[\"STOP\"])\n",
    "    padding = np.array([model[\"PAD\"]]*(seq_max_len - len(definition)))\n",
    "    definition = np.array(definition)\n",
    "    if len(definition) != seq_max_len:\n",
    "        definition = np.append(definition, padding, axis = 0)\n",
    "    return definition\n",
    "\n",
    "def get_batch(n, val = False):\n",
    "    words = []\n",
    "    defs = []\n",
    "    y = []\n",
    "    for _ in range(n):\n",
    "        i = random.randint(0,len(dictionary[0])-1)\n",
    "        if val:\n",
    "            i=i%10\n",
    "        else:\n",
    "            i += random.randint(1,9)\n",
    "        i=i%len(dictionary[0])\n",
    "        word = get_vec(dictionary[0][i])\n",
    "        definition = get_vectors_for_definition(dictionary[1][i])\n",
    "        words.append(word)\n",
    "        defs.append(definition)\n",
    "        y.append([1,0])\n",
    "\n",
    "    words = np.array(words)\n",
    "    defs = np.array(defs)\n",
    "    y= np.array(y)\n",
    "    return words, defs, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
