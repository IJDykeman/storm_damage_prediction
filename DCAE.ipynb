{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%pylab inline\n",
    "\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from os import listdir\n",
    "import functools\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (4.0, 4.0)\n",
    "\n",
    "import pylab\n",
    "from tsne import bh_sne\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy as np\n",
    "import sklearn.manifold\n",
    "import os\n",
    "import gc\n",
    "from time import gmtime, strftime\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import math\n",
    "from scipy.stats.stats import pearsonr\n",
    "import random as rand\n",
    "import pyproj\n",
    "import functools\n",
    "import pickle \n",
    "\n",
    "image_width = 128\n",
    "\n",
    "def display(image, min = 0.0, max = 1.0):\n",
    "    plt.imshow(image, cmap = plt.get_cmap('gray'), interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def display(image):\n",
    "    plt.imshow(image, cmap = plt.get_cmap('gray'), interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "def memoize(obj, maxlen = 2000):\n",
    "    \"\"\"A decorator to cache advice objects using the advice key\"\"\"\n",
    "    cache = obj.cache = {}\n",
    "    deck = obj.deck = deque([], maxlen = maxlen)\n",
    "\n",
    "    @functools.wraps(obj)\n",
    "    def memoizer(*args, **kwargs):\n",
    "        key = args[0]\n",
    "        if key not in cache:\n",
    "            if len(deck) == deck.maxlen:\n",
    "              del cache[deck.popleft()[0]]\n",
    "            temp = obj(*args, **kwargs)\n",
    "            cache[key] = temp\n",
    "            deck.append((key, temp))\n",
    "        return cache[key]\n",
    "        \n",
    "    return memoizer\n",
    "\n",
    "@memoize\n",
    "def load_mat_from_file(key):\n",
    "    f = file(\"/home/isaac/Desktop/devika/ARCGIS/ArcGis/pickle_files/\"+key+\".npy\",\"rb\")\n",
    "    return np.load(f)\n",
    "\n",
    "@memoize\n",
    "def get_file_containing(housex, housey):\n",
    "#         print \"house coords:\", housex, housey\n",
    "    for key in file_attributes:\n",
    "        block = file_attributes[key]\n",
    "    #         print block['xllcorner'], block['yllcorner']\n",
    "        if block['xllcorner'] < housex < block['xllcorner'] + block['width']:\n",
    "            if block['yllcorner'] < housey < block['yllcorner'] + block['height']:\n",
    "                return key\n",
    "    assert(1==0)\n",
    "\n",
    "def imagex(housex, housey, block):\n",
    "    return (int(housex)-block['xllcorner'])/5\n",
    "def imagey(housex, housey, block):\n",
    "    return (int(housey)-block['yllcorner'])/5\n",
    "\n",
    "\n",
    "\n",
    "#housex and housey in meters\n",
    "def get_heightmap_around(housex, housey, window_width_pixels = 28):\n",
    "    \n",
    "    def image_slice(key, housex, housey, radius = window_width_pixels/2):\n",
    "        x = housex\n",
    "        y = housey\n",
    "        block = file_attributes[key]\n",
    "        mat = load_mat_from_file(key)\n",
    "        x1 = np.max([0,              imagex(x,y,block) - radius])\n",
    "        x2 = np.min([block['ncols'], imagex(x,y,block) + radius])\n",
    "        \n",
    "        y1 = np.max([0,              imagey(x,y,block) - radius])\n",
    "        y2 = np.min([block['nrows'], imagey(x,y,block) + radius])\n",
    "        \n",
    "        assert(x1 <= x2)\n",
    "        assert(y1 <= y2)\n",
    "        return  mat[block['nrows'] - y2:block['nrows'] - y1, x1:x2]\n",
    "    \n",
    "    window_width_feet = window_width_pixels * 5\n",
    "    housex /= .3048 #convert to feet\n",
    "    housey /= .3048\n",
    "    x = housex\n",
    "    y = housey\n",
    "    ul = get_file_containing(x - window_width_feet/2, y - window_width_feet/2)\n",
    "    ur = get_file_containing(x + window_width_feet/2, y - window_width_feet/2)\n",
    "    ll = get_file_containing(x - window_width_feet/2, y + window_width_feet/2)\n",
    "    lr = get_file_containing(x + window_width_feet/2, y + window_width_feet/2)\n",
    "\n",
    "    for key in [ul, ur, ll, lr]:\n",
    "        image = image_slice(key, x, y)\n",
    "#         print image.shape\n",
    "        if image.shape == (window_width_pixels, window_width_pixels):\n",
    "            return image\n",
    "\n",
    "    \n",
    "\n",
    "    if ul == ur and lr == ll : # horizontal split\n",
    "\n",
    "\n",
    "        result = np.zeros([window_width_pixels, window_width_pixels])\n",
    "        upper_slice = image_slice(ul, housex, housey)\n",
    "        lower_slice = image_slice(ll, housex, housey)\n",
    "        \n",
    "        result[:upper_slice.shape[0], :] = upper_slice\n",
    "        result[ window_width_pixels - lower_slice.shape[0]:, :] = lower_slice\n",
    "\n",
    "\n",
    "        return result\n",
    "    elif ul == ll and ur == lr and ul != ur and ll != lr: # vertical split\n",
    "        result = np.zeros([window_width_pixels, window_width_pixels])\n",
    "        left_slice = image_slice(ll, housex, housey)\n",
    "        right_slice = image_slice(ur, housex, housey)\n",
    "#         print left_slice.shape\n",
    "#         print right_slice.shape\n",
    "        \n",
    "        result[:, :left_slice.shape[1]] = left_slice\n",
    "        result[:, window_width_pixels - right_slice.shape[1]:] = right_slice\n",
    "\n",
    "#         plt.imshow(result)\n",
    "#         plt.show()\n",
    "#         display(result)\n",
    "\n",
    "        return result\n",
    "    else: # four way split\n",
    "#         print \"four way\"\n",
    "        ll_slice = image_slice(ll, housex, housey)\n",
    "        ul_slice = image_slice(ul, housex, housey)\n",
    "        lr_slice = image_slice(lr, housex, housey)\n",
    "        ur_slice = image_slice(ur, housex, housey)\n",
    "        \n",
    "        result = np.zeros([window_width_pixels, window_width_pixels])\n",
    "        result[:ll_slice.shape[0], :ll_slice.shape[1]] = ll_slice\n",
    "        result[:lr_slice.shape[0], window_width_pixels - lr_slice.shape[1]:] = lr_slice\n",
    "        \n",
    "        result[window_width_pixels - ul_slice.shape[0]:, :ul_slice.shape[1]] = ul_slice\n",
    "        result[window_width_pixels - ur_slice.shape[0]:, window_width_pixels - ur_slice.shape[1]:] = ur_slice\n",
    "#         display(result)\n",
    "        return result\n",
    "\n",
    "        \n",
    "\n",
    "def window_violated_chunk_borders(housex, housey):\n",
    "    corners = [(housex-window_width_feet/2, housey-window_width_feet/2),\n",
    "              (housex-window_width_feet/2, housey+window_width_feet/2),\n",
    "              (housex+window_width_feet/2, housey+window_width_feet/2),\n",
    "              (housex+window_width_feet/2, housey-window_width_feet/2)]\n",
    "    files = map(lambda x: get_file_containing(x[0], x[1]), corners)\n",
    "    return len(set(files)) > 1\n",
    "\n",
    "\n",
    "\n",
    "file_attributes = {}\n",
    "\n",
    "mypath = \"/home/isaac/Desktop/devika/ARCGIS/ArcGis/ascii_files\"\n",
    "for filename in [f for f in listdir(mypath) if isfile(join(mypath, f))]: \n",
    "    attributes = {}\n",
    "    with open(mypath + \"/\" + filename) as FileObj:\n",
    "        for index, line in enumerate(FileObj):\n",
    "            if(index < 6):\n",
    "#                 print line\n",
    "                attributes[line.split(\" \")[0]] = int(line.split(\" \")[-1][:-2])\n",
    "            else:\n",
    "                break # don't load the other lines into memory becuase that's a waste of time.\n",
    "\n",
    "    attributes['width'] = attributes['ncols'] * attributes['cellsize']\n",
    "    attributes['height'] = attributes['nrows'] * attributes['cellsize']\n",
    "    file_attributes[filename] = attributes\n",
    "print \"loaded all file header attributes into dict\"\n",
    "\n",
    "\n",
    "def load_data(path, normalize_columns = False, only_columns_containing = \"\"):\n",
    "    print(\"loading...\")\n",
    "    gc.collect() # collect garbage\n",
    "    data = pandas.read_hdf(path, '/df')\n",
    "    df = pandas.DataFrame(data)\n",
    "    df = df.sort(['hcad']).fillna(0)\n",
    "    df = df.reset_index()#(ascending=True)\n",
    "\n",
    "    data_dict = {}\n",
    "    for label in set(df._get_numeric_data().columns).union({'hcad'}):\n",
    "        # union hcad to ensure that hcad col comes in even if not considered numerical\n",
    "        if normalize_columns and  label != 'hcad':\n",
    "            column_data = np.array(df[label].astype(float))\n",
    "            column_data -= np.min(column_data)\n",
    "            data_dict[label] = column_data / np.max(column_data)\n",
    "        else:\n",
    "            data_dict[label] = df[label].astype(float)\n",
    "\n",
    "        result = pandas.DataFrame.from_dict(data_dict)\n",
    "\n",
    "    result = result.replace([np.inf, -np.inf], 1)\n",
    "    \n",
    "    \n",
    "    for label in result:\n",
    "        if not only_columns_containing in str(label):\n",
    "            result.drop([str(label)], axis = 1 , inplace= True)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(batch_size = 100, random = True, start_index = 0, val = False):\n",
    "    metamat = np.zeros([batch_size, image_width, image_width, 2]).astype(float32)\n",
    "    regression_y = np.zeros([batch_size,1]).astype(float32)\n",
    "    categorical_y = np.zeros([batch_size]).astype(int32)\n",
    "    extra_features = np.zeros([batch_size,len(wind_data[0])]).astype(float32)\n",
    "    i = 0\n",
    "    for _ in range(batch_size):\n",
    "\n",
    "        index = i + start_index\n",
    "        if random:\n",
    "            index = rand.randint(0, len(meta)-1)\n",
    "            if index % 100 == 0 and not val:\n",
    "                index +=  rand.randint(1, 99)\n",
    "                if index > len(meta):\n",
    "                    index = len(meta) -1\n",
    "            elif val:\n",
    "                index -= index % 100\n",
    "        lon = [meta['lon'][index]]\n",
    "        lat = [meta['lat'][index]]\n",
    "        max_wind_speed = np.max(wind_data[index])\n",
    "        regression_y[i] = y_gaussian[index]\n",
    "        categorical_y[i] = y_data['y'][index]\n",
    "        extra_features[i] = wind_data[index]\n",
    "\n",
    "        xx, yy = pyproj.transform(wgs84, UTM26N, lon, lat)\n",
    "        housex=xx[0]\n",
    "        housey=yy[0]\n",
    "\n",
    "        lidar = None\n",
    "        try:\n",
    "            lidar = get_heightmap_around(housex, housey, window_width_pixels=image_width)\n",
    "        except:\n",
    "            continue\n",
    "        if(lidar == None):\n",
    "            continue\n",
    "        \n",
    "        lidar = lidar.copy()\n",
    "        lidar[lidar < -100] = -100\n",
    "        lidar[lidar >250] = 250\n",
    "        lidar /= 250.0\n",
    "#         lidar -= np.min(lidar)\n",
    "#         lidar /= np.max(lidar)\n",
    "\n",
    "\n",
    "        metamat[i, :, :, 0] = lidar\n",
    "\n",
    "        wind_data_channel = np.zeros([image_width, image_width]).astype(float32)\n",
    "        wind_data_channel.fill(max_wind_speed)\n",
    "        metamat[i, :, :, 1] = wind_data_channel\n",
    "\n",
    "\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    #metamat = metamat.reshape([batch_size, image_width**2])\n",
    "\n",
    "    return metamat, extra_features, regression_y, np.eye(2)[categorical_y]\n",
    "\n",
    "\n",
    "# metamat, extra_features, regression_y, cat_y = get_batch(1, random = False, start_index=12314)\n",
    "\n",
    "# meta = load_data(\"/home/isaac/Dropbox/data_for_brian/meta/df_meta.hd\")\n",
    "\n",
    "# X, _, _ = get_batch(100)\n",
    "# plt.imshow(X[0,:,:,0])\n",
    "# plt.show()\n",
    "# X = X[:,:,:,0].reshape([-1])\n",
    "# print np.min(X)\n",
    "# plt.hist(X, bins = 50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "wind_data = load_data(\"/home/isaac/Dropbox/data_for_brian/wind_features/hcad_interp_withoutpartial_rad100_hist8x8.mat.hd\", \n",
    "              normalize_columns=True, only_columns_containing = \"spd\")\n",
    "meta = load_data(\"/home/isaac/Dropbox/data_for_brian/meta/df_meta.hd\")\n",
    "y_data = load_data(\"/home/isaac/Dropbox/data_for_brian/y_df.hd\")\n",
    "hcad_dpr = load_data(\"/home/isaac/Dropbox/data_for_brian/hcad_features/hcad_df_200.hd\", \n",
    "              normalize_columns=True, only_columns_containing = \"mean_dpr\")\n",
    "wind_data = np.concatenate((wind_data, hcad_dpr), axis = 1)\n",
    "# wind_data = wind_data[meta['img0'] == 1]\n",
    "# y_data = wind_data[meta['img0'] == 1]\n",
    "y_gaussian = pickle.load(open( \"/home/isaac/Desktop/house_guassian_damage_balance.p\", \"rb\" ) )\n",
    "\n",
    "wgs84=pyproj.Proj(\"+init=EPSG:4326\") # LatLon with WGS84 datum used by GPS units and Google Earth\n",
    "UTM26N=pyproj.Proj(\"+init=EPSG:2278\") # UTM coords, zone Texas Central, WGS84 datum\n",
    "\n",
    "# lon = [meta['lon'][250000]] #these are good becuase of clear local features\n",
    "# lat = [meta['lat'][250000]]\n",
    "\n",
    "\n",
    "lon = [meta['lon'][12000]]\n",
    "lat = [meta['lat'][12000]]\n",
    "\n",
    "print \"house lat lon\", lat, lon\n",
    "xx, yy = pyproj.transform(wgs84, UTM26N, lon, lat)\n",
    "print xx, yy\n",
    "housex=xx[0]\n",
    "housey=yy[0]\n",
    "display(get_heightmap_around(housex,housey, window_width_pixels = image_width))\n",
    "\n",
    "n=10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "tf.reset_default_graph()\n",
    "# Data loading and preprocessing\n",
    "# image_data_data, extra_features_data, batch_y = get_batch(batch_size=13640, random = True)\n",
    "image_data_data, _, batch_y_regression, batch_y_categorical = get_batch(batch_size=160, random = True)\n",
    "# print batch_y[0]\n",
    "# np.random.shuffle(batch_y)\n",
    "# print batch_y[0]\n",
    "# batch_y = np.round(batch_y)\n",
    "# for i in range(len(batch_y)):\n",
    "#     extra_features_data[i].fill(batch_y[i][0])\n",
    "\n",
    "image_data = tf.placeholder(tf.float32, [None, image_width, image_width, 2], name = 'image_data_placeholder')\n",
    "extra_features = tf.placeholder(tf.float32, [None, len(wind_data[0])], name = 'extrafeatures_ph')\n",
    "# extra_features = tf.placeholder(tf.float32, [None, 1], name = 'extrafeatures_ph')\n",
    "\n",
    "regression_y_in = tf.placeholder(tf.float32, [None, 1], name = 'y_regression_ph')\n",
    "categorical_y_in = tf.placeholder(tf.float32, [None, 2], name = 'y_categorical_ph')\n",
    "\n",
    "\n",
    "\n",
    "print image_data.get_shape(), \"<- image input\"\n",
    "\n",
    "network = tflearn.conv_2d(image_data, 8, 5, strides=2, activation='relu', regularizer=\"L2\", weight_decay=0.0)\n",
    "bottom_conv = network\n",
    "network = tflearn.conv_2d(network, 8, 5, strides=2, activation='relu', regularizer=\"L2\", weight_decay=0.0)\n",
    "\n",
    "middle = network\n",
    "\n",
    "network = tflearn.conv_2d_transpose(network, 8, 5, [image_width, image_width, 8], strides=4, activation='relu', \n",
    "                                      regularizer=\"L2\", weight_decay=0.0)\n",
    "\n",
    "AE_output = tflearn.conv_2d_transpose(network, 2, 5, [image_width, image_width, 2], strides=1, activation='sigmoid', \n",
    "                                      regularizer=\"L2\", weight_decay=0.0)\n",
    "\n",
    "\n",
    "# network = tflearn.conv_2d(image_data, 32, 11, strides=2, activation='relu', regularizer=\"L2\", weight_decay=0.0)\n",
    "# bottom_conv = network\n",
    "# network = tflearn.conv_2d(network, 32, 11, strides=2, activation='relu', regularizer=\"L2\", weight_decay=0.0)\n",
    "\n",
    "# print network.get_shape()\n",
    "# network = tflearn.layers.conv.max_pool_2d (network, 2, strides=2, padding='same', name='MaxPool2D')\n",
    "# print network.get_shape()\n",
    "# network = tflearn.conv_2d(network, 128, 5,  strides=1, activation='relu', regularizer=\"L2\", weight_decay=0.0)\n",
    "# network = tflearn.conv_2d(network, 128, 5,  strides=1, activation='relu', regularizer=\"L2\", weight_decay=0.0)\n",
    "\n",
    "# print network.get_shape()\n",
    "# netowrk = tflearn.layers.core.dropout (network, .7, name='Dropout')\n",
    "\n",
    "# network = tflearn.layers.conv.max_pool_2d (network, 2, strides=2, padding='same', name='MaxPool2D')\n",
    "# print network.get_shape()\n",
    "\n",
    "# network = tflearn.conv_2d(network, 128, 3,  strides=1, activation='relu', regularizer=\"L2\", weight_decay=0.0)\n",
    "# network = tflearn.conv_2d(network, 128, 3,  strides=1, activation='relu', regularizer=\"L2\", weight_decay=0.0)\n",
    "\n",
    "# print network.get_shape()\n",
    "# netowrk = tflearn.layers.core.dropout (network, .7, name='Dropout')\n",
    "\n",
    "# network = tflearn.conv_2d(network, 64, 3, activation='relu', regularizer=\"L2\", weight_decay=0.0)\n",
    "\n",
    "# middle = network\n",
    "\n",
    "# print middle.get_shape(), \"<- middle\"\n",
    "\n",
    "\n",
    "# ## Reconstruct the input\n",
    "# network = tflearn.conv_2d_transpose(middle, 32, 5, [16, 16, 32], strides=2, activation='relu', regularizer=\"L2\", weight_decay=0.0)\n",
    "# print network.get_shape()\n",
    "# network = tflearn.layers.conv.upsample_2d (network, 2)\n",
    "# print network.get_shape()\n",
    "\n",
    "# network = tflearn.conv_2d_transpose(network, 32, 5, [64, 64, 32], strides=2, activation='relu', regularizer=\"L2\", weight_decay=0.0)\n",
    "# print network.get_shape()\n",
    "# print network.get_shape()\n",
    "# AE_output = tflearn.conv_2d_transpose(network, 2, 5, [image_width, image_width, 2], strides=2, activation='tanh', regularizer=\"L2\", weight_decay=0.0)\n",
    "# print AE_output.get_shape()\n",
    "\n",
    "\n",
    "## predict damage\n",
    "dense1 = tflearn.fully_connected(middle, 460, activation='relu',\n",
    "                                 regularizer='L2', weight_decay=0.0)\n",
    "print \"pre merge\", dense1.get_shape()\n",
    "dense2 = tflearn.layers.merge_ops.merge ([dense1, extra_features], 'concat', axis = 1, name='MergeOutputs')\n",
    "print \"post merge\", dense2.get_shape()\n",
    "\n",
    "dense3 = tflearn.fully_connected(dense2, 130, activation='relu',\n",
    "                                 regularizer='L2', weight_decay=0.0)\n",
    "\n",
    "d = tflearn.fully_connected(dense2, 130, activation='relu',\n",
    "                                 regularizer='L2', weight_decay=0.0)\n",
    "\n",
    "d = tflearn.fully_connected(d, 130, activation='relu',\n",
    "                                 regularizer='L2', weight_decay=0.0)\n",
    "\n",
    "y_regression_pred = tflearn.fully_connected(d, 1, activation='sigmoid',\n",
    "                                 regularizer='L2', weight_decay=0.0)\n",
    "\n",
    "\n",
    "# y_categorical_pred = tflearn.fully_connected(d, 2, activation='softmax',\n",
    "#                                  regularizer='L2', weight_decay=0.0)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_regression_loss = tf.reduce_mean(tf.square(tf.sub(y_regression_pred, regression_y_in))) \n",
    "# cross_entropy = 0.143 * tf.reduce_mean(-tf.reduce_sum(categorical_y_in * tf.log(y_categorical_pred), reduction_indices=[1]))\n",
    "average_conv_weights = tf.reduce_mean(tf.square(bottom_conv.W)) + tf.reduce_mean(tf.square(AE_output.W))\n",
    "autoencoder_loss = tf.reduce_mean(tf.square(tf.sub(image_data, AE_output)) ) + .001 * average_conv_weights\n",
    "autoencoder_optimizer = tf.train.RMSPropOptimizer(learning_rate=.0004).minimize(autoencoder_loss)\n",
    "\n",
    "regression_alpha = .7\n",
    "categorical_alpha = 0\n",
    "cost = ((regression_alpha *y_regression_loss ) \n",
    "#     + (categorical_alpha * cross_entropy) \n",
    "    + ((1 - regression_alpha + categorical_alpha) * autoencoder_loss))\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=.0004).minimize(cost)\n",
    "\n",
    "# y_regression_optimizer = tf.train.RMSPropOptimizer(learning_rate=.02).minimize(cost)\n",
    "\n",
    "# dense2 = tflearn.fully_connected(dense1, 164, activation='relu',\n",
    "#                                  regularizer='L2', weight_decay=0.0)\n",
    "# network = tflearn.fully_connected(network, 1, activation='tanh')\n",
    "\n",
    "# # Regression using SGD with learning rate decay and Top-3 accuracy\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sgd = tflearn.RMSProp(learning_rate=0.001)\n",
    "# net = tflearn.regression(network, optimizer=sgd,\n",
    "#                          loss='mean_square')\n",
    "\n",
    "# # Training\n",
    "# model = tflearn.DNN(net, tensorboard_verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottom_conv_weights = sess.run(bottom_conv.W)\n",
    "print bottom_conv_weights.shape\n",
    "for i in range(bottom_conv_weights.shape[-1]):\n",
    "    plt.imshow(bottom_conv_weights[:,:,0,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "step = 1\n",
    "image_data_data, extra_features_data, batch_y_regression, batch_y_categorical \\\n",
    "    = get_batch(batch_size=400, random = False, start_index = 10000)\n",
    "\n",
    "\n",
    "#print \"batch_y:\", batch_y\n",
    "# plt.scatter(extra_features_data[0], extra_features_data[1])\n",
    "# plt.show() \n",
    "\n",
    "def show_image(image_mat, index):\n",
    "    print np.min(image_mat[index,:,:,0]),np.max(image_mat[index,:,:,0])\n",
    "\n",
    "    plt.imshow(image_mat[index,:,:,0])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# show_image(image_data_data, 0)\n",
    "# show_image(image_data_data, 1)\n",
    "# show_image(image_data_data, 2)\n",
    "# show_image(image_data_data, 3)\n",
    "\n",
    "\n",
    "# step = 0\n",
    "print \"image shape:\", image_data_data.shape\n",
    "while step * batch_size < 10000000000:\n",
    "    image_data_data, extra_features_data, batch_y_regression, batch_y_categorical\\\n",
    "            = get_batch(batch_size=200, random = True, start_index = 0)  \n",
    "\n",
    "\n",
    "    _, batch_cost = sess.run([autoencoder_optimizer, autoencoder_loss], feed_dict={image_data: image_data_data,\n",
    "                                   extra_features: extra_features_data,\n",
    "                                  regression_y_in: batch_y_regression,\n",
    "                                   categorical_y_in: batch_y_categorical })\n",
    "    batch_costs.append(batch_cost)\n",
    "    if  step % 500 == 0:\n",
    "        saver.save(sess, 'joint_conv_model', global_step=step)\n",
    "        image_data_data, extra_features_data, batch_y_regression, batch_y_categorical\\\n",
    "            = get_batch(batch_size=100, random = True, start_index = 0)      \n",
    "            \n",
    "        print \"mean MSE:\", np.mean(np.square(np.array([y_average]*len(batch_y_regression)) - batch_y_regression))\n",
    "\n",
    "        # Calculate batch loss and accuracy\n",
    "        y_guess, y_loss_fetched, autoencoder_loss_fetched, joint_loss, autoencoder_result = \\\n",
    "                                            sess.run([y_regression_pred, y_regression_loss, autoencoder_loss, cost, AE_output], \n",
    "                                              feed_dict={image_data: image_data_data,\n",
    "                                                          extra_features: extra_features_data,\n",
    "                                                          regression_y_in: batch_y_regression,\n",
    "                                                           categorical_y_in: batch_y_categorical })\n",
    "        print (\"Iter \" + str(step*batch_size) + \", Minibatch Y Loss= \", \n",
    "            str(y_loss_fetched), \"Joint cost:\", str(joint_loss),\n",
    "               \"Autoencoder cost:\", str(autoencoder_loss_fetched))\n",
    "        \n",
    "        print np.max(autoencoder_result), np.min(autoencoder_result)\n",
    "\n",
    "        plt.imshow(autoencoder_result[0,:,:,0])\n",
    "        plt.show()\n",
    "        plt.imshow(image_data_data[0,:,:,0])\n",
    "        plt.show()\n",
    "        plt.scatter(list(batch_y_regression.reshape([-1])), y_guess)\n",
    "        plt.show()\n",
    "        moving_window_size = 5\n",
    "        #plt.plot(batch_costs[moving_window_size:-moving_window_size])\n",
    "        plt.plot(np.convolve(batch_costs, \n",
    "                             np.ones(moving_window_size)/moving_window_size)[moving_window_size:-moving_window_size])\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        image_data_data, extra_features_data, batch_y_regression, batch_y_categorical\\\n",
    "            = get_batch(batch_size=200, random = True, start_index = 0, val = True)  \n",
    "        _, y_val_loss_fetched, _, _ = \\\n",
    "                    sess.run([y_regression_pred, y_regression_loss, autoencoder_loss, cost], \n",
    "                              feed_dict={image_data: image_data_data,\n",
    "                                          extra_features: extra_features_data,\n",
    "                                          regression_y_in: batch_y_regression,\n",
    "                                           categorical_y_in: batch_y_categorical })\n",
    "        print \"mean MSE on val data:\", np.mean(np.square(np.array([y_average]*len(batch_y_regression)) - batch_y_regression))\n",
    "\n",
    "        print (\"val y gaussian field loss:\", y_val_loss_fetched)\n",
    "\n",
    "    step += 1\n",
    "print \"Optimization Finished!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
