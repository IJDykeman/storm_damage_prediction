{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# An approach to computing wind risk using HAZUS\n",
    "- Approach 1: compute damage probability at the building level and finally average damages at keymap square level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Read fragility curves from HAZUS\n",
    "Fragility curves are indexed by\n",
    "- floors (WSF1 or WSF2)\n",
    "- roof (hip or gable)\n",
    "- garage (attached, detached, none)\n",
    "- shutters (shutters or no shutters) [for Houston it is noshutters]\n",
    "- terrain (1 though 5)\n",
    "- damage level (1 through 6)\n",
    "- wind speed (50 to 250 mph)\n",
    "\n",
    "Averaged fragility curves are indexed by (averaged over roof and garage)\n",
    "- floors (WSF1 or WSF2)\n",
    "- terrain (1 though 5)\n",
    "- damage level (1 through 6)\n",
    "- wind speed (50 to 250 mph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# huListofWindBldgTypes.txt is obtained from HAZUS-MH3 source files\n",
    "# huDamLossFun_Fit.txt is obtained from HAZUS-MH3 source files\n",
    "\n",
    "# This function reads the raw fragility curve data from HAZUS-MH3 source files and\n",
    "# constructs a data frame with the following structure\n",
    "#\n",
    "# [u'wbID', u'Floors', u'charDescription', u'Roof', u'Shutters', u'Garage',\n",
    "#       u'TERRAINID', u'DamLossDescID', u'WS50', u'WS55', u'WS60', u'WS65',\n",
    "#       u'WS70', u'WS75', u'WS80', u'WS85', u'WS90', u'WS95', u'WS100',\n",
    "#       u'WS105', u'WS110', u'WS115', u'WS120', u'WS125', u'WS130', u'WS135',\n",
    "#       u'WS140', u'WS145', u'WS150', u'WS155', u'WS160', u'WS165', u'WS170',\n",
    "#       u'WS175', u'WS180', u'WS185', u'WS190', u'WS195', u'WS200', u'WS205',\n",
    "#       u'WS210', u'WS215', u'WS220', u'WS225', u'WS230', u'WS235', u'WS240',\n",
    "#      u'WS245', u'WS250', u'mu', u'sigma']\n",
    "# For Floors in WSF1, WSF2; Roof in hip, gable, Shutters in shutters, noshutters,\n",
    "# for Garage in attached, detached, none, for TerrainID in 1-5, for DamLossDescID in 1-6,\n",
    "# probability of damage due to winds from 50 mph to 250 mph is provided along with\n",
    "# a mu and sigma for the fitted log normal distribution.\n",
    "\n",
    "def read_fragility_data():\n",
    "#     bldg_types = pd.read_csv('FragilityCurves/huListofWindBldgTypes.txt')\n",
    "    bldg_types = pd.read_csv('/home/isaac/Desktop/storm_damage_prediction/data/FragilityCurves/huListOfWindBldgTypes.txt')\n",
    "\n",
    "\n",
    "    # extract data for 1 floor and 2 floor building data only\n",
    "    bldgs1 = bldg_types[((bldg_types[\"sbtName\"]==\"WSF1\") | (bldg_types[\"sbtName\"]==\"WSF2\"))]\n",
    "    # extract data for 6in roof nails, toe nail, and no roof straps\n",
    "    bldgs1 = bldgs1[(bldgs1[\"charDescription\"].str.contains(\"rda6d\")) & \n",
    "                    (bldgs1[\"charDescription\"].str.contains(\"tnail\")) &\n",
    "                    (bldgs1[\"charDescription\"].str.contains(\"swrno\"))]\n",
    "\n",
    "    # rename column sbtName to be floors (WSF1 and WSF2)\n",
    "    bldgs2 = bldgs1[[\"wbID\",\"sbtName\",\"charDescription\"]].copy()\n",
    "    bldgs2 = bldgs2.rename(columns = {'sbtName':'Floors'})\n",
    "    \n",
    "    # add the Roof column: hip or gable\n",
    "    bldgs2['Roof'] = None\n",
    "    bldgs2.loc[bldgs2['charDescription'].str.contains('rship'),['Roof']] = 'hip'\n",
    "    bldgs2.loc[bldgs2['charDescription'].str.contains('rsgab'),['Roof']] = 'gable'\n",
    "\n",
    "    # add the Shutters column: shutters or noshutters\n",
    "    bldgs2['Shutters'] = None\n",
    "    bldgs2.loc[bldgs2['charDescription'].str.contains('shtys'),['Shutters']] = 'shutters'\n",
    "    bldgs2.loc[bldgs2['charDescription'].str.contains('shtno'),['Shutters']] = 'noshutters'\n",
    "\n",
    "    # add the Garage column: attached, detached, none\n",
    "    bldgs2['Garage'] = None\n",
    "    bldgs2.loc[bldgs2['charDescription'].str.contains('gdstd'),['Garage']] = 'attached'\n",
    "    bldgs2.loc[bldgs2['charDescription'].str.contains('gdsup'),['Garage']] = 'attached'\n",
    "    bldgs2.loc[bldgs2['charDescription'].str.contains('gdwkd'),['Garage']] = 'detached'\n",
    "    # bldgs2.loc[bldgs2['charDescription'].str.contains('gdno'),['Garage']] = 'none'\n",
    "    bldgs2.loc[bldgs2['charDescription'].str.contains('gdno'),['Garage']] = 'none'\n",
    "\n",
    "    # populate the fragility statistics\n",
    "#     fragility1 = pd.read_csv('FragilityCurves/huDamLossFun_Fit.txt')\n",
    "    fragility1 = pd.read_csv('/home/isaac/Desktop/storm_damage_prediction/data/FragilityCurves/huDamLossFun_Fit.txt')\n",
    "\n",
    "    fragility = pd.merge(bldgs2,fragility1,on=\"wbID\",how=\"inner\")\n",
    "\n",
    "    return fragility\n",
    "\n",
    "# average the fragility curves over roof and garage:\n",
    "# now fragility curves are indexed by damage state, terrain and floors\n",
    "\n",
    "def average_fragility(fragility):\n",
    "    fragility_noshutters = fragility[fragility['Shutters']=='noshutters'].copy()\n",
    "    wlist = ['WS' + str(i) for i in range(50,255,5)]\n",
    "    avg_fragility = {}\n",
    "\n",
    "    grps = fragility_noshutters.groupby(['DamLossDescID','TERRAINID','Floors','Garage','Roof'])\n",
    "    factors = {('attached','hip'): 0.5 * 0.34, ('attached','gable'): 0.5 * 0.66, ('detached','hip'): 0.25 * 0.34,\n",
    "                  ('detached','gable'): 0.25 * 0.66,('none','hip'): 0.25 * 0.34,\n",
    "                  ('none','gable'): 0.25 * 0.66}\n",
    "    for dam_state  in range(1,5):\n",
    "        for terrain in range(1,5):\n",
    "            for floors in ['WSF1','WSF2']:\n",
    "                g1 = pd.DataFrame()  \n",
    "                for garage in ['attached','detached','none']:\n",
    "                    for roof in ['hip','gable']:\n",
    "                        g = grps.get_group((dam_state,terrain,floors,garage,roof))\n",
    "                        x = g[wlist].mean(axis=0) * factors[(garage,roof)]\n",
    "                        g1 = g1.append(x,ignore_index=True)\n",
    "                \n",
    "    \n",
    "                avg_fragility[(dam_state,terrain,floors)] = g1.sum(axis=0)                                                          \n",
    "\n",
    "    avg_frag = pd.DataFrame.from_items([(avg_fragility.keys()[k],avg_fragility.values()[k]) for k in range(len(avg_fragility))])\n",
    "    avg_frag = avg_frag.transpose()\n",
    "    avg_frag['Floors'] = [t[2] for t in avg_frag.index]\n",
    "    avg_frag['DamLossDescID'] = [t[0] for t in avg_frag.index]\n",
    "    avg_frag['TERRAINID'] = [t[1] for t in avg_frag.index]\n",
    "    return avg_frag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Read building level data (df_meta and data.p; shared with Isaac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_meta: meta data\n",
    "# check the pickled data.p and df_meta.p from may2015 directory\n",
    "\n",
    "def read_dfs():\n",
    "#     df_meta = pd.read_csv('../../code/new_wind_model/may2015/df_meta.csv')\n",
    "    df_meta = pd.read_csv('/home/isaac/Desktop/storm_damage_prediction/data/df_meta.csv')\n",
    "\n",
    "    df_meta.drop(df_meta.columns[0],1,inplace=True)\n",
    "    print \"df_meta shape: \", df_meta.shape\n",
    "\n",
    "    # read the pickled data file data.p\n",
    "\n",
    "#     df_data = pd.read_pickle('../../code/new_wind_model/may2015/data.p')\n",
    "    print \"reading pickle...\"\n",
    "    df_data = pd.read_pickle('./data/data.p')\n",
    "    print \"done.\"\n",
    "\n",
    "    df_data = pd.DataFrame(df_data).transpose()\n",
    "    df_data['hcad'] = df_data.index\n",
    "    return df_meta, df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Extract number of floors and wind data from df_data\n",
    "- extract number of floors, max wind, low_wind, high_wind (rounding of maxwind in mutliples of 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_bld_wind(df_data, factor):\n",
    "    wlist = ['spd' + str(i) for i in range(1,12)]\n",
    "    bld_wind_data = df_data[['hcad','bld_ar']+wlist].copy()\n",
    "    bld_wind_data['Floors'] = 'WSF1'\n",
    "    bld_wind_data.loc[bld_wind_data['bld_ar'] > 1800,'Floors'] = 'WSF2'\n",
    "    bld_wind_data = bld_wind_data[['hcad','Floors'] + wlist]\n",
    "\n",
    "    # add the MaxWind field\n",
    "    bld_wind_data['MaxWind'] = bld_wind_data[['spd1','spd2','spd3','spd4','spd5','spd6','spd7','spd8','spd9','spd10','spd11',]].apply(max,axis=1)\n",
    "    bld_wind_data['MaxWind'] = bld_wind_data['MaxWind'].apply(lambda x: x * 2.237 * factor)  # convert from meters/sec to miles/hr\n",
    "    bld_wind_data = bld_wind_data[['hcad','Floors','MaxWind']]\n",
    "\n",
    "    # put the bounding speeds on (rounded to 50s) based on MaxWind\n",
    "\n",
    "    w = bld_wind_data['MaxWind'] \n",
    "    bld_wind_data['low_wind'] = 50 + np.floor((w-50)/5.0) * 5\n",
    "    bld_wind_data['low_wind'] = bld_wind_data['low_wind'].astype(int)\n",
    "    bld_wind_data['high_wind'] = 50 + np.ceil((w-50)/5.0) * 5\n",
    "    bld_wind_data['high_wind'] = bld_wind_data['high_wind'].astype(int)\n",
    "    print \"bld_wind data shape: \", bld_wind_data.shape\n",
    "    return bld_wind_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Extract terrain (use Isaac's terrain info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# terrain data (Isaac)\n",
    "def extract_terrain(bld_wind_data):\n",
    "#     terrain = pd.read_csv('hcad_land_cover.txt')\n",
    "    terrain = pd.read_csv('./data/hcad_land_cover.txt')\n",
    "\n",
    "    terrain = terrain.rename(columns={'HCAD_NUM':'hcad'})\n",
    "\n",
    "# Terrain coding\n",
    "# 1 = developed high intensity\n",
    "# 2 = developed low intensity\n",
    "# 3 = developed open space\n",
    "# 4  =  cultivated\n",
    "# 5  =  grassland/shrub\n",
    "# 6  =  forest\n",
    "# 7  =  woody wetland\n",
    "# 8 =  herbaceous wetland\n",
    "# 9 =  barren\n",
    "# 10 = water\n",
    "\n",
    "    # Translation table from HAZUS Technical Manual (Table 3.9; Texas values)\n",
    "    roughness_map = {1:0.55, 2:0.35, 3:0.44, 4:0.05, 5:0.07, 6:0.55, 7:0.50, 8: 0.10, 9:0.13, 10: 0.011}\n",
    "\n",
    "# In HAZUS: we have terrainid 1-5 mapped according to\n",
    "# 0 - 0.03 = 1\n",
    "# 0.03 - 0.15 = 2\n",
    "# 0.15 - 0.35 = 3\n",
    "# 0.35 - 0.7 = 4\n",
    "# 0.7 - 1.00 = 5 \n",
    "\n",
    "#rterrain_map = pd.DataFrame({1:4, 2:3, 3:4, 4:2, 5:2, 6:4, 7:4, 8: 2, 9:2, 10: 1}.items())\n",
    "    rterrain_map = pd.DataFrame({1:4, 2:4, 3:4, 4:2, 5:2, 6:4, 7:4, 8: 2, 9:3, 10: 1}.items())\n",
    "    rterrain_map.columns = ['LAND_COVER','TERRAINID']\n",
    "\n",
    "\n",
    "    # Map each roughness value to fragility curve terrain model\n",
    "    # map to roughness scale in HAZUS (HAZUS has roughness length)\n",
    "\n",
    "    terrain.groupby(['LAND_COVER']).count()\n",
    "    rterrain = pd.merge(terrain,rterrain_map,on=['LAND_COVER'],how='inner')\n",
    "    rterrain = rterrain[['hcad','TERRAINID']]\n",
    "\n",
    "    # merge building, wind and terrain data\n",
    "    hazus_data = pd.merge(rterrain,bld_wind_data,on=['hcad'],how='inner')\n",
    "    # print hazus_data.shape, hazus_data.columns\n",
    "    return hazus_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compute individual home level probability of damage\n",
    "- use averaged fragility curves to estimate probability of damage for each home from number of floors, maxwind (low wind, high wind), also include actual damage information for each home from Ragland damage data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "# merge average fragility curves with the hazus_data\n",
    "def compute_hazus_prob(hazus_data, df_meta, avg_frag):\n",
    "    \n",
    "    hazus_data1 = pd.merge(hazus_data,avg_frag,on=['Floors','TERRAINID'], how='inner')\n",
    "    hazus_data2 = hazus_data1[(hazus_data1['DamLossDescID']==1) | (hazus_data1['DamLossDescID']==2)]\n",
    "    hazus_data2 = hazus_data2.reset_index()\n",
    "\n",
    "    # calculate probability associated with lower wind speed (rounded to the 50-250 in steps of 5)\n",
    "\n",
    "    low_probs = np.zeros((hazus_data2.shape[0],))\n",
    "    for speed in range(50,255,5):\n",
    "        low_probs[hazus_data2[hazus_data2['low_wind']==speed].index.tolist()]= hazus_data2.loc[hazus_data2['low_wind']==speed,'WS'+str(speed)]\n",
    "    hazus_data2.loc[:,'low_prob'] = pd.Series(low_probs,index=hazus_data2.index)\n",
    "\n",
    "    # calculate probability associated with higher wind speed (rounded to the 55-90 in steps of 5)\n",
    "\n",
    "    high_probs = np.zeros((hazus_data2.shape[0],))\n",
    "    for speed in range(55,255,5):\n",
    "        high_probs[hazus_data2[hazus_data2['high_wind']==speed].index.tolist()]= hazus_data2.loc[hazus_data2['high_wind']==speed,'WS'+str(speed)]\n",
    "    hazus_data2.loc[:,'high_prob'] = pd.Series(high_probs,index=hazus_data2.index)\n",
    "\n",
    "    ptable = hazus_data2[['hcad','Floors','TERRAINID','DamLossDescID','MaxWind','low_wind','high_wind','low_prob','high_prob']].copy()\n",
    "\n",
    "    # linear interpolation between low prob and high prob\n",
    "\n",
    "    ptable['factor'] = ((ptable['high_prob']-ptable['low_prob'])/(ptable['high_wind']-ptable['low_wind'])) \n",
    "    ptable['dprob'] = ptable['low_prob'] + ptable['factor'] * (ptable['MaxWind']-ptable['low_wind'])\n",
    "\n",
    "    # merge keymap information\n",
    "\n",
    "    ptable = pd.merge(ptable,df_meta[['hcad','keymap']],on=['hcad'],how='inner')\n",
    "\n",
    "    # merge with damage information \n",
    "#     pdamage = pd.read_csv('../new_wind_model/may2015/pdamage.csv')\n",
    "    pdamage = pd.read_csv('./data/pdamage.csv')\n",
    "\n",
    "    pdamage.drop(pdamage.columns[0],1,inplace=True)\n",
    "    pdamage = pdamage[['hcad','damage']]\n",
    "    ptable1 = pd.merge(pdamage,ptable,on=[\"hcad\"],how=\"inner\")\n",
    "\n",
    "    # probability of damage = prob of ds1 - prob of ds2\n",
    "\n",
    "    s1 = ptable1[ptable1['DamLossDescID']==1][['hcad','dprob','keymap','damage']]\n",
    "    s2 = ptable1[ptable1['DamLossDescID']==2][['hcad','dprob','keymap','damage']]\n",
    "    results = pd.merge(s1,s2,on=[\"hcad\",\"keymap\",\"damage\"],how=\"inner\")\n",
    "    results['dprob1'] = results['dprob_x'] - results['dprob_y']\n",
    "    print metrics.roc_auc_score(results.damage, results.dprob1, average='macro', sample_weight=None)\n",
    "    return results\n",
    "\n",
    "def agg_prob_keymap(results):\n",
    "    # aggregate results by keymap square\n",
    "    pcounts = results.groupby('keymap').agg({'dprob1':'sum','damage': {'damage_sum':'sum','total':'count'}})\n",
    "    return pcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Compute probability at home level and then aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# run the protocol at the building level, then aggregate to keymap\n",
    "\n",
    "def run_approach1():\n",
    "    fragility = read_fragility_data()\n",
    "    df_meta, df_data = read_dfs()\n",
    "    avg_frag = average_fragility(fragility)\n",
    "\n",
    "    factor = 0.9\n",
    "    \n",
    "    bld_wind_data = extract_bld_wind(df_data,factor)\n",
    "    hazus_data = extract_terrain(bld_wind_data)    \n",
    "    results09 = compute_hazus_prob(hazus_data, df_meta, avg_frag)\n",
    "    pcounts09 = agg_prob_keymap(results09)\n",
    "\n",
    "    factor = 1.0\n",
    "    bld_wind_data = extract_bld_wind(df_data,factor)\n",
    "    hazus_data = extract_terrain(bld_wind_data)\n",
    "    results10 = compute_hazus_prob(hazus_data, df_meta, avg_frag)\n",
    "    pcounts10 = agg_prob_keymap(results10)\n",
    "\n",
    "    factor = 1.1\n",
    "    bld_wind_data = extract_bld_wind(df_data,factor)\n",
    "    hazus_data = extract_terrain(bld_wind_data)\n",
    "    results11 = compute_hazus_prob(hazus_data, df_meta, avg_frag)\n",
    "    pcounts11 = agg_prob_keymap(results11)\n",
    "\n",
    "    # is actual damage between pcounts09 and pcounts11\n",
    "    r = pd.DataFrame()\n",
    "    r['0.9'] = pcounts09['dprob1']['sum']\n",
    "    r['1.1'] = pcounts11['dprob1']['sum']\n",
    "    r['1.0'] = pcounts10['dprob1']['sum']\n",
    "    r['actual'] = pcounts09['damage']['damage_sum']\n",
    "    r['total'] = pcounts09['damage']['total']\n",
    "\n",
    "    # is actual damage between pcounts09 and pcounts11?\n",
    "    print \"Raw count analysis\"\n",
    "    print \"Correct: \", r[(r['0.9'] <= r['actual']) & (r['1.1'] >= r['actual'])].shape\n",
    "    print \"Over: \", r[r['0.9'] > r['actual']].shape\n",
    "    print \"Under:\", r[r['1.1'] < r['actual']].shape\n",
    "\n",
    "    # take ratios into account in the comparison\n",
    "    ratios = pd.DataFrame()\n",
    "    ratios['0.9'] = pcounts09['dprob1']['sum']/pcounts09['damage']['total']\n",
    "    ratios['1.1'] = pcounts11['dprob1']['sum']/pcounts09['damage']['total']\n",
    "    ratios['1.0'] = pcounts10['dprob1']['sum']/pcounts09['damage']['total']\n",
    "    ratios['actual'] = pcounts09['damage']['damage_sum']/pcounts09['damage']['total']\n",
    "    ratios['keymap'] = ratios.index\n",
    "    \n",
    "    # correct if in interval or within 0.1 of the interval edge\n",
    "    print \"Ratio analysis\"\n",
    "    threshold = 0.1\n",
    "    print \"Correct: \", ratios[((ratios['0.9']-threshold) <= ratios['actual']) \n",
    "                          & ((ratios['1.1']+threshold) >= ratios['actual'])].shape\n",
    "    print \"Over: \", ratios[(ratios['0.9']-threshold) > ratios['actual']].shape\n",
    "    print \"Under:\", ratios[(ratios['1.1']+threshold) < ratios['actual']].shape\n",
    "\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# correct if in interval or within 0.1 of the interval edge\n",
    "def test_threshold():\n",
    "    for threshold in [0,0.01,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]:\n",
    "        print \"Ratio analysis\", threshold\n",
    "        print \"Correct: \", r1[((r1['0.9']-threshold) <= r1['actual']) & ((r1['1.1']+threshold) >= r1['actual'])].shape\n",
    "        print \"Over: \", r1[(r1['0.9']-threshold) > r1['actual']].shape\n",
    "        print \"Under:\", r1[(r1['1.1']+threshold) < r1['actual']].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_meta shape:  (870476, 8)\n",
      "reading pickle...\n",
      "done.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-913a76ab52e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# harris = gpd.GeoDataFrame.from_file(\"../new_wind_model/otherSHPs/keymap_harris.shp\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mharris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/keymap_harris.shp\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdamage_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_approach1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# merge harris with damage_probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-115c50982084>\u001b[0m in \u001b[0;36mrun_approach1\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfragility\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_fragility_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf_meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_dfs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mavg_frag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage_fragility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfragility\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mfactor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-84dfa7629c83>\u001b[0m in \u001b[0;36maverage_fragility\u001b[1;34m(fragility)\u001b[0m\n\u001b[0;32m     85\u001b[0m                         \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdam_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mterrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgarage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mroof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwlist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfactors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgarage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mroof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                         \u001b[0mg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[0;32m   3519\u001b[0m                               index=index, columns=combined_columns).convert_objects()\n\u001b[0;32m   3520\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3521\u001b[1;33m                 \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3522\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3523\u001b[0m             \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, index, columns, **kwargs)\u001b[0m\n\u001b[0;32m   2160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2161\u001b[0m         return super(DataFrame, self).reindex(index=index, columns=columns,\n\u001b[1;32m-> 2162\u001b[1;33m                                               **kwargs)\n\u001b[0m\u001b[0;32m   2163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2164\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reindex_axis'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1563\u001b[0m         return self._reindex_axes(axes, level, limit,\n\u001b[0;32m   1564\u001b[0m                                   \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1565\u001b[1;33m                                   takeable=takeable).__finalize__(self)\n\u001b[0m\u001b[0;32m   1566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1567\u001b[0m     def _reindex_axes(self, axes, level, limit, method, fill_value, copy,\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_reindex_axes\u001b[1;34m(self, axes, level, limit, method, fill_value, copy, takeable)\u001b[0m\n\u001b[0;32m   2110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2111\u001b[0m             frame = frame._reindex_columns(columns, copy, level, fill_value,\n\u001b[1;32m-> 2112\u001b[1;33m                                            limit, takeable=takeable)\n\u001b[0m\u001b[0;32m   2113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2114\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_reindex_columns\u001b[1;34m(self, new_columns, copy, level, fill_value, limit, takeable)\u001b[0m\n\u001b[0;32m   2137\u001b[0m         return self._reindex_with_indexers({1: [new_columns, indexer]},\n\u001b[0;32m   2138\u001b[0m                                            \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m                                            allow_dups=takeable)\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_reindex_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[1;34m(self, reindexers, method, fill_value, limit, copy, allow_dups)\u001b[0m\n\u001b[0;32m   1689\u001b[0m                 new_data = new_data.reindex_indexer(index, indexer, axis=baxis,\n\u001b[0;32m   1690\u001b[0m                                                     \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1691\u001b[1;33m                                                     allow_dups=allow_dups)\n\u001b[0m\u001b[0;32m   1692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1693\u001b[0m             elif (baxis == 0 and index is not None and\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups)\u001b[0m\n\u001b[0;32m   3245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3246\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3247\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_indexer_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3249\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_reindex_indexer_items\u001b[1;34m(self, new_items, indexer, fill_value)\u001b[0m\n\u001b[0;32m   3313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3314\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3315\u001b[1;33m                 \u001b[0mna_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_items\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3316\u001b[0m                 \u001b[0mplacement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_unique\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3317\u001b[0m                 na_block = self._make_na_block(na_items,\n",
      "\u001b[1;31mTypeError\u001b[0m: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead."
     ]
    }
   ],
   "source": [
    "# draw the shape files for keymaps in Harris county\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # \n",
    "\n",
    "# read the shapefile as a GeoDataFrame\n",
    "# harris = gpd.GeoDataFrame.from_file(\"../new_wind_model/otherSHPs/keymap_harris.shp\")\n",
    "harris = gpd.GeoDataFrame.from_file(\"./data/keymap_harris.shp\")\n",
    "damage_probs = run_approach1()\n",
    "\n",
    "# merge harris with damage_probs\n",
    "harris1 = pd.merge(damage_probs,harris,left_on='keymap', right_on='PageSq',how='inner')\n",
    "geo_harris1 = gpd.GeoDataFrame(harris1,geometry=harris1.geometry)\n",
    "geo_harris1.plot(column='1.1',cmap='jet',scheme='fisher_jenks',legend=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The actual damage probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "geo_harris1.plot(column='actual',cmap='jet',scheme='fisher_jenks',legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "189px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
